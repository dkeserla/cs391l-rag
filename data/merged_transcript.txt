---- START OF LECTURE 1 ----
WEBVTT

1
00:00:15.110 --> 00:00:16.370
Inderjit Dhillon: English.

2
00:00:21.340 --> 00:00:22.266
Nilesh Gupta: Hey niji.

3
00:00:24.040 --> 00:00:26.799
Inderjit Dhillon: Okay, so let's wait for people to join.

4
00:01:31.930 --> 00:01:36.870
Inderjit Dhillon: Okay, just waiting for a couple of minutes. So that, you know, more people can join.

5
00:02:19.830 --> 00:02:25.410
Inderjit Dhillon: Okay, let's start. Hopefully, you can all hear me.

6
00:02:28.590 --> 00:02:31.069
Inderjit Dhillon: Great. I see that there are about

7
00:02:31.950 --> 00:02:36.960
Inderjit Dhillon: so 23 people in the meeting. So hopefully, most of the people

8
00:02:37.350 --> 00:02:42.260
Inderjit Dhillon: who are registered for the course are here.

9
00:02:43.400 --> 00:02:50.890
Inderjit Dhillon: I see some people who are listed as guests, so I don't know what they're there. This is

10
00:02:56.200 --> 00:03:00.710
Inderjit Dhillon: so maybe just let me just quickly ask, are the people who are listed as guests? Are you

11
00:03:01.750 --> 00:03:03.279
Inderjit Dhillon: enrolled in the course

12
00:03:12.450 --> 00:03:14.739
Inderjit Dhillon: a turb, for example.

13
00:03:18.820 --> 00:03:22.639
Inderjit Dhillon: can somebody come up and just say something, so that I know that I can hear people or.

14
00:03:22.640 --> 00:03:26.109
Minkyu Choi: Yeah, I can hear you. And yeah, I'm all enrolled. This class.

15
00:03:27.150 --> 00:03:28.190
Inderjit Dhillon: Okay.

16
00:03:28.730 --> 00:03:29.309
Hormoz Shahrzad: I can't hear.

17
00:03:29.310 --> 00:03:32.400
Inderjit Dhillon: Yeah, yeah, but you're not listed as guest. So thank you.

18
00:03:32.690 --> 00:03:41.410
Inderjit Dhillon: So I'm just asking the people who are listed as guests like Ananya Athar Argo, near Mohammed.

19
00:03:41.740 --> 00:03:45.079
Atharva M Kalamkar: Oh, I'm enrolled in class, but I don't know why I'm listed as a.

20
00:03:45.260 --> 00:03:45.980
Inderjit Dhillon: How do we go?

21
00:03:46.472 --> 00:03:51.716
Inderjit Dhillon: Me, too? Okay, okay, well, we'll figure it out. Okay, well, welcome to

22
00:03:52.230 --> 00:03:55.730
Inderjit Dhillon: this course, which is on machine learning

23
00:03:56.263 --> 00:04:03.470
Inderjit Dhillon: my name is Indiji Dillon and I will be your professor who teaches this course.

24
00:04:03.650 --> 00:04:10.109
Inderjit Dhillon: and we also have Nilesh Gupta over here, who will be the ta for the course.

25
00:04:10.280 --> 00:04:13.539
Inderjit Dhillon: Nilesh, just come out and say Hi! To everybody.

26
00:04:13.540 --> 00:04:17.000
Nilesh Gupta: Hey, everyone? Yeah, I'll be your ticket.

27
00:04:17.310 --> 00:04:24.760
Nilesh Gupta: I'm a Phd student at ut. Computer science. I do machine learning here, excited to

28
00:04:25.060 --> 00:04:27.580
Nilesh Gupta: it, teaches this course, along with Indiji.

29
00:04:28.470 --> 00:04:29.700
Inderjit Dhillon: Okay, awesome.

30
00:04:29.820 --> 00:04:33.579
Inderjit Dhillon: Let me share my screen so hopefully, you can

31
00:04:33.780 --> 00:04:43.000
Inderjit Dhillon: share the screen. Today. I'm not going to. Actually, you know, in some sense teach any particular material. But I will go over all the course basics.

32
00:04:44.030 --> 00:04:53.757
Inderjit Dhillon: So this is Cs 391 l. Machine learning. It's a web based course. All the instruction will be

33
00:04:54.480 --> 00:05:03.110
Inderjit Dhillon: offline online through zoom the course timings. There is a specific time. This course meets. Monday, Wednesday.

34
00:05:03.250 --> 00:05:10.319
Inderjit Dhillon: 3, 30 to 5 pm. The lecture will be from 3 30 to 4, 45

35
00:05:10.991 --> 00:05:16.059
Inderjit Dhillon: this is the course web page. By the way, the one that I'm showing you. Here's the URL.

36
00:05:16.390 --> 00:05:24.179
Inderjit Dhillon: There is a link to it from the canvas page over here. It says, course, web page.

37
00:05:26.330 --> 00:05:29.380
Inderjit Dhillon: Here is the unique number.

38
00:05:29.760 --> 00:05:33.489
Inderjit Dhillon: As I said, my name is Indiji Dillon.

39
00:05:34.120 --> 00:05:37.900
Inderjit Dhillon: and the ta is Dilesh Gupta. Okay?

40
00:05:38.180 --> 00:05:45.239
Inderjit Dhillon: And just you know, this is a fundamentals of machine learning course.

41
00:05:45.550 --> 00:06:07.860
Inderjit Dhillon: So you know, the one of the important things to do in the class is to cover the the breadth of machine learning. So it's not supposed to be, you know. Go deep into any one particular subject, so you can think of it as a gateway course to more advanced and specialized graduate courses on the topic of machine learning.

42
00:06:08.371 --> 00:06:15.760
Inderjit Dhillon: So you know, for example, even, you know, deep learning is one part of machine learning, we will obviously cover it in cloud.

43
00:06:47.420 --> 00:06:54.769
Inderjit Dhillon: Okay. I'm not sure what happened, but I got disconnected, huh? And then I got reconnected. That happened to other people also, or.

44
00:06:55.530 --> 00:07:00.159
Nilesh Gupta: Yeah, we were not able to see you for a while. But yeah, now we can hear. You see, you.

45
00:07:01.310 --> 00:07:08.699
Inderjit Dhillon: I see. Okay? Yeah, hopefully, you did not. Miss, too much, I guess. Let me start the sharing again.

46
00:07:13.420 --> 00:07:21.399
Inderjit Dhillon: Okay? So you know, here's the like a rough list of topics over here.

47
00:07:22.118 --> 00:07:33.780
Inderjit Dhillon: We'll talk about some, you know. Pretty basic methods. 1st least squares regression. We'll realize that we need linear algebra and probability in this course.

48
00:07:34.417 --> 00:07:38.000
Inderjit Dhillon: Linear algebra form the prerequisite for the course.

49
00:07:38.230 --> 00:07:57.730
Inderjit Dhillon: So I will actually spend one lecture going into linear algebra that we need for this course. But of course, you know, that's just covering it in one lecture. You need to have had done a course in linear algebra before. It's just going to be a review, so that it refreshes your memory and makes you realize

50
00:07:57.730 --> 00:08:13.660
Inderjit Dhillon: that you know we are going to use some amount of mathematics in this course. Then we will, you know, in addition to regression, we'll cover classification. One particular problem, which is of interest is matrix completion. We'll do that.

51
00:08:13.750 --> 00:08:20.189
Inderjit Dhillon: We'll have homework throughout the course, so there will be 4 homeworks in the course.

52
00:08:21.830 --> 00:08:46.229
Inderjit Dhillon: and many of them will be, you know, programming based to get you to understand, sort of some of the theory, but also to implement it. We look at optimization methods. We look at things like unsupervised learning, like clustering graph analysis, kernel methods. And then, of course, we will look at neural networks, deep learning architectures.

53
00:08:46.808 --> 00:09:08.019
Inderjit Dhillon: More recent innovations like transformers. We'll talk a little bit about large language models. Look at other topics in deep learning, and that's how we will kind of conclude the course I told you a little bit about so so you know, that's information you can kind of find over here.

54
00:09:08.370 --> 00:09:29.589
Inderjit Dhillon: Right? So you know, we will cover topics in supervised learning, unsupervised learning, self, supervised learning. And also, you know, obviously, deep learning, like I've told you. But the one thing that to keep in mind is, you know, the technical tools used in the course will draw from

55
00:09:29.820 --> 00:09:34.179
Inderjit Dhillon: linear algebra probability, multivariate statistics

56
00:09:34.280 --> 00:09:42.305
Inderjit Dhillon: and optimization. So you know, obviously, you don't need to be an expert in all of these topics. We do want you to have

57
00:09:43.402 --> 00:09:55.937
Inderjit Dhillon: prerequisites basics of linear algebra is the pre required prerequisite, and then the other material. We will kind of also cover in class. So

58
00:09:57.258 --> 00:10:09.441
Inderjit Dhillon: so hopefully, you know, that might fill in some of the gaps that you might have. But at the same time, you know, you do need to have a decent background in linear algebra. Okay?

59
00:10:10.400 --> 00:10:14.670
Inderjit Dhillon: let's see. So this is the course web page.

60
00:10:15.020 --> 00:10:24.620
Inderjit Dhillon: There is going to be discussion which will be carried out on Ed. Discussion over here, right and then, of course, we will use canvas, for.

61
00:10:24.730 --> 00:10:33.170
Inderjit Dhillon: you know, giving you the homework problems for homework, submission grades and submitted work, and so on. And you can find a lot of the course information over there

62
00:10:33.975 --> 00:10:38.520
Inderjit Dhillon: grading. Like, I said, there'll be 4 homeworks

63
00:10:39.299 --> 00:10:45.890
Inderjit Dhillon: each will be weighted 10% towards your overall grade. Then there will be a final project in class

64
00:10:46.230 --> 00:10:48.789
Inderjit Dhillon: which will be 30% of your grade.

65
00:10:49.398 --> 00:10:51.441
Inderjit Dhillon: We will have a midterm

66
00:10:52.080 --> 00:10:58.240
Inderjit Dhillon: which will actually be in person. So that's the one in person meeting that we will require in class.

67
00:10:58.400 --> 00:11:00.590
Inderjit Dhillon: So the midterm will be in person.

68
00:11:00.770 --> 00:11:05.424
Inderjit Dhillon: and the tentative date for the midterm is.

69
00:11:06.070 --> 00:11:09.280
Inderjit Dhillon: I think it's the Wednesday before spring break.

70
00:11:09.910 --> 00:11:15.829
Inderjit Dhillon: Okay, so please keep. Keep this in mind that you will need to do this midterm in person.

71
00:11:16.290 --> 00:11:25.109
Inderjit Dhillon: and then we'll have 5% of the grade for class participation attending the Zoom Meetings, and also participating in the lectures.

72
00:11:26.190 --> 00:11:28.610
Inderjit Dhillon: So any questions so far.

73
00:11:34.140 --> 00:11:36.890
Hormoz Shahrzad: Probably she asked the question in the chat.

74
00:11:37.820 --> 00:11:38.699
Inderjit Dhillon: Oh, it says.

75
00:11:38.800 --> 00:11:45.249
Hormoz Shahrzad: Homework and final project meant to be individual assignment, or maybe collaborate with our classmates.

76
00:11:47.661 --> 00:11:50.330
Inderjit Dhillon: So the projects will be in teams.

77
00:11:50.610 --> 00:11:56.469
Inderjit Dhillon: There'll be teams of 2 or 3. Right? We'll decide that as

78
00:11:57.070 --> 00:12:03.190
Inderjit Dhillon: we get the final enrollment numbers the homeworks. We will require you to do them by yourself.

79
00:12:06.520 --> 00:12:08.300
Inderjit Dhillon: Does that answer your question?

80
00:12:10.240 --> 00:12:11.390
Hormoz Shahrzad: Thank you. Yeah.

81
00:12:13.050 --> 00:12:15.500
Inderjit Dhillon: Oh, I see. Asked that question. We are good.

82
00:12:16.340 --> 00:12:18.110
Inderjit Dhillon: Okay? Any other questions.

83
00:12:22.320 --> 00:12:31.716
Inderjit Dhillon: Okay. So I do need everybody to kind of, you know. Acknowledge that the midterm will be in person, and that you will need to do this midterm

84
00:12:32.515 --> 00:12:36.479
Inderjit Dhillon: and it counts for 25% of your of your grade. Huh?

85
00:12:37.050 --> 00:12:40.450
Nilesh Gupta: Someone asked this question, is this for Ms student only class.

86
00:12:41.367 --> 00:12:43.240
Inderjit Dhillon: I don't know what that question means.

87
00:12:43.240 --> 00:12:43.710
Nilesh Gupta: A month.

88
00:12:43.710 --> 00:12:45.519
Inderjit Dhillon: Tested only class, I think.

89
00:12:45.520 --> 00:12:47.610
Inderjit Dhillon: No, no, Minkyo, could you?

90
00:12:47.710 --> 00:12:50.530
Inderjit Dhillon: You're here.

91
00:12:50.530 --> 00:12:51.069
Minkyu Choi: Yeah, yeah.

92
00:12:51.070 --> 00:12:52.800
Minkyu Choi: Ask me the question and not, yeah.

93
00:12:52.800 --> 00:12:53.400
Inderjit Dhillon: Way of chat.

94
00:12:53.400 --> 00:13:15.750
Minkyu Choi: Yeah, I'm I'm trying to like find the like coursework for my Phd, requirement. And I've been told that a lot of web based classes are only for the master's student, and I was not able to register it. But somehow I was registered this class, and that's how I enrolled this class. I just so I just wanna make sure that this is allowed for phd, student as well.

95
00:13:16.090 --> 00:13:27.000
Inderjit Dhillon: Yeah, this is like supposed to be like a regular. It doesn't actually have to do with our online master's program. This is a regular machine learning class which is just being offered

96
00:13:28.058 --> 00:13:30.429
Inderjit Dhillon: in a web based way.

97
00:13:30.600 --> 00:13:35.339
Inderjit Dhillon: So I believe, I believe that you know many of the students are who are here.

98
00:13:35.820 --> 00:13:51.759
Inderjit Dhillon: Some of them are Phd students, some might someone, some might be master students. There also have been a couple of undergraduates who have asked me for permission to take this course, and be, if you have the prerequisite. You know, I've allowed a couple of undergraduates to join this course.

99
00:13:52.490 --> 00:13:56.180
Minkyu Choi: Does that answer the question? Thank you. Or Yes, thank you.

100
00:13:59.570 --> 00:14:03.390
Inderjit Dhillon: Okay, any other questions.

101
00:14:07.380 --> 00:14:10.850
Inderjit Dhillon: Okay, so go ahead.

102
00:14:10.850 --> 00:14:21.330
Prachi Ingle: For the in person midterm. Will there be like a place in time where we all take it together? Or do we need to arrange some kind of testing center where.

103
00:14:21.330 --> 00:14:32.349
Inderjit Dhillon: No, no, no, I will not make you jump through those hoops. We will basically find a room on campus, and you will all be required to come there and take the test over there.

104
00:14:32.540 --> 00:14:34.490
Prachi Ingle: Alright! Thank you for the clarification.

105
00:14:34.490 --> 00:14:37.386
Inderjit Dhillon: Yeah. Yeah. So maybe you know, maybe

106
00:14:38.722 --> 00:14:40.687
Inderjit Dhillon: dilish, maybe just send out

107
00:14:41.552 --> 00:14:45.540
Inderjit Dhillon: an email to everybody saying that there will be an in-class midterm.

108
00:14:45.660 --> 00:14:50.799
Inderjit Dhillon: the tentative date, and just clarify that, you know, we'll arrange a room for that material.

109
00:14:52.620 --> 00:14:53.680
Inderjit Dhillon: Sounds good.

110
00:14:54.660 --> 00:14:57.243
Inderjit Dhillon: Okay? So you know, let's

111
00:14:57.890 --> 00:15:02.159
Inderjit Dhillon: go on to look at more of the material. So

112
00:15:02.270 --> 00:15:07.190
Inderjit Dhillon: you know, there's a tentative schedule for this course. You know I don't.

113
00:15:07.776 --> 00:15:25.260
Inderjit Dhillon: We might change it around a little bit, but that is the tentative schedule my style of teaching is that I will, even though we are you know, doing this online, I will basically write everything out on my ipad

114
00:15:25.500 --> 00:15:48.799
Inderjit Dhillon: and share your the screen. So it will be like, you know, you being able to see the material as it is going on. Of course, some of the material I might teach using slides, so some will be kind of like handwritten. Some will be slides at the end of each lecture. I we will upload the videos

115
00:15:48.960 --> 00:15:58.289
Inderjit Dhillon: for the class, and also the handwritten notes, so that you actually have both with you, as you try to understand the material.

116
00:15:58.560 --> 00:16:01.310
Inderjit Dhillon: Was there another chat message, or

117
00:16:02.560 --> 00:16:05.109
Inderjit Dhillon: sorry? I just saw a notification.

118
00:16:06.690 --> 00:16:08.150
Inderjit Dhillon: There's no chat. Right?

119
00:16:08.360 --> 00:16:10.535
Inderjit Dhillon: Okay? Okay, so

120
00:16:12.730 --> 00:16:21.860
Inderjit Dhillon: and you know. So you can just look at the tentative schedule. We'll talk about regression in particular, you know, classical, least squares regression.

121
00:16:22.030 --> 00:16:28.439
Inderjit Dhillon: We will see that as we start getting into it, we start getting into some mathematics.

122
00:16:29.400 --> 00:16:33.660
Inderjit Dhillon: some multi weighted calculus, some linear algebra.

123
00:16:34.060 --> 00:16:38.140
Inderjit Dhillon: so that motivates kind of the linear algebra review.

124
00:16:39.310 --> 00:16:46.500
Inderjit Dhillon: In the next lecture we'll talk about things like singular value decomposition, which is kind of a cornerstone

125
00:16:46.670 --> 00:17:00.099
Inderjit Dhillon: method in linear algebra. See how it relates to regression. We talk about regularization techniques in classical regression, such as ridge regression plus. So we look at a very interesting

126
00:17:00.692 --> 00:17:22.359
Inderjit Dhillon: example in matrix completion. Which was basically like, you know, there was a recommender system competition held years ago, and which predated deep learning. And that's where you know, this problem came up very rich in theory and also in practice.

127
00:17:23.041 --> 00:17:43.688
Inderjit Dhillon: Then we'll switch gears to start talking about classification before that I will do a probability theory review. So that, you know, for example, Bayes theorem, we'll talk about different approaches to classification regression approaches logistic regression. We'll talk about this expected risk minimization framework, which is,

128
00:17:44.447 --> 00:18:03.680
Inderjit Dhillon: kind of very important topic in machine learning. And then we will. You know, once we talk a little bit. These, I think I may change the order of some of these lectures. We'll talk a little bit about support vector machines. We look at optimization theory for support vector machines, how there is duality.

129
00:18:04.117 --> 00:18:19.432
Inderjit Dhillon: And that'll give us an understanding about these classical machine learning methods. We'll talk about kernel Svms, and then we'll start talking about deep learning. We'll talk about topics in

130
00:18:22.150 --> 00:18:28.679
Inderjit Dhillon: unsupervised learning. Also, like Pca. Autoencoders

131
00:18:29.357 --> 00:18:49.039
Inderjit Dhillon: talk a little bit about clustering, and then we will also then move on to, you know, the more recent or deep learning architectures, Mlps, Rnns, and the more recent Transformer models. The way that self supervised learning is done

132
00:18:49.485 --> 00:19:09.354
Inderjit Dhillon: to. For example, develop the latest you know, large language model, and so on. So you know we will. And then at some point of time. So let's see, over here is the tentative midterm. So it's March 12, th which is just before the spring break.

133
00:19:10.040 --> 00:19:14.769
Inderjit Dhillon: and so we want to do the exam a little bit earlier Sean in this course.

134
00:19:14.920 --> 00:19:35.390
Inderjit Dhillon: And then the last part of the course is, of course, lectures. And also I want the students to concentrate on their class projects so hopefully, we will kind of decide on the projects and give you like, you know, a little over about a month at least to do these projects

135
00:19:36.283 --> 00:19:40.410
Inderjit Dhillon: and then we'll have project presentations towards the end of the course.

136
00:19:43.860 --> 00:19:51.490
Inderjit Dhillon: Let me see, Staff, you all I already told you about us. There's a calendar.

137
00:19:51.760 --> 00:20:00.139
Inderjit Dhillon: and I think you guys should have all the calendar invites right? So there's a link to the zoom link over here

138
00:20:00.430 --> 00:20:01.870
Inderjit Dhillon: and then

139
00:20:04.040 --> 00:20:31.759
Inderjit Dhillon: books. So we we are not going to use any. There's no required textbook to this course. But there are various books that you can use for. If you want to go deeper into a material, you're also welcome to ask me if there's a particular topic. But generally my courses my lectures will be self contained in the sense that you know my handwritten notes will be available to you. My video lectures will be available to you.

140
00:20:31.770 --> 00:20:37.450
Inderjit Dhillon: But of course, if you want more detailed reading, then you can look at some of these books.

141
00:20:37.870 --> 00:20:40.290
Inderjit Dhillon: I guess you know we should. Yeah.

142
00:20:43.410 --> 00:20:47.032
Inderjit Dhillon: okay, okay, I think that's about it.

143
00:20:47.770 --> 00:21:03.244
Inderjit Dhillon: that I have. So I think I do want the class. I'm hoping that, you know, even though it is a zoom class that we can be a little bit interactive in this class. I encourage people to kind of speak up. So why don't we go

144
00:21:03.560 --> 00:21:09.830
Nilesh Gupta: Sorry to interrupt. There's this homework 0 that we have like the initial survey form.

145
00:21:10.110 --> 00:21:15.400
Inderjit Dhillon: Yeah, I was gonna talk to people once we do the kind of round of introductions.

146
00:21:15.680 --> 00:21:16.510
Nilesh Gupta: Sounds good.

147
00:21:16.510 --> 00:21:17.320
Inderjit Dhillon: Okay.

148
00:21:18.090 --> 00:21:27.239
Inderjit Dhillon: So I'm gonna stop sharing over here. So I'm now going to actually turn the floor over to you. I've talked a little bit, you know, and you'll

149
00:21:27.650 --> 00:21:49.119
Inderjit Dhillon: spend the majority of the class listening to me. So. But today I just want to hear from each one of you like. Maybe just give your name, your Major, whether you're a master undergraduate master's or Phd. Student. And then just one line about why you're taking this course.

150
00:21:51.180 --> 00:21:53.540
Inderjit Dhillon: So how should we do this?

151
00:21:56.340 --> 00:21:59.371
Inderjit Dhillon: I have a zoom screen with list of

152
00:22:01.214 --> 00:22:05.330
Inderjit Dhillon: participants. So I can just call out people

153
00:22:05.430 --> 00:22:08.119
Inderjit Dhillon: in the list that I have. If that's okay.

154
00:22:10.165 --> 00:22:11.310
Inderjit Dhillon: Hormones.

155
00:22:12.177 --> 00:22:17.549
Hormoz Shahrzad: Hi, this is Hormuz Shazad. I'm a Phd student, computer science

156
00:22:18.296 --> 00:22:34.609
Hormoz Shahrzad: and working at cognizant right now. But yeah. And the reason I'm getting that course. I'm mostly working on evolutionary computation and neuro evolution. But then machine learning is always.

157
00:22:34.810 --> 00:22:47.959
Hormoz Shahrzad: you know, something that you need. You know, to have a stronger you know, background. And you know, be deep in fundamental. So that is

158
00:22:48.450 --> 00:22:49.850
Hormoz Shahrzad: that is why I'm here.

159
00:22:50.230 --> 00:22:53.379
Inderjit Dhillon: Sounds good, and I guess I'm guessing you are restore student.

160
00:22:53.380 --> 00:22:53.950
Hormoz Shahrzad: Yes.

161
00:22:54.204 --> 00:22:58.775
Inderjit Dhillon: Yeah, okay, great. I think he had. I had met him at neurops, and he had told me

162
00:22:59.210 --> 00:23:02.129
Inderjit Dhillon: that some of the students might be taking this course.

163
00:23:02.490 --> 00:23:03.570
Hormoz Shahrzad: Yeah, he mentioned that.

164
00:23:04.410 --> 00:23:04.930
Hormoz Shahrzad: Yeah.

165
00:23:05.130 --> 00:23:13.709
Inderjit Dhillon: Okay, great welcome, Hormuth. And I'm sorry if I mispronounce your name. You know this happens to me all the time. My name gets mispronounced.

166
00:23:14.030 --> 00:23:21.360
Inderjit Dhillon: so please feel free to correct me if I am butchering your name and give me the right pronunciation.

167
00:23:22.378 --> 00:23:24.719
Inderjit Dhillon: Okay, next up is Abhiram.

168
00:23:27.540 --> 00:23:45.510
Abhiram Maddukuri: Hello, everyone! I'm Abiram. I'm a 5 year master's student so, and I guess one reason I'm taking it is like I took the undergrad in machine learning. But I also want just like a better mathematical foundation of like some of the machine learning algorithms. And I think this is a good class for that.

169
00:23:46.820 --> 00:23:51.360
Inderjit Dhillon: Okay, welcome up here on next. We have Ananya.

170
00:23:54.330 --> 00:24:06.009
Ananya G M: Hi, I'm Ananya. I am a Phd student. I work in human computer interaction field. The reason I'm taking this class is to just provide some of the basic concepts that I've

171
00:24:06.280 --> 00:24:09.689
Ananya G M: learn in a long time. Now. So yeah.

172
00:24:10.260 --> 00:24:16.700
Inderjit Dhillon: Okay, nice welcome, Ananya. Next we have.

173
00:24:17.060 --> 00:24:24.780
Inderjit Dhillon: Wait this. Names actually keep us moving a little bit. So we just went. We just did, Abhiram, right? So we don't need to

174
00:24:25.250 --> 00:24:29.223
Inderjit Dhillon: go to Abhiram again. So Anshu Anshul muntra.

175
00:24:30.950 --> 00:24:40.280
Anshul Moondra: Hello! I'm anshul. I'm a 5th year, integrated master's student, and I guess I've just enjoyed machine learning from undergrad and want to learn more.

176
00:24:41.310 --> 00:24:42.550
Inderjit Dhillon: Okay, sounds good.

177
00:24:44.490 --> 00:24:49.699
Inderjit Dhillon: Next we have 0, 1, 2, a tariff.

178
00:24:51.010 --> 00:25:07.979
Atharva M Kalamkar: I'm Atharva. I'm a 4th year, integrated master student. So it's technically still an undergrad and yeah, I'm taking this course because I mean, I want to work kind of in this industry. I'm already working in the industry and kind of wanted some experience with Ml, and yeah.

179
00:25:08.330 --> 00:25:11.939
Inderjit Dhillon: Okay, tell me more about when you say you're already working in industry or.

180
00:25:11.940 --> 00:25:35.319
Atharva M Kalamkar: Oh, well, yeah. So I'm doing. I'm currently with spectrum. Right now, I'm kind of in the data science area. And we do some Nlp work. And then eventually, I think next year I'm going to be like at Toyota with doing some more data science work. So I think, having that like math foundation is important, because so far, I've just had like that, you know, you just go in and code it up. But you don't really like understand the

181
00:25:35.720 --> 00:25:38.390
Atharva M Kalamkar: the fundamentals of it, so.

182
00:25:38.700 --> 00:25:47.660
Inderjit Dhillon: Okay, okay, great Let's see, Pargo, I think, is next.

183
00:25:52.220 --> 00:26:05.170
Bhargav Samineni: Yeah, Hi, I'm Bargav. I'm doing. My master's in Cs. I guess the reason I'm taking this course is kind of interested in like the theory behind machine learning. So I figured this would be like a good introduction to learn about all that.

184
00:26:06.230 --> 00:26:27.369
Inderjit Dhillon: Okay, great, that's good. This is all music to my ears. By the way, right? So next time onward we'll roll up our sleeves and we'll basically be doing a lot of mathematics right related to machine learning. So all of you have said that you so far have said that you want a mathematical or theoretical introduction. So that's what I aim to give in this course.

185
00:26:28.530 --> 00:26:30.780
Inderjit Dhillon: Chloe is next

186
00:26:35.650 --> 00:26:36.799
Inderjit Dhillon: employee. Chan.

187
00:26:48.050 --> 00:26:49.060
Inderjit Dhillon: play.

188
00:26:57.090 --> 00:26:58.400
Inderjit Dhillon: Okay?

189
00:27:01.990 --> 00:27:08.830
Inderjit Dhillon: Okay? And and you guys, just make a note that we'll try to revisit Chloe at the end.

190
00:27:09.410 --> 00:27:09.960
Nilesh Gupta: Okay.

191
00:27:11.110 --> 00:27:18.990
Inderjit Dhillon: And then I'm sorry. I think I'm going to butcher the next name call you up.

192
00:27:19.440 --> 00:27:21.270
Inderjit Dhillon: or is it Choleon?

193
00:27:21.270 --> 00:27:25.219
Cholyeon Cho: Oh, yeah, Hi, it always happens. It's it's Cholian and.

194
00:27:25.783 --> 00:27:26.909
Inderjit Dhillon: Okay, yeah.

195
00:27:27.450 --> 00:27:30.020
Inderjit Dhillon: At least my second attempt was a closer right? Yeah.

196
00:27:30.469 --> 00:27:37.209
Cholyeon Cho: Yeah, I mean, yeah, I'm a, I'm also a master's students in Cs, and

197
00:27:37.570 --> 00:27:42.030
Cholyeon Cho: yeah, I don't think I don't think anything I wanted to mention like

198
00:27:42.350 --> 00:27:50.049
Cholyeon Cho: wasn't mentioned beforehand. So I'm also interested in the mathematical foundations of machine learning. And that's why I kind of

199
00:27:50.270 --> 00:27:52.000
Cholyeon Cho: opted to take this class.

200
00:27:52.640 --> 00:27:55.413
Inderjit Dhillon: Okay, okay, great. Thank you for holding on.

201
00:27:56.500 --> 00:27:57.609
Inderjit Dhillon: Is that good?

202
00:27:58.480 --> 00:27:59.150
Cholyeon Cho: Yeah.

203
00:28:00.058 --> 00:28:02.440
Inderjit Dhillon: Not really? Why don't you say it? One more time.

204
00:28:05.213 --> 00:28:06.720
Cholyeon Cho: Cholian, so.

205
00:28:06.720 --> 00:28:14.690
Inderjit Dhillon: Julian. Okay? Well, I'll try. Okay. Thanks, Julian. Now, I think we have Duane.

206
00:28:15.760 --> 00:28:16.280
Dewayne Benson: I already have.

207
00:28:16.280 --> 00:28:17.130
Inderjit Dhillon: Dwayne, Benson.

208
00:28:17.130 --> 00:28:46.339
Dewayne Benson: That's me. I am a master's student. So I'm actually in the 5th year of the Integrated Masters. I'm actually a grad student. I'm also a rhetoric major. So yeah, I'm kind of like interdisciplinary. I will say that ignorance is bliss, but knowledge is power. So I kind of wish I knew less computer science. This is really ugly at the grad level. But I'm taking this class just to learn more and see what is cooking at the grad level for machine learning is really foundational. So I'm just hoping to get more out

209
00:28:46.510 --> 00:28:48.169
Dewayne Benson: in that area.

210
00:28:48.550 --> 00:28:52.040
Inderjit Dhillon: Okay. Sounds good. Okay. Awesome. Welcome. Dwayne.

211
00:28:53.679 --> 00:28:54.759
Inderjit Dhillon: Dinesh.

212
00:28:57.504 --> 00:29:17.690
Dinesh Keserla: Hi, yeah, that's me. Hi, I'm dinesh, I'm also in the integrated master's program. But I'm in my 1st year, and yeah, like like Abiram, I did machine learning at the undergrad level. But I want to kind of sharpen the fundamentals as I go into like the second year of the integrated masters, and

213
00:29:17.850 --> 00:29:19.559
Dinesh Keserla: like learn more

214
00:29:19.690 --> 00:29:26.339
Dinesh Keserla: a fundamental level. So when I go into other machine learning classes, I have a better foundation, so to speak.

215
00:29:26.960 --> 00:29:30.270
Inderjit Dhillon: Okay. Okay, welcome dinesh?

216
00:29:30.910 --> 00:29:32.760
Inderjit Dhillon: Then we have Eddie.

217
00:29:36.340 --> 00:29:43.549
Eddie: Hey? My name's Eddie. Also, in the 5 Year Masters. I'm in the final year. I am a computer science major, and

218
00:29:43.610 --> 00:30:08.420
Eddie: I mostly wanted to take this course. I recently took the undergrad machine learning course. So just also wanted to get some more practice with the fundamentals and then possibly go into some new topics that we didn't cover there. And then, as well as I'm job searching. So possibly expanding to not just soft. I'm currently looking for software engineering, but possibly trying out data science roles. In the job search. So yeah.

219
00:30:09.840 --> 00:30:11.399
Inderjit Dhillon: Okay, welcome, Eddie.

220
00:30:12.740 --> 00:30:18.580
Inderjit Dhillon: Next is on my list is I'll rep.

221
00:30:20.910 --> 00:30:22.260
Inderjit Dhillon: I'll run new.

222
00:30:22.600 --> 00:30:43.130
Haoran Niu: Hello, my name is Horan, and I'm a Phd student in Ece department. So I'm taking this course because machine learning is one of the most important parts of my research about identity system. And I'm also taking the course to make my schedule have a regular time to learn things.

223
00:30:43.720 --> 00:30:45.000
Inderjit Dhillon: Okay. Okay.

224
00:30:45.220 --> 00:30:46.720
Inderjit Dhillon: Welcome, Haran.

225
00:30:48.567 --> 00:30:50.510
Inderjit Dhillon: Next I have.

226
00:30:52.260 --> 00:30:55.280
Inderjit Dhillon: You meant Joseph Joseph Stanley.

227
00:30:58.540 --> 00:31:11.159
Joseph Stanley: Yeah, I'm just a Stanley. I'm a 1st year master's student in computer science. And I'm taking this course to learn more about the math mathematical background for a lot of these machine learning algorithms and techniques.

228
00:31:11.600 --> 00:31:12.990
Inderjit Dhillon: Okay. Okay.

229
00:31:13.680 --> 00:31:20.019
Inderjit Dhillon: Awesome. Joseph. Welcome then we have Louis Louis Luna.

230
00:31:21.490 --> 00:31:37.350
Luis Luna: Hi, everyone. Sorry I don't have a camera. My computer doesn't come with one, but I'm a 1st year master's student in the mechanical engineering department. And the reason I'm taking this course is because I am hoping to one day become a robotic software engineer, and I believe this course will help me in that path.

231
00:31:38.560 --> 00:31:46.059
Inderjit Dhillon: Okay, okay. Nice to hear from you, Louis. Now I know that you did not somehow have the zoom link. Right? So.

232
00:31:46.260 --> 00:31:49.629
Inderjit Dhillon: And but we emailed it to you before the class.

233
00:31:49.880 --> 00:31:52.020
Inderjit Dhillon: Do do you know why you didn't have the zoom link.

234
00:31:52.020 --> 00:32:03.700
Luis Luna: Yeah. So the reason so the reason was, because, I'm not from the Cs department. And so I wasn't able to register until later, when I registered, I didn't have access to the canvas. So I didn't know

235
00:32:04.116 --> 00:32:11.220
Luis Luna: where to get the zoom link from. But I Nilesh already emailed me, able to get the zoom link from him.

236
00:32:11.220 --> 00:32:15.280
Inderjit Dhillon: But you are. You are enrolled in the course, and you don't need any more help right.

237
00:32:15.280 --> 00:32:19.800
Luis Luna: I am enrolled in the course. And I I just checked, and I'm

238
00:32:20.200 --> 00:32:24.679
Luis Luna: also enrolled or canvas. The canvas page shows up already on my end.

239
00:32:24.680 --> 00:32:32.300
Inderjit Dhillon: Okay. Awesome. Okay? Good. Good. Okay. Sure. You're welcome, Louis, and then we have. Minkyu.

240
00:32:36.590 --> 00:33:01.789
Minkyu Choi: Hello! My name is Minty Choi. I'm a Phd. Student from Ece department as well. I'm working on my thesis on neuro symbolic AI taking this class because I'm also taking statistical machine learning from Ece Department, and I'm looking for some synergy and having a good like forming a good foundation in machine learning, and AI in general. So, looking forward to learning a lot of things.

241
00:33:02.020 --> 00:33:06.299
Inderjit Dhillon: And who's teaching that? Statistical machine learning course in the Ce.

242
00:33:07.750 --> 00:33:11.189
Minkyu Choi: Pop, pop yellow. Sorry. I.

243
00:33:11.630 --> 00:33:12.590
Inderjit Dhillon: Sorry. Who's that?

244
00:33:13.725 --> 00:33:15.550
Minkyu Choi: Let me look for it.

245
00:33:15.550 --> 00:33:20.800
Inderjit Dhillon: Okay, no worries. I thought you had that. I guess today is kind of the 1st day of class. So.

246
00:33:21.200 --> 00:33:23.460
Minkyu Choi: You might not have gone to that class, probably. Right?

247
00:33:23.460 --> 00:33:26.650
Minkyu Choi: Oh, it's a Professor Bicolo Harris, bical.

248
00:33:26.860 --> 00:33:28.799
Inderjit Dhillon: Okay, we call Harvest. Okay?

249
00:33:28.800 --> 00:33:29.310
Minkyu Choi: Yeah.

250
00:33:29.310 --> 00:33:32.993
Inderjit Dhillon: Nice. Okay, thank you.

251
00:33:34.220 --> 00:33:38.679
Inderjit Dhillon: thank you, Minkyu. Welcome. And then we have Mir Mohammed.

252
00:33:48.820 --> 00:34:12.330
Mir Mohammad Shamszadeh: Hello, my name is Mir Mohamed, or you can just go by. Mir. I am a Phd student in civil engineering department. I'm taking this course. Because basically, my whole project is about applications of AI in a problem in structural monitoring of bridges, particularly computer vision.

253
00:34:12.429 --> 00:34:19.830
Mir Mohammad Shamszadeh: So that's why. And I'm also looking for some more mathematical foundations of machine learning. That's why.

254
00:34:20.400 --> 00:34:22.429
Inderjit Dhillon: Okay, okay, welcome, Mir.

255
00:34:23.141 --> 00:34:28.770
Inderjit Dhillon: And then next, well, maybe I'll go back to Chloe to see if Chloe is around.

256
00:34:30.389 --> 00:34:31.189
Chloe Chen: Pipe.

257
00:34:32.040 --> 00:34:32.900
Inderjit Dhillon: Hi Chloe!

258
00:34:33.120 --> 00:34:40.149
Chloe Chen: Hi, yeah. Sorry about that. I there's something up with my laptop and yeah.

259
00:34:40.520 --> 00:34:43.370
Chloe Chen: But I think we were doing introductions.

260
00:34:44.159 --> 00:34:55.989
Inderjit Dhillon: Yes, so introduction just everybody saying, you know whether they are a undergraduate student, master's student, Phd. Student in what department? And then one sentence about why they want to take this course.

261
00:34:56.800 --> 00:35:10.969
Chloe Chen: Okay, I am a Phd student. This is my 3rd year. I'm in the Cs department. My research is mostly with neural networks. So that's why I'm looking to understand this a little more, and taking this class.

262
00:35:12.030 --> 00:35:16.550
Inderjit Dhillon: Sounds good. And who's your Supervisor? And.

263
00:35:16.550 --> 00:35:21.089
Chloe Chen: I'm working with bristel overdue store. Okay? Right?

264
00:35:21.610 --> 00:35:22.550
Inderjit Dhillon: Awesome.

265
00:35:23.360 --> 00:35:28.190
Inderjit Dhillon: Okay, thank you, Chloe. Welcome. And then next we have. Prachi.

266
00:35:29.700 --> 00:35:44.509
Prachi Ingle: Hi! I'm prachi. I'm in my 4th year undergrad slash 1st Years masters as part of the integrated program. And I'm also really interested in learning the math behind machine learning. I liked math in undergrad, so I'm excited to see how it applies to the subject.

267
00:35:45.110 --> 00:35:48.079
Inderjit Dhillon: Awesome, awesome welcome, prachi.

268
00:35:48.380 --> 00:35:50.679
Inderjit Dhillon: Next we have prana.

269
00:35:53.920 --> 00:35:57.249
Pranav Venkatesh: Hi, my name is Prennov. I'm a master's student

270
00:35:57.580 --> 00:36:02.170
Pranav Venkatesh: taking this class to better understand the fundamentals and the underlying math.

271
00:36:03.590 --> 00:36:07.410
Inderjit Dhillon: Okay, great welcome, Prano.

272
00:36:08.330 --> 00:36:10.270
Inderjit Dhillon: Then we have Ritesh.

273
00:36:13.296 --> 00:36:14.129
Ritesh Thakur: Hey, guys?

274
00:36:14.609 --> 00:36:20.300
Ritesh Thakur: Yeah, I'm also in the 5 year Integrated master's program my answer. I'm a 4th year right now.

275
00:36:20.600 --> 00:36:30.540
Ritesh Thakur: And I also took this class to understand the mathematical foundations better. I took the undergrad one, and it covered. It gave me like broad exposure, and I want to dive deeper.

276
00:36:31.810 --> 00:36:36.350
Inderjit Dhillon: Okay. Great welcome.

277
00:36:37.635 --> 00:36:40.720
Inderjit Dhillon: Then I think next I have Tian Cenk.

278
00:36:47.110 --> 00:37:00.159
Tiancheng Xiao: Right. Hello! My name's engine and my 1st year, Ec, master student. So the reason I'm taking this course is because I want to have refresh my memory on machine learning, and maybe dive deeper into the optimization. Deep learning, part.

279
00:37:01.030 --> 00:37:03.369
Inderjit Dhillon: Okay, good. Welcome.

280
00:37:07.830 --> 00:37:12.300
Inderjit Dhillon: Welcome to the engine. Now we have rigid.

281
00:37:19.256 --> 00:37:37.580
Wei-Jie Huang: Hello! My name is Weijay. I'm a second year master student with Cs department, and I'm also I've also taken on a grad level. Ml, course before, and I just wanna have a recap on the foundations in Ml.

282
00:37:39.050 --> 00:37:43.389
Inderjit Dhillon: Good. Okay, welcome, Bridget.

283
00:37:43.850 --> 00:37:48.130
Inderjit Dhillon: And then I think it's the last z yank.

284
00:37:53.060 --> 00:38:08.239
Ziyang Tan: Hi, everyone. I'm Jiang, and I'm a 1st year master's student in Cs. I choose this class because I want to build a stronger foundation in Ml. And its mass concept application more systematically.

285
00:38:08.650 --> 00:38:09.570
Ziyang Tan: Thanks.

286
00:38:09.830 --> 00:38:17.350
Inderjit Dhillon: Okay, okay, welcome the Yank. I think I missed Roque.

287
00:38:19.760 --> 00:38:21.360
Inderjit Dhillon: Okay, Zain.

288
00:38:21.360 --> 00:38:38.340
Ruoke Zhang: Yeah, I'm here. Hi, my name is the 1st year master's student in computer science. And the reason that I'm taking this course is because I'm really interested in machine learning. So I just want to learn more stuff in this course.

289
00:38:38.730 --> 00:38:39.320
Ruoke Zhang: Thanks.

290
00:38:39.320 --> 00:38:47.240
Inderjit Dhillon: Okay, great. Thank you. Okay. Okay. I hope. Well, let me know if I did not call your name.

291
00:38:48.160 --> 00:38:55.130
Inderjit Dhillon: I tried to go through this. But the the order of participants actually, I think, keeps on changing a little bit on me. So

292
00:38:55.800 --> 00:38:57.510
Inderjit Dhillon: anybody I did not call.

293
00:39:02.550 --> 00:39:04.019
Inderjit Dhillon: Okay? Sounds good.

294
00:39:05.566 --> 00:39:31.513
Inderjit Dhillon: So okay. So now, you also all know, like, what is the kind of you know? I, I saw that there are a lot of people who are in the integrated master's program, who are taking this course. And then there are some Phd. Students from Cs. From Ece, from a couple of other departments. All of you have said, or many of you have said, that you are looking for kind of foundations of machine learning, and

295
00:39:32.180 --> 00:39:40.490
Inderjit Dhillon: especially mathematical foundations. That is definitely what will be taught in this class.

296
00:39:42.062 --> 00:40:04.409
Inderjit Dhillon: I will really dive deep into the mathematics almost right away right? So and that is to really kind of let you know how this course will proceed. And so, if you have any kind of difficulty, or so feel free to approach me or the ta. Whether this is kind of the right course for you or not.

297
00:40:06.530 --> 00:40:17.390
Inderjit Dhillon: and then I think one team that also came up quite a bit when I listened to you is that several of you have taken kind of an undergraduate course in machine learning.

298
00:40:17.530 --> 00:40:28.760
Inderjit Dhillon: So maybe some of you who have taken that course can tell me a little bit about. Maybe you know just the textbook that was followed of no textbook. If it was whether there was no textbook followed.

299
00:40:30.290 --> 00:40:42.590
Inderjit Dhillon: And I'm guessing that that undergraduate machine learning course was in ut, because I think several of you who said that they've taken the undergrad machine learning course are in the master's program.

300
00:40:43.190 --> 00:40:47.060
Inderjit Dhillon: So anybody wants to volunteer what book was used or.

301
00:40:47.920 --> 00:40:57.440
Anshul Moondra: It doesn't really follow with a book. But they just talk about a lot of different classification, clustering and association analysis techniques.

302
00:40:58.470 --> 00:40:58.969
Inderjit Dhillon: Oh, I see!

303
00:40:58.970 --> 00:41:00.370
Anshul Moondra: Algorithms behind that.

304
00:41:01.100 --> 00:41:04.560
Inderjit Dhillon: Okay. So it did not actually follow any particular textbooks.

305
00:41:04.560 --> 00:41:05.190
Anshul Moondra: No.

306
00:41:06.180 --> 00:41:11.059
Inderjit Dhillon: Okay? And was that the course talked@at ut, or.

307
00:41:11.060 --> 00:41:13.490
Anshul Moondra: Yes, it's the one with Professor Beasley.

308
00:41:13.490 --> 00:41:21.669
Inderjit Dhillon: Professor Beasley. Okay, that's what I thought, anybody have a different undergraduate machine learning course than the one that was just mentioned.

309
00:41:28.860 --> 00:41:29.470
Inderjit Dhillon: Okay.

310
00:41:30.870 --> 00:41:57.200
Inderjit Dhillon: okay, so thank you for all, all for kind of participating in this round, like I said, you know, most of the future lectures will be really me teaching, and I'm hoping that you will try to participate in class. As I write things you're welcome to ask me clarifications, or in what I'm doing in class. So hopefully.

311
00:41:57.250 --> 00:42:09.770
Inderjit Dhillon: you know, even though the course is online, I can actually have participation in the class, right? It makes me feel better when I realize that you know I'm not just speaking into a vacuum, and that people are actually listening.

312
00:42:10.297 --> 00:42:39.460
Inderjit Dhillon: And then, you know, today, I'm actually not really going to teach any material. There's only one last thing that we wanted to do which was, now that you've kind of given me. I have a sense of what the class composition is like, and I would like to know a little bit more. So. There's a particular survey which is like homework. 0. No, there's no points attached to doing the survey, or maybe there is no well, no.

313
00:42:39.720 --> 00:42:46.140
Inderjit Dhillon: or maybe I'll keep you guessing but please everybody should fill out the survey.

314
00:42:46.733 --> 00:42:51.460
Inderjit Dhillon: Nilash. Is there a where is the link that people can find out.

315
00:42:51.620 --> 00:43:00.250
Nilesh Gupta: Yeah, it's on the course web page. If they go to homeworks I'll post a link here as well, and I'll post an Ed announcement.

316
00:43:01.930 --> 00:43:16.030
Inderjit Dhillon: Okay, sounds good. Okay? So actually, that's it for this particular class. I'll give you half an hour of your time back. But next time we will start earnestly, start talking about regression

317
00:43:16.080 --> 00:43:32.479
Inderjit Dhillon: and see how it leads to. You know some interesting things about in multiple. So also linear algebra. And we'll see how we can solve that problem

318
00:43:32.710 --> 00:43:49.240
Inderjit Dhillon: in multiple ways, using using linear algebra. And then the class after that for the next class, I will kind of say, Hey, there's a need for you guys to know your linear algebra, and then I will do one review course

319
00:43:49.390 --> 00:43:54.270
Inderjit Dhillon: in linear algebra, because I realize, having taught the course, for

320
00:43:54.420 --> 00:44:13.279
Inderjit Dhillon: you know over 20 years that even though many of you have taken linear algebra, either you have forgotten and not realize how it could be useful in a topic like machine learning. Okay, so that's the plan. Concrete plan for the next 2 classes. And we will proceed from there.

321
00:44:13.390 --> 00:44:17.690
Inderjit Dhillon: Okay, let me know if there are any final questions, and

322
00:44:18.200 --> 00:44:21.229
Inderjit Dhillon: if not, then we will conclude this class.

323
00:44:21.880 --> 00:44:23.300
Dewayne Benson: Is this class hard.

324
00:44:25.100 --> 00:44:36.219
Inderjit Dhillon: You know, I don't know how to answer that question. It's all I think sometimes relative, right? But there will definitely be math in it right? Quite a lot of math

325
00:44:36.350 --> 00:44:38.159
Inderjit Dhillon: if you enjoy maths

326
00:44:38.560 --> 00:44:46.010
Inderjit Dhillon: it like, I said. It depends right. It may not be hard for somebody who enjoys math, but you know I teach the material as I teach it.

327
00:44:46.546 --> 00:44:54.389
Inderjit Dhillon: I do have a a list of prerequisites, so if you don't have the prerequisites, yes, the car class could actually be quite hard.

328
00:44:55.570 --> 00:44:57.236
Inderjit Dhillon: Right? So

329
00:45:01.720 --> 00:45:04.694
Inderjit Dhillon: relationship should we just post like

330
00:45:05.940 --> 00:45:09.405
Inderjit Dhillon: lecture some of the lecture notes from last year

331
00:45:10.300 --> 00:45:24.073
Inderjit Dhillon: so that people can actually see ahead to see what material we will be covering for, let's say, 4 or 5 lectures, so that I don't know. But is there a particular time that you need to make a decision about the course like Drop

332
00:45:25.530 --> 00:45:28.119
Inderjit Dhillon: like, not taking the course, and so on, or.

333
00:45:28.760 --> 00:45:40.410
Dewayne Benson: Oh, no, I was just asking, just like I'm committed to take the course. Whether, like I fail or I pass, I'm in it for like in for a penny in for a pound right? But I was just curious to get your perspective, like I was just just question I'd like to ask.

334
00:45:40.890 --> 00:45:50.789
Inderjit Dhillon: Yeah, I mean, look, I again, like, you know, is the question, is the class hard is relative? Right? I can't really answer the question. It is going to be mathematical.

335
00:45:51.233 --> 00:46:00.959
Inderjit Dhillon: Like, for example, when I talk about support vector machines, I will not draw. I will not just draw a couple of pictures of 2 straight lines and a bunch of points

336
00:46:01.564 --> 00:46:30.169
Inderjit Dhillon: but I will actually go through the mathematics about how what that means. And I found that actually can benefit students quite a lot. So you know, I'm just trying to give you a good foundation. But I was also like going to say that, you know. If you, for example, find the course hard, there's no harm in saying that, hey? You know the courses right now. Hard for me to take. Let me take the prerequisites first, st and then come back and take a take this course at a later time.

337
00:46:30.390 --> 00:46:44.590
Inderjit Dhillon: So you know, I think you guys all have the option of like dropping the course till a particular point. So there is no point in taking a course that you find too hard right? I mean, maybe it'll actually be very easy for some of you. I don't know.

338
00:46:44.700 --> 00:47:08.419
Inderjit Dhillon: but I just want to like kind of let you know that you know you should, you should do what you feel is best for you. So you have various options, and all I was trying to say is that, you know, we could post some of the, and that's why I'll actually dive in kind of straight away into the math, starting next time. So next 2 lectures. You'll get a sense.

339
00:47:08.620 --> 00:47:13.110
Inderjit Dhillon: for you know what kind of material I'll be teaching the style of teaching

340
00:47:13.220 --> 00:47:16.909
Inderjit Dhillon: right? And so it'll give you a chance within

341
00:47:17.120 --> 00:47:22.900
Inderjit Dhillon: a week within the next 3 classes to kind of gauge what the class would be like.

342
00:47:23.630 --> 00:47:26.540
Inderjit Dhillon: Does that help Dwayne or.

343
00:47:26.780 --> 00:47:28.169
Dewayne Benson: That helps that helps.

344
00:47:28.300 --> 00:47:34.220
Inderjit Dhillon: Yeah. Okay, okay, thanks any other questions

345
00:47:37.970 --> 00:47:39.590
Inderjit Dhillon: whatever. The

346
00:47:40.920 --> 00:47:49.658
Inderjit Dhillon: comment by prachi. Right? She said. Could you please post preview of the note so we can get a better idea. So is that something that we can do

347
00:47:51.080 --> 00:47:59.920
Nilesh Gupta: Yeah, we can do it. It's just that like, we need to decide like, how do we want to post it like we can just post a collection of the notes as an analysis.

348
00:47:59.920 --> 00:48:04.449
Inderjit Dhillon: Yeah, I would almost post it in a separate folder, or as a zip, yeah, or something.

349
00:48:04.450 --> 00:48:07.937
Inderjit Dhillon: because I don't want people to get confused that this is what we've already done.

350
00:48:08.170 --> 00:48:08.810
Nilesh Gupta: Yeah.

351
00:48:09.060 --> 00:48:09.810
Inderjit Dhillon: Wife.

352
00:48:09.810 --> 00:48:10.440
Nilesh Gupta: Yeah.

353
00:48:11.190 --> 00:48:23.519
Inderjit Dhillon: Okay, so we'll figure it out, Rachi, and we will let you know. And then survey. Could you follow up with an email. So Nilesh has

354
00:48:24.431 --> 00:48:36.280
Inderjit Dhillon: given a link in the chat about the survey. But maybe let's just follow up like, you know, make it kind of homework 0, so that people also start knowing how they can be, they will receive announcements.

355
00:48:36.560 --> 00:48:37.190
Nilesh Gupta: Yeah.

356
00:48:37.450 --> 00:48:43.929
Inderjit Dhillon: Okay? And then, you know, if people don't fill it out, then you know, I can hound them next time. Call on them.

357
00:48:47.001 --> 00:48:50.820
Inderjit Dhillon: Okay, I'm just kidding. Okay, okay, thank you. Everybody. Bye.

358
00:48:51.250 --> 00:48:52.160
Hormoz Shahrzad: Thank you.

359
00:48:53.250 --> 00:48:54.080
Inderjit Dhillon: But.

360
00:48:54.080 --> 00:48:54.710
Hormoz Shahrzad: Bye.

361
00:48:56.030 --> 00:48:56.700
Eddie: Thank you.

362
00:48:57.380 --> 00:48:58.349
Dinesh Keserla: Thank you.

363
00:49:06.550 --> 00:49:10.720
Inderjit Dhillon: Okay, I'm going to end the but.

---- END OF LECTURE -------- START OF LECTURE 2 ----
WEBVTT

1
00:00:00.620 --> 00:00:11.480
Inderjit Dhillon: R, oh, okay, so we are actually going to post these lecture videos online.

2
00:00:11.600 --> 00:00:19.759
Inderjit Dhillon: I think Nilesh has sent an email to you. So if you have any issues or concerns or so, please let us know.

3
00:00:20.100 --> 00:00:32.600
Inderjit Dhillon: But this is a lecture number one. So let's get started. I am sharing my ipad window so hopefully you can see my screen as I start writing.

4
00:00:33.190 --> 00:00:37.170
Inderjit Dhillon: So this is lecture one.

5
00:00:37.530 --> 00:00:42.179
Inderjit Dhillon: And today I will be talking about. You know one of the 1st very 1st topics

6
00:00:42.530 --> 00:00:46.289
Inderjit Dhillon: in machine learning, which is linear regression.

7
00:00:46.860 --> 00:00:50.810
Inderjit Dhillon: So as an example of a problem, right? Let's consider

8
00:00:51.060 --> 00:00:55.260
Inderjit Dhillon: an example predictive problem which is.

9
00:00:55.370 --> 00:01:10.860
Inderjit Dhillon: let's suppose you want to predict levels of some particular Bsa levels from various measurements

10
00:01:14.170 --> 00:01:20.000
Inderjit Dhillon: measurements on the prostate.

11
00:01:22.830 --> 00:01:35.640
Inderjit Dhillon: Okay? So you know, it's not that the details are not that important. But what is important is that there are 2 things. One is that there are some measurements on something that we want to do prediction on.

12
00:01:35.890 --> 00:01:41.910
Inderjit Dhillon: So those we will represent by the excise. So we'll typically have training data

13
00:01:42.140 --> 00:01:51.400
Inderjit Dhillon: which will be x 1, x 2 x 3 x 4, and so on till Xn.

14
00:01:51.630 --> 00:02:04.140
Inderjit Dhillon: and along with every training data which is the measurements, we'll actually have a label and or a real number. Okay? So in regression it will be a real number.

15
00:02:04.300 --> 00:02:08.180
Inderjit Dhillon: and so we will denote that by the corresponding Y,

16
00:02:08.419 --> 00:02:10.809
Inderjit Dhillon: so y, 1, along with x, 1

17
00:02:11.060 --> 00:02:15.470
Inderjit Dhillon: y. 2, along with x 2 y. 3, along with x 3

18
00:02:15.670 --> 00:02:19.290
Inderjit Dhillon: y. 4, along with x 4, and so on.

19
00:02:19.440 --> 00:02:24.500
Inderjit Dhillon: Right. So if you have n amount, and this is known as the training data.

20
00:02:29.730 --> 00:02:34.590
Inderjit Dhillon: So basically, we'll be given measurements and we'll be given the levels of Psa

21
00:02:34.700 --> 00:02:46.420
Inderjit Dhillon: for N amount of training data. Okay, so just to clarify the y 1 y, 2 through yn, or

22
00:02:47.120 --> 00:02:48.240
Inderjit Dhillon: known.

23
00:02:49.300 --> 00:02:57.160
Inderjit Dhillon: Okay, each yi in regression belongs to R, which means it's a real number. So this is

24
00:02:57.510 --> 00:03:14.109
Inderjit Dhillon: regression. And later on we'll see that we can have classification problems where these yi's may be, you know, integer valued or are categorical variables. Okay? And again, reminding you that x, 1, x, 2 xn.

25
00:03:14.310 --> 00:03:20.080
Inderjit Dhillon: Or measurements on the prostate.

26
00:03:27.750 --> 00:03:32.880
Inderjit Dhillon: Okay, and typically, they will be not just a single value.

27
00:03:33.050 --> 00:03:56.280
Inderjit Dhillon: but they may be d features or d variables that you're going to base your prediction on. So typically Xi is going to belongs to Rd, so this means that xi. x, 1 is a d dimensional vector x 2 is a D dimensional vector and so on. So all the Xi's are d dimensional vectors. So that basically says that you have

28
00:03:56.380 --> 00:04:06.280
Inderjit Dhillon: d measurements on the prostate. Okay, let me talk a little bit about another interesting example, right? Which is.

29
00:04:06.971 --> 00:04:10.220
Inderjit Dhillon: let's suppose you are on Netflix.

30
00:04:12.610 --> 00:04:20.399
Inderjit Dhillon: Okay, so this is kind of the recommender system problem, right? And there is a user.

31
00:04:21.190 --> 00:04:24.729
Inderjit Dhillon: There is a particular movie or a show

32
00:04:25.030 --> 00:04:27.960
Inderjit Dhillon: right? Or and there's a particular rating

33
00:04:29.190 --> 00:04:33.620
Inderjit Dhillon: that this user has given to this movie.

34
00:04:34.800 --> 00:04:35.690
Inderjit Dhillon: Okay?

35
00:04:35.860 --> 00:04:40.530
Inderjit Dhillon: And so, for example, an X could be, you know.

36
00:04:40.630 --> 00:04:44.200
Inderjit Dhillon: an example of user and movie

37
00:04:44.910 --> 00:04:57.600
Inderjit Dhillon: features which are for this user and for this movie. And then the rating is actually the label or the wise right? So, for example, X could be, you know. Let's see, it's me

38
00:04:58.240 --> 00:05:02.540
Inderjit Dhillon: in the git. And there may be features that are available to

39
00:05:02.950 --> 00:05:10.480
Inderjit Dhillon: the prediction algorithm about me. Right? And let's say a movie or the show might be, you know, breaking bad

40
00:05:12.410 --> 00:05:15.780
Inderjit Dhillon: episode one all season. One.

41
00:05:18.000 --> 00:05:23.360
Inderjit Dhillon: Okay. And then let's suppose I have given you know.

42
00:05:24.550 --> 00:05:26.490
Inderjit Dhillon: I actually ended up

43
00:05:26.650 --> 00:05:33.459
Inderjit Dhillon: not knowing about this show surprisingly till Covid auto display basic

44
00:05:34.259 --> 00:05:40.010
Inderjit Dhillon: so I actually watched all this during Covid times. And you know, the 1st year was okay.

45
00:05:40.480 --> 00:05:42.719
Inderjit Dhillon: And then, of course, I got hooked onto it.

46
00:05:43.160 --> 00:05:46.140
Inderjit Dhillon: right? So for sure, maybe I gave 4 stars.

47
00:05:49.030 --> 00:05:49.840
Inderjit Dhillon: Okay?

48
00:05:50.860 --> 00:05:59.399
Inderjit Dhillon: And so this would correspond to the yi, and like, I said, this

49
00:06:00.170 --> 00:06:02.860
Inderjit Dhillon: features correspond to the X.

50
00:06:03.390 --> 00:06:09.079
Inderjit Dhillon: Okay? And the reason I mention it is. There was actually a a million dollar

51
00:06:10.040 --> 00:06:13.339
Inderjit Dhillon: 1 million dollar Netflix prize

52
00:06:16.830 --> 00:06:21.589
Inderjit Dhillon: about 20 years ago that Netflix actually sponsored right? So what they said is

53
00:06:21.720 --> 00:06:32.720
Inderjit Dhillon: that they are going to basically release. I forget how many ratings they released. Of course they did not release any user features because of privacy, right? But they basically gave

54
00:06:33.530 --> 00:06:54.750
Inderjit Dhillon: movie. Let's say, a user id, and the ring of the movie. And they released maybe 20,000 ratings. And they had a held out data set. And they said, whoever gets 10% better than their current system at the time would actually get a million dollars.

55
00:06:54.850 --> 00:07:04.869
Inderjit Dhillon: And this happened around, you know, nearly 20 years ago, and it actually spurred a lot of research in this area in machine learning, right? And especially on recommended systems.

56
00:07:05.440 --> 00:07:13.479
Inderjit Dhillon: Now, in both the above cases, right in example one, an example 2.

57
00:07:13.650 --> 00:07:20.640
Inderjit Dhillon: You're already given. For example, in example one, you're already given measurements, and you're given the level of Psa

58
00:07:20.910 --> 00:07:37.120
Inderjit Dhillon: in example 2, you're given a user movie pair and you are given the rating of this movie by this particular user? Right? So what is the actual technical problem that we need to solve? What is the prediction problem that we need to solve

59
00:07:37.460 --> 00:07:50.550
Inderjit Dhillon: in both these cases, the goal is to predict why for a new x.

60
00:07:52.200 --> 00:07:57.599
Inderjit Dhillon: Okay? So in particular, in the 1st case, you will be given a new D dimensional. Vector

61
00:07:57.980 --> 00:08:00.570
Inderjit Dhillon: and then you need to predict the level of Psa

62
00:08:00.830 --> 00:08:04.421
Inderjit Dhillon: in the example 2, you will be given

63
00:08:05.490 --> 00:08:09.019
Inderjit Dhillon: a user movie pair through its features.

64
00:08:09.130 --> 00:08:17.009
Inderjit Dhillon: And then you, what you need to do is you can you need to predict whether what rating this user would give to this particular movie?

65
00:08:17.350 --> 00:08:24.250
Inderjit Dhillon: Right? And of course, the interesting cases are exactly those where this X is not in the training data.

66
00:08:24.620 --> 00:08:35.290
Inderjit Dhillon: right? Otherwise, you can just look up. So it's not a lookup problem. But it is a prediction problem. Right? So predict Y for a new X, and then when YI belongs to R,

67
00:08:35.390 --> 00:08:37.489
Inderjit Dhillon: this is a regression problem.

68
00:08:39.429 --> 00:08:54.879
Inderjit Dhillon: Okay. I mentioned a couple of times. This Yi equal to belongs to R or a real value. Well, what are cases where the yi's might not belong to a real value. Right? So, for example, spam prediction is a problem.

69
00:08:55.280 --> 00:09:00.770
Inderjit Dhillon: Right? So let me give you another example which is not a regression problem, right?

70
00:09:01.080 --> 00:09:16.300
Inderjit Dhillon: Which is product, whether and email is spam or not.

71
00:09:17.400 --> 00:09:20.819
Inderjit Dhillon: Okay. And whether you know it or not, all the

72
00:09:21.895 --> 00:09:31.669
Inderjit Dhillon: various platforms like Gmail Yahoo mail, and so on. Use some sort of spam filter, because otherwise your email will be overrun by spam. Right?

73
00:09:31.770 --> 00:09:34.589
Inderjit Dhillon: So over here, what is the X

74
00:09:34.950 --> 00:09:41.619
Inderjit Dhillon: if I say, Capital X is the set of the excise. Right? So it's set of emails.

75
00:09:43.330 --> 00:09:44.130
Inderjit Dhillon: Okay?

76
00:09:45.100 --> 00:09:53.809
Inderjit Dhillon: And now remember that in many of these prediction problems, even though you know, in this class. Maybe we'll not be looking into it. Maybe you'll be looking into your in the projects.

77
00:09:54.120 --> 00:09:58.390
Inderjit Dhillon: We are just saying that you know you are actually given a d dimensional vector for for

78
00:09:58.560 --> 00:10:14.660
Inderjit Dhillon: practical problems, you may need to figure out exactly what are the features that you're going to use? Right. So, for example, you might end up using, you know, the words that are in the email. You may want to tokenize that, or you may look at, you know, maybe the sentiment or something else in that email. Okay.

79
00:10:14.790 --> 00:10:22.640
Inderjit Dhillon: and what is Y over here? Okay, so y is not really a real number over here. You want to predict whether an email is spam.

80
00:10:24.970 --> 00:10:27.630
Inderjit Dhillon: okay or normal.

81
00:10:31.460 --> 00:10:37.559
Inderjit Dhillon: Okay, so it's a categorical variable, right? So you need to predict whether it's spam or not spam.

82
00:10:37.810 --> 00:11:01.220
Inderjit Dhillon: Now you might encode it, and you will in most cases that we'll see you will encode it, using a real number like, for example, you can have 0 for one class and y equal to one for the other class, or you can have minus one for one class or y equal to one for another class. Right? But that's just an encoding. The nature of the problem is that y is categorical. Okay? So when

83
00:11:02.090 --> 00:11:12.159
Inderjit Dhillon: why is is categorical, then this is a classification problem.

84
00:11:15.480 --> 00:11:21.430
Inderjit Dhillon: And then when y is real valued.

85
00:11:23.340 --> 00:11:25.370
Inderjit Dhillon: then this is a regression flow.

86
00:11:28.600 --> 00:11:32.770
Inderjit Dhillon: Okay? And we are going to start off this class by talking about regression problems

87
00:11:33.100 --> 00:11:38.259
Inderjit Dhillon: right? And later on in the class, actually, the next topic after this will be classification problems.

88
00:11:41.210 --> 00:11:42.130
Inderjit Dhillon: Okay?

89
00:11:43.270 --> 00:11:53.360
Inderjit Dhillon: So now let's formulate this mathematically. See what it means to try to get a prediction algorithm. And let's look at how we might

90
00:11:53.520 --> 00:11:58.429
Inderjit Dhillon: formulate it and then actually solve this prediction problem. Okay, so let me write this again.

91
00:11:58.820 --> 00:12:04.409
Inderjit Dhillon: We are going to do regression regression problem.

92
00:12:08.380 --> 00:12:14.639
Inderjit Dhillon: So what we are, what we are given is XIYI,

93
00:12:15.760 --> 00:12:22.469
Inderjit Dhillon: Xi belongs to rd, so d, real numbers. So think of Xi as a vector

94
00:12:22.750 --> 00:12:25.320
Inderjit Dhillon: and YI belongs to a park.

95
00:12:25.990 --> 00:12:27.430
Inderjit Dhillon: So that's a regression problem.

96
00:12:27.570 --> 00:12:31.610
Inderjit Dhillon: Okay? And then, typically, you're given 1, 2

97
00:12:32.600 --> 00:12:48.459
Inderjit Dhillon: and amount of training data. Okay? And now, of course, in modern day times, N can be very, very large right? But N can vary from tens to hundreds to millions. Okay? And when N start becoming very large.

98
00:12:48.680 --> 00:12:56.469
Inderjit Dhillon: we will also look at you know how much time does it actually train take to train such to to actually form the model?

99
00:12:56.590 --> 00:13:02.679
Inderjit Dhillon: Okay, so let me just take X, which is a particular. Xi.

100
00:13:02.830 --> 00:13:08.220
Inderjit Dhillon: and let me denote it by its D features. Right? So it belongs to DRD.

101
00:13:08.460 --> 00:13:13.010
Inderjit Dhillon: Select says, D components x, 1, through XD.

102
00:13:15.940 --> 00:13:22.219
Inderjit Dhillon: Okay, so what could be the simplest model that you can try and get with these D features?

103
00:13:23.040 --> 00:13:30.699
Inderjit Dhillon: Right? So the simplest model that you can have is, you can say, Okay, I will take a linear combination of these features.

104
00:13:31.810 --> 00:13:37.839
Inderjit Dhillon: And I need to figure out what linear combination to get so that it actually matches the training data.

105
00:13:38.170 --> 00:13:44.829
Inderjit Dhillon: Right? That's very one very simple formulation of the problem. Right? So let's formalize this formulation.

106
00:13:45.270 --> 00:13:50.630
Inderjit Dhillon: So if I want to do prediction and the simplest

107
00:13:50.960 --> 00:13:56.699
Inderjit Dhillon: way of doing prediction is to linearly combine these different features, these d features.

108
00:13:57.120 --> 00:14:03.020
Inderjit Dhillon: So given an X, I'll make a prediction. Why?

109
00:14:03.580 --> 00:14:08.280
Inderjit Dhillon: And I will say that my linear combinations let me denote them by W's.

110
00:14:08.970 --> 00:14:20.909
Inderjit Dhillon: So I have w naught plus w. 1 x one plus W. 2 x 2 plus wd.

111
00:14:22.120 --> 00:14:23.100
Inderjit Dhillon: Next.

112
00:14:27.020 --> 00:14:32.139
Inderjit Dhillon: Okay, so this is just a linear combination of these D values in X,

113
00:14:32.360 --> 00:14:37.419
Inderjit Dhillon: or like, I've already called it multiple times. These are the D features of the problem.

114
00:14:38.280 --> 00:14:42.669
Inderjit Dhillon: And now I can write it. And this is where we start getting into start seeing

115
00:14:42.890 --> 00:14:45.279
Inderjit Dhillon: some linear algebra which will make the

116
00:14:45.880 --> 00:14:54.920
Inderjit Dhillon: Us. Allow us to form the problem much more concisely right, and be able to solve it also in a way concisely right. So I'm going to write this as W naught

117
00:14:55.220 --> 00:15:00.109
Inderjit Dhillon: plus an inner product between the vector of

118
00:15:00.630 --> 00:15:04.659
Inderjit Dhillon: my coefficients. Times, my, vector X,

119
00:15:05.080 --> 00:15:16.099
Inderjit Dhillon: so this is W bar, transpose X and W bar is w, 1 w, 2 WD.

120
00:15:16.580 --> 00:15:19.210
Inderjit Dhillon: So W. Bar is a D dimensional vector.

121
00:15:19.740 --> 00:15:20.590
Inderjit Dhillon: Right?

122
00:15:21.090 --> 00:15:33.159
Inderjit Dhillon: And I can also. So now I have, you know, 2 parts to it. Right we have. I have w. 1 w. Naught plus W Bar transpose XI can actually write. I'm going to write it more simply, as

123
00:15:33.350 --> 00:15:37.199
Inderjit Dhillon: just W transpose X bar.

124
00:15:37.690 --> 00:15:40.420
Inderjit Dhillon: Okay, because I can write this as

125
00:15:41.010 --> 00:15:46.859
Inderjit Dhillon: W. Naught. Times one plus W Bar transpose X,

126
00:15:47.090 --> 00:15:53.690
Inderjit Dhillon: so I can basically form one bigger dimensional vector, X bar, which is

127
00:15:56.480 --> 00:16:00.270
Inderjit Dhillon: X is going to be its 1st component, is going to be one.

128
00:16:00.580 --> 00:16:06.270
Inderjit Dhillon: and the rest of the components are going to be as they were before. So x 1

129
00:16:06.910 --> 00:16:10.860
Inderjit Dhillon: x 2 x.

130
00:16:16.630 --> 00:16:25.860
Inderjit Dhillon: okay, and like Nice and W. Is going to be equal to W. Naught.

131
00:16:26.020 --> 00:16:32.200
Inderjit Dhillon: w. 1 w. 2 WD.

132
00:16:33.550 --> 00:16:36.410
Inderjit Dhillon: And I've written them next to each other. Right? So you know.

133
00:16:36.900 --> 00:16:45.919
Inderjit Dhillon: the definition of an inner product in linear algebra is that you know, you basically take the corresponding coordinates, and you add them up right? So it's w naught

134
00:16:46.030 --> 00:16:53.409
Inderjit Dhillon: plus w 1 I guess maybe I should use it closer.

135
00:16:54.280 --> 00:16:56.760
Inderjit Dhillon: Oh, let's see.

136
00:16:57.060 --> 00:16:57.830
Inderjit Dhillon: But

137
00:17:03.310 --> 00:17:06.710
Inderjit Dhillon: you guys can see a cursor here, or, yeah.

138
00:17:07.140 --> 00:17:10.600
Inderjit Dhillon: so you can see. And I can take one times, W, naught.

139
00:17:11.819 --> 00:17:15.640
Inderjit Dhillon: Okay. w. 1 times x, 1, and so on. Right.

140
00:17:20.900 --> 00:17:30.219
Inderjit Dhillon: and WIW. And X. Both are now d plus one dimensional vectors.

141
00:17:30.860 --> 00:17:33.090
Inderjit Dhillon: Okay. W. And X bar. Sorry.

142
00:17:33.540 --> 00:17:41.950
Inderjit Dhillon: Okay, so this is now my prediction. It is W transpose X bar.

143
00:17:42.440 --> 00:17:50.940
Inderjit Dhillon: Okay, so what happens with W transpose x 1 ball.

144
00:17:51.840 --> 00:17:55.580
Inderjit Dhillon: Okay, let me just write this again so that you can see clearly.

145
00:17:56.830 --> 00:18:02.360
Inderjit Dhillon: So I have W transpose x 1 bar.

146
00:18:03.270 --> 00:18:08.309
Inderjit Dhillon: Well, remember x, 1 is the 1st training data.

147
00:18:08.470 --> 00:18:10.860
Inderjit Dhillon: And what was this corresponding label?

148
00:18:10.960 --> 00:18:15.990
Inderjit Dhillon: Well, its corresponding label is, Why one

149
00:18:16.970 --> 00:18:19.570
Inderjit Dhillon: right? But remember that this is the prediction.

150
00:18:20.560 --> 00:18:25.929
Inderjit Dhillon: This prediction we may not actually exactly want it to be x, 1, y. One.

151
00:18:26.520 --> 00:18:31.970
Inderjit Dhillon: Right? So I'm going to talk. Call this as Y of x 1.

152
00:18:33.880 --> 00:18:40.890
Inderjit Dhillon: This is my prediction, the given label, the value that we wanted to take is y 1.

153
00:18:43.000 --> 00:18:50.710
Inderjit Dhillon: Now, one question that you should think about is, do you want this to be exactly equal or not

154
00:18:52.800 --> 00:18:58.139
Inderjit Dhillon: right? So think about the case where N is in the millions.

155
00:18:59.010 --> 00:19:05.710
Inderjit Dhillon: typically, my d will be fixed. So let's say, D is in the let's say 10 right.

156
00:19:06.250 --> 00:19:08.480
Inderjit Dhillon: and N is in the millions.

157
00:19:08.680 --> 00:19:16.540
Inderjit Dhillon: then I only have 10 or 11 parameters right? If I count W. Naught, then it's 11 parameters right?

158
00:19:17.280 --> 00:19:23.190
Inderjit Dhillon: And I have 1 million point, you know, values to to predict.

159
00:19:23.510 --> 00:19:28.320
Inderjit Dhillon: So there is no way, typically that you will be able to get exact equality

160
00:19:28.690 --> 00:19:41.759
Inderjit Dhillon: right? And in many cases it may actually not be good to actually have exact equality, because that leads to a phenomena in machine learning called overfitting. And you don't want to do that. You don't want to necessarily predict

161
00:19:41.990 --> 00:19:50.890
Inderjit Dhillon: the existing labels in the training data perfectly. But you want to do a good prediction of new new data points that come

162
00:19:51.160 --> 00:19:51.940
Inderjit Dhillon: okay.

163
00:19:52.060 --> 00:19:54.300
Inderjit Dhillon: So we will not do this.

164
00:19:55.400 --> 00:19:57.609
Inderjit Dhillon: but we want to try to fit it. Well.

165
00:19:57.900 --> 00:20:00.860
Inderjit Dhillon: So I'm just going to say that this should be approximately equal.

166
00:20:02.290 --> 00:20:10.679
Inderjit Dhillon: Okay, so similarly, I have the prediction for x 2, which is W transpose x 2 bar.

167
00:20:10.930 --> 00:20:13.519
Inderjit Dhillon: I want it to be y. 2,

168
00:20:14.470 --> 00:20:18.240
Inderjit Dhillon: and then I have y of x 3

169
00:20:18.550 --> 00:20:22.180
Inderjit Dhillon: W. Transpose W. Bar. Well, sorry.

170
00:20:22.860 --> 00:20:24.180
Inderjit Dhillon: Too many bars.

171
00:20:27.970 --> 00:20:30.959
Inderjit Dhillon: so I have. W transpose.

172
00:20:31.620 --> 00:20:33.860
Inderjit Dhillon: Gosh, don't worry.

173
00:20:36.820 --> 00:20:44.040
Inderjit Dhillon: Is W. Transpose x 3 bar to be y. 3.

174
00:20:44.360 --> 00:20:57.710
Inderjit Dhillon: And similarly, I have y of xn, which is W transpose x, and more equal to one.

175
00:20:59.230 --> 00:21:07.079
Inderjit Dhillon: Okay, so now, what does this mean? What is this nearly equal to? Symbol mean? Okay? Which means.

176
00:21:07.360 --> 00:21:12.470
Inderjit Dhillon: if I look at the prediction, my prediction is, W transpose x, 1 bar

177
00:21:13.080 --> 00:21:17.039
Inderjit Dhillon: right. My actual value is y. 1.

178
00:21:18.100 --> 00:21:25.380
Inderjit Dhillon: So I need some measure of error between y. 1 and w transpose x 1.

179
00:21:25.710 --> 00:21:29.310
Inderjit Dhillon: So the simplest thing to do is to look at the squared error.

180
00:21:31.970 --> 00:21:34.330
Inderjit Dhillon: So I want this to be small.

181
00:21:36.940 --> 00:21:39.930
Inderjit Dhillon: So W transpose x, 1 bar is the prediction.

182
00:21:40.970 --> 00:21:49.010
Inderjit Dhillon: y, 1 is the corresponding label, and I want the prediction to be close to y. 1. i don't necessarily want it to be equal to y 1,

183
00:21:49.440 --> 00:21:51.949
Inderjit Dhillon: for the various reasons I've mentioned before.

184
00:21:52.120 --> 00:22:00.329
Inderjit Dhillon: But I want them to be close. Okay? Similarly, I have the prediction for the second training data is, W transpose

185
00:22:00.690 --> 00:22:06.059
Inderjit Dhillon: x 2 bar minus y 2 square is small.

186
00:22:08.200 --> 00:22:11.569
Inderjit Dhillon: I have w transpose x 3 bar

187
00:22:12.190 --> 00:22:19.189
Inderjit Dhillon: minus y 3 square is small, and so on.

188
00:22:21.000 --> 00:22:28.460
Inderjit Dhillon: W transpose xn bar minus YN square S smells.

189
00:22:30.520 --> 00:22:41.140
Inderjit Dhillon: So that is kind of the what I desire. Right? So now I have n error estimates right, and I need to

190
00:22:41.250 --> 00:22:48.780
Inderjit Dhillon: somehow make them small. So what I can do is I can basically take the sum of all these squares.

191
00:22:49.590 --> 00:22:56.249
Inderjit Dhillon: and I can say that I will, you know, make the either some or the average of these values to be small.

192
00:22:56.610 --> 00:23:00.610
Inderjit Dhillon: So I can take, you know, summation of.

193
00:23:01.280 --> 00:23:05.710
Inderjit Dhillon: So this is a summation sign. I equals one through N.

194
00:23:06.740 --> 00:23:14.570
Inderjit Dhillon: W. Transpose XI bar minus YI, okay.

195
00:23:15.130 --> 00:23:23.509
Inderjit Dhillon: I want this to be small, and for convenience, because I will see when we take the derivatives. I'm just going to do one divided by 2.

196
00:23:23.810 --> 00:23:29.770
Inderjit Dhillon: And I'm going to call this as a function of my coefficients.

197
00:23:33.360 --> 00:23:34.210
Inderjit Dhillon: Okay.

198
00:23:34.640 --> 00:23:40.810
Inderjit Dhillon: so this is fw, and what do I want to do? Right? This is a measure so given of the particular W.

199
00:23:41.400 --> 00:23:45.920
Inderjit Dhillon: This gives a measure of the error on the training data.

200
00:23:46.240 --> 00:23:50.620
Inderjit Dhillon: And in particular, this is the least squares error on the training data.

201
00:23:51.560 --> 00:23:53.950
Inderjit Dhillon: Okay, so one goal.

202
00:23:55.300 --> 00:23:59.390
Inderjit Dhillon: what do you want to do? We want to make this small right? Like I've written above

203
00:23:59.640 --> 00:24:05.039
Inderjit Dhillon: right. The one goal is to find the W

204
00:24:06.220 --> 00:24:09.449
Inderjit Dhillon: such that I minimize F of W.

205
00:24:12.050 --> 00:24:16.709
Inderjit Dhillon: And this is called least squares regression.

206
00:24:24.670 --> 00:24:29.300
Inderjit Dhillon: Okay, let me take a little pause and see if there are any questions so far.

207
00:24:29.820 --> 00:24:35.560
Inderjit Dhillon: By the way, you should feel feel free to kind of, you know. Ask questions, you know, feel free to raise your hand and interrupt me.

208
00:24:39.513 --> 00:24:44.859
Hormoz Shahrzad: Question. Why is like when we are

209
00:24:44.990 --> 00:24:55.779
Hormoz Shahrzad: thinking about the distance, like the the square error is the the most natural one that comes up. Why don't we use, for example, absolute error.

210
00:24:57.960 --> 00:25:03.509
Inderjit Dhillon: Great question. So one is, the is actually going to make the computations much more simple.

211
00:25:04.430 --> 00:25:08.679
Inderjit Dhillon: And they are actually going to be, you know, we'll see that this has a closed form solution.

212
00:25:08.980 --> 00:25:13.320
Inderjit Dhillon: But when we take something like absolute values.

213
00:25:13.610 --> 00:25:28.490
Inderjit Dhillon: problem is a little bit more complicated. Many people would want to solve it. And there are algorithms to solve it. We're probably not actually going to touch upon it in this class. But you know, for example, the absolute value is, for example, not differentiable everywhere.

214
00:25:29.370 --> 00:25:41.849
Inderjit Dhillon: Right? So so it leads to some complications. And so this is classical. Least squares regression. It's the 1st topic in this course, and we'll see that you know it ends up having a simplified solution.

215
00:25:42.840 --> 00:25:44.810
Inderjit Dhillon: Does that answer your question or.

216
00:25:44.810 --> 00:25:45.869
Hormoz Shahrzad: Yes, thank you.

217
00:25:46.210 --> 00:26:08.959
Inderjit Dhillon: Okay, and then they will. So I don't know if I always monitor the chat. So feel free to kind of remind me if there are questions on chat. So I do see a couple. So there's a half in front, because, you know, it's just a scalar. It doesn't really matter, but we'll see that when we take the derivative of you know, when we take the derivative of a square or 2 comes out and it'll cancel out

218
00:26:08.980 --> 00:26:17.600
Inderjit Dhillon: right? So it's just a convenience. Another question, prachi, which is, you know, we can easily kind of have taken one divided by N, it doesn't really affect things.

219
00:26:19.950 --> 00:26:25.910
Inderjit Dhillon: Okay, okay, any more questions.

220
00:26:28.220 --> 00:26:33.130
Inderjit Dhillon: Okay, so now, we have a particular mathematical problem.

221
00:26:33.750 --> 00:26:34.580
Inderjit Dhillon: Right?

222
00:26:35.350 --> 00:26:37.489
Inderjit Dhillon: We need to find the W

223
00:26:37.920 --> 00:26:41.710
Inderjit Dhillon: that minimizes the sum squared. Error

224
00:26:42.580 --> 00:26:45.240
Inderjit Dhillon: of the prediction from the true value.

225
00:26:46.200 --> 00:26:49.100
Inderjit Dhillon: So how do we go about solving this problem?

226
00:26:49.790 --> 00:26:55.989
Inderjit Dhillon: Okay, so this is where, for example, linear algebra comes in and we'll see that with linear algebra

227
00:26:56.310 --> 00:27:00.019
Inderjit Dhillon: there are advantages. We'll be able to express the problem compactly.

228
00:27:00.290 --> 00:27:11.050
Inderjit Dhillon: We'll be able to find the pose, the solution also very compactly, and we'll end up getting rid of these very kind of annoying indices. Otherwise.

229
00:27:11.150 --> 00:27:19.309
Inderjit Dhillon: you know, it can actually get overwhelming when you, you know, have to manipulate all these indices. I'm almost sure

230
00:27:20.098 --> 00:27:24.640
Inderjit Dhillon: that. I made probably made some error already in what I wrote

231
00:27:24.860 --> 00:27:35.429
Inderjit Dhillon: right, because I had to manipulate the indices. Maybe I haven't right, but it becomes cumbersome. So let's see how to express the problem in a very compact way.

232
00:27:35.650 --> 00:27:41.609
Inderjit Dhillon: using linear algebra, and then that will also lead to a compact way to get the solution.

233
00:27:41.740 --> 00:27:46.859
Inderjit Dhillon: Okay, so here's what we will, we are, gonna do. We are gonna take all the

234
00:27:48.770 --> 00:27:50.070
Inderjit Dhillon: the training data.

235
00:27:50.400 --> 00:27:53.589
Inderjit Dhillon: And we are actually going to put it into a matrix.

236
00:27:53.930 --> 00:28:00.730
Inderjit Dhillon: Okay, so I'm gonna take my x 1 bar X 2 bar

237
00:28:01.770 --> 00:28:04.070
Inderjit Dhillon: all the way up to Xn bar.

238
00:28:04.890 --> 00:28:09.919
Inderjit Dhillon: Okay, remember that this is d plus one dimensional.

239
00:28:11.930 --> 00:28:18.470
Inderjit Dhillon: Okay? So there are d plus one roles in this matrix.

240
00:28:18.840 --> 00:28:22.430
Inderjit Dhillon: And obviously, the column dimension is just n.

241
00:28:23.740 --> 00:28:33.919
Inderjit Dhillon: okay, so this matrix, I'm going to call it as X, right? So X belongs to d plus one.

242
00:28:34.600 --> 00:28:39.360
Inderjit Dhillon: But okay. So there are N columns.

243
00:28:39.590 --> 00:28:41.720
Inderjit Dhillon: The Ith column is Xi Bar

244
00:28:41.960 --> 00:28:51.880
Inderjit Dhillon: and Xi bar is D plus one dimensional. Its 1st coefficient 1st column is sorry. 1st element is one.

245
00:28:52.760 --> 00:28:56.660
Inderjit Dhillon: Okay? And the rest of them are the D features.

246
00:28:59.100 --> 00:29:00.030
Inderjit Dhillon: Okay?

247
00:29:00.760 --> 00:29:09.669
Inderjit Dhillon: So well, how does it? Well, let's also do the same thing for y, which are the labels? Okay, I'm going to say that y is y 1

248
00:29:10.270 --> 00:29:18.610
Inderjit Dhillon: through y end. So these are the end labels, and Y belongs to all right.

249
00:29:21.000 --> 00:29:25.859
Inderjit Dhillon: And I and I can actually just write out the X in more detail, right? Just so that you guys

250
00:29:26.483 --> 00:29:37.319
Inderjit Dhillon: realize that it is a d plus one there are d plus one roles. Right? So I can actually expand this out. So x 1 bar is one

251
00:29:38.160 --> 00:29:41.850
Inderjit Dhillon: x, 1 1 x, 1, 2.

252
00:29:43.230 --> 00:29:56.360
Inderjit Dhillon: Please keep me honest if I make a mistake, one x 2, 1 x, 2 2 x to n.

253
00:29:57.640 --> 00:30:02.620
Inderjit Dhillon: and then I have one XN.

254
00:30:02.740 --> 00:30:08.639
Inderjit Dhillon: One xm, 2 XM.

255
00:30:14.070 --> 00:30:16.369
Inderjit Dhillon: Okay, did I expand it correctly?

256
00:30:20.360 --> 00:30:22.079
Inderjit Dhillon: Yeah. All good.

257
00:30:26.320 --> 00:30:31.100
Inderjit Dhillon: Okay. Somebody should have corrected me. I deliberately put a made a mistake right?

258
00:30:31.100 --> 00:30:40.109
Hormoz Shahrzad: Yeah, we have it d d plus one by n, but now we have an n plus one by n, something.

259
00:30:40.570 --> 00:30:42.660
Inderjit Dhillon: Yes, good good catch.

260
00:30:43.840 --> 00:30:48.859
Inderjit Dhillon: So I this time I deliberately put this mistake. So just I was just trying to see whether

261
00:30:49.590 --> 00:30:51.239
Inderjit Dhillon: people are paying attention.

262
00:30:52.930 --> 00:30:59.840
Inderjit Dhillon: But it's also my inbuilt excuse. Now I can actually make a mistake, and just tell you that you know I did it deliberately to see if you would catch it.

263
00:31:00.420 --> 00:31:01.489
Inderjit Dhillon: I'm just kidding.

264
00:31:02.770 --> 00:31:14.639
Inderjit Dhillon: But what I have over here is D, okay, so remember, X is d plus one.

265
00:31:16.100 --> 00:31:27.180
Inderjit Dhillon: Why am so each column of this matrix X is, basically corresponds to the corresponding training data.

266
00:31:27.420 --> 00:31:30.870
Inderjit Dhillon: Okay, okay, prachi also pointed out

267
00:31:31.740 --> 00:31:33.819
Inderjit Dhillon: that I should make this correction

268
00:31:33.960 --> 00:31:42.090
Inderjit Dhillon: feel free to like, you know, interrupt me, and you know, or raise your hand or so right? I I don't always see the chat. I actually have it open right now. So that's good.

269
00:31:43.510 --> 00:31:49.430
Inderjit Dhillon: Okay, so how do I use this? How do I use X,

270
00:31:49.870 --> 00:31:53.600
Inderjit Dhillon: W, and y. So now I have a matrix X,

271
00:31:54.590 --> 00:32:02.330
Inderjit Dhillon: this I know is, you know, d plus one by ND plus one by Ni have y

272
00:32:02.880 --> 00:32:09.760
Inderjit Dhillon: y is RM, and then I have W. Which belongs to RD.

273
00:32:11.190 --> 00:32:13.780
Inderjit Dhillon: Is it already, or d plus one?

274
00:32:15.110 --> 00:32:19.270
Inderjit Dhillon: Right? So because I have W. Naught w. 1 through? Wd.

275
00:32:22.720 --> 00:32:24.949
Inderjit Dhillon: Remember my objective. Is this

276
00:32:34.770 --> 00:32:37.230
Inderjit Dhillon: so what I can do is I can actually write.

277
00:32:39.410 --> 00:32:42.270
Inderjit Dhillon: If I think about my matrix X, right?

278
00:32:42.810 --> 00:32:48.209
Inderjit Dhillon: I can think of my prediction. X transpose W

279
00:32:48.810 --> 00:32:58.830
Inderjit Dhillon: as taking an inner product between the column of X right and W.

280
00:32:59.920 --> 00:33:03.120
Inderjit Dhillon: So what I can do is I can take a transpose of X,

281
00:33:05.400 --> 00:33:08.049
Inderjit Dhillon: and I can write X transpose W.

282
00:33:09.620 --> 00:33:13.119
Inderjit Dhillon: So now think about it right. What is X transpose? W.

283
00:33:15.360 --> 00:33:20.829
Inderjit Dhillon: X. Transpose is N. By d plus one

284
00:33:23.050 --> 00:33:29.340
Inderjit Dhillon: n. By d plus one, and my W is d plus one dimensional.

285
00:33:29.940 --> 00:33:35.089
Inderjit Dhillon: So when I multiply the 2, I basically get my prediction

286
00:33:35.420 --> 00:33:40.910
Inderjit Dhillon: right, I basically get a vector Y of all the X's

287
00:33:42.550 --> 00:33:48.390
Inderjit Dhillon: right. The 1st value is going to be W transpose x, 1.

288
00:33:49.130 --> 00:33:52.899
Inderjit Dhillon: Second value will be W transpose x, 2, and so on.

289
00:33:53.160 --> 00:33:55.810
Inderjit Dhillon: And now I need to match it up with the voice.

290
00:33:55.930 --> 00:33:56.750
Inderjit Dhillon: Right?

291
00:33:56.910 --> 00:33:58.740
Inderjit Dhillon: So the so there are.

292
00:33:59.230 --> 00:34:02.739
Inderjit Dhillon: X transpose. W is an n-dimensional. Vector

293
00:34:03.860 --> 00:34:08.960
Inderjit Dhillon: and if I now subtract it from y right?

294
00:34:09.270 --> 00:34:10.810
Inderjit Dhillon: And if I look at it.

295
00:34:11.239 --> 00:34:17.090
Inderjit Dhillon: then each if I look at F of w, then each

296
00:34:18.354 --> 00:34:27.199
Inderjit Dhillon: each term inside the sum is exactly equal to one element in this. Vector okay?

297
00:34:27.600 --> 00:34:32.249
Inderjit Dhillon: And then in linear algebra, there's something called the norm of a, vector

298
00:34:33.750 --> 00:34:36.579
Inderjit Dhillon: so if I take the L 2 square norm

299
00:34:37.860 --> 00:34:40.610
Inderjit Dhillon: F of W is exactly that

300
00:34:51.300 --> 00:34:53.000
Inderjit Dhillon: any question about this.

301
00:34:54.880 --> 00:35:00.719
Inderjit Dhillon: So this is where I started using basically things that you have to. You should have learned in your linear algebra course.

302
00:35:01.140 --> 00:35:06.950
Inderjit Dhillon: Right? So a norm, the L, 2 norm of a vector

303
00:35:07.100 --> 00:35:11.839
Inderjit Dhillon: is the square root of the sum of squares of the components of that vector.

304
00:35:12.190 --> 00:35:18.630
Inderjit Dhillon: Right? In this case, we don't need to take square root. Right? So it is the squared. L, 2, naught.

305
00:35:22.360 --> 00:35:27.039
Inderjit Dhillon: Okay? So that's the same as the. So this is the least square subject.

306
00:35:30.900 --> 00:35:31.570
Inderjit Dhillon: Okay.

307
00:35:38.330 --> 00:35:39.740
Inderjit Dhillon: any question.

308
00:35:42.260 --> 00:35:44.300
Inderjit Dhillon: Everybody familiar with L. 2, norm.

309
00:35:48.030 --> 00:35:51.690
Inderjit Dhillon: Okay, but this is identical. So what I've written over here

310
00:35:53.870 --> 00:35:56.139
Inderjit Dhillon: is a very, very compact way

311
00:35:57.270 --> 00:35:59.420
Inderjit Dhillon: of writing what was over here.

312
00:36:02.490 --> 00:36:03.840
Inderjit Dhillon: They're exactly equal.

313
00:36:06.380 --> 00:36:07.290
Inderjit Dhillon: Okay.

314
00:36:08.020 --> 00:36:17.300
Inderjit Dhillon: So now again, my, my, you know, I haven't really done anything. I haven't really made any progress towards solving it, except that I've written things more compact.

315
00:36:17.620 --> 00:36:30.830
Inderjit Dhillon: Okay, so remember the goal as before is to find W. Such that FW.

316
00:36:32.360 --> 00:36:34.390
Inderjit Dhillon: Is minimized.

317
00:36:40.830 --> 00:36:42.109
Inderjit Dhillon: So how would you do it?

318
00:36:42.710 --> 00:36:45.252
Inderjit Dhillon: Well, just think about it right? This is

319
00:36:46.862 --> 00:36:53.910
Inderjit Dhillon: my w's are my coefficients of variables that I need to figure out right.

320
00:36:54.420 --> 00:36:57.519
Inderjit Dhillon: And it's quadratic in W.

321
00:36:59.990 --> 00:37:03.460
Inderjit Dhillon: So what do I do? I take derivatives.

322
00:37:03.460 --> 00:37:07.640
Hormoz Shahrzad: Take the derivative and default it to 0 and solve it.

323
00:37:07.640 --> 00:37:13.499
Inderjit Dhillon: I set it to 0. Right? So take derivative and set it to equal to 0. Okay, so let's do that.

324
00:37:13.900 --> 00:37:14.740
Inderjit Dhillon: Okay.

325
00:37:14.970 --> 00:37:21.000
Inderjit Dhillon: Now, remember that if I take a vector Z,

326
00:37:25.260 --> 00:37:29.710
Inderjit Dhillon: so a property of a square norm is

327
00:37:29.870 --> 00:37:32.880
Inderjit Dhillon: that it's basically the same as Z transpose. Z,

328
00:37:38.920 --> 00:37:52.220
Inderjit Dhillon: okay, so I can write F of W is half of X transpose W minus y transpose

329
00:37:52.810 --> 00:37:55.710
Inderjit Dhillon: times X transpose W minus one.

330
00:37:57.970 --> 00:37:58.710
Inderjit Dhillon: Okay.

331
00:37:59.220 --> 00:38:08.850
Inderjit Dhillon: let me just do some manipulations. I have a transpose over here. I can write it as W transpose X minus y transpose

332
00:38:09.120 --> 00:38:12.049
Inderjit Dhillon: times X transpose W minus y.

333
00:38:13.880 --> 00:38:19.590
Inderjit Dhillon: Okay, then I multiply the whole thing out right. I get half of

334
00:38:20.200 --> 00:38:25.679
Inderjit Dhillon: 1st term with the 1st term W transpose XX transpose W

335
00:38:26.640 --> 00:38:44.810
Inderjit Dhillon: minus y transpose x, transpose W minus W transpose XY plus y transpose. Y, okay.

336
00:38:46.800 --> 00:38:51.609
Inderjit Dhillon: Another fact is that you know, if I take a transpose B

337
00:38:52.340 --> 00:38:57.229
Inderjit Dhillon: where A and B are 2 different vectors, that's exactly the same as B transpose a

338
00:38:58.360 --> 00:38:59.949
Inderjit Dhillon: right. So I can write.

339
00:39:00.670 --> 00:39:05.379
Inderjit Dhillon: So these are essentially transposes of each other, this and this.

340
00:39:05.710 --> 00:39:15.490
Inderjit Dhillon: So I can write this as half of W transpose XX transpose W. Sorry.

341
00:39:22.440 --> 00:39:30.309
Inderjit Dhillon: minus 2 y transpose X transpose W plus y transpose? Y.

342
00:39:32.450 --> 00:39:36.249
Inderjit Dhillon: Okay. So I've just expanded it so far. I haven't taken the derivative.

343
00:39:36.450 --> 00:39:40.180
Inderjit Dhillon: But then that's the next step I need to take

344
00:39:40.610 --> 00:39:43.649
Inderjit Dhillon: the derivative, except that in this case.

345
00:39:43.810 --> 00:39:47.010
Inderjit Dhillon: you know, you have multi multiple variables.

346
00:39:47.390 --> 00:39:50.520
Inderjit Dhillon: Right? So I basically take what's called the gradient.

347
00:39:51.530 --> 00:39:58.620
Inderjit Dhillon: So I take the gradient my way. And so this is notation for the gradient.

348
00:40:00.330 --> 00:40:06.500
Inderjit Dhillon: So my variable that I'm trying to differentiate with respect to is W,

349
00:40:06.830 --> 00:40:20.900
Inderjit Dhillon: right? So I'll basically have half of 2 XX transpose W minus 2 of XY,

350
00:40:22.700 --> 00:40:31.579
Inderjit Dhillon: and this is why I have this half right? It's basically this half will cancel out with the 2, and I'll get XX transpose W minus

351
00:40:32.140 --> 00:40:33.130
Inderjit Dhillon: XY.

352
00:40:37.160 --> 00:40:38.910
Inderjit Dhillon: And then what do I need to do?

353
00:40:39.220 --> 00:40:44.729
Inderjit Dhillon: I take the derivative and I set it equal to 0.

354
00:40:46.640 --> 00:40:48.700
Inderjit Dhillon: Does that always lead to the minimum?

355
00:40:52.840 --> 00:40:57.760
Inderjit Dhillon: Well, it always leads to minimum. Whenever it is a, you know, convex function.

356
00:40:58.950 --> 00:41:03.629
Inderjit Dhillon: Right? So in this case, it turns out that this is a convex function.

357
00:41:03.970 --> 00:41:08.759
Inderjit Dhillon: right? And I basically take the derivative, I set it to 0.

358
00:41:08.880 --> 00:41:13.860
Inderjit Dhillon: And my optimal W. Star is then going to be

359
00:41:14.860 --> 00:41:17.730
Inderjit Dhillon: the minimizer of F of W.

360
00:41:25.670 --> 00:41:27.220
Prachi Ingle: I had a good question.

361
00:41:27.220 --> 00:41:28.240
Inderjit Dhillon: Yeah. Go ahead.

362
00:41:28.410 --> 00:41:32.419
Prachi Ingle: In general, how can we know that F is a convex function?

363
00:41:33.210 --> 00:41:40.410
Inderjit Dhillon: So you know, one is, if you have a sum of squares, it's a convex function

364
00:41:40.790 --> 00:41:48.019
Inderjit Dhillon: right? And in general, there's a test to see. Like, for example, if it's a multivariate function.

365
00:41:48.130 --> 00:41:50.930
Inderjit Dhillon: we can look at the matrix of second derivatives.

366
00:41:51.150 --> 00:41:54.570
Inderjit Dhillon: And if that matrix is positive, definite.

367
00:41:55.450 --> 00:41:56.919
Inderjit Dhillon: Then it's a convex function.

368
00:41:59.280 --> 00:41:59.960
Prachi Ingle: Okay.

369
00:41:59.960 --> 00:42:01.719
Prachi Ingle: I see. I see. Thank you.

370
00:42:01.720 --> 00:42:06.770
Inderjit Dhillon: Oh, so remember that. For example, if you have just a scalar function.

371
00:42:07.700 --> 00:42:12.690
Inderjit Dhillon: right? Test, whether it's a minimizer or maximizer, you typically take the second derivative.

372
00:42:13.190 --> 00:42:14.000
Prachi Ingle: Right.

373
00:42:14.330 --> 00:42:22.659
Inderjit Dhillon: If the second derivative is greater than 0, then if you minimize, if you set the derivative equal to 0, then that's a minimizer.

374
00:42:23.130 --> 00:42:24.010
Prachi Ingle: Right.

375
00:42:24.520 --> 00:42:30.590
Inderjit Dhillon: Right? So it's basically a generalization of this except to multivariable multiple variables.

376
00:42:30.590 --> 00:42:33.249
Prachi Ingle: Okay. I think that makes a lot of sense. Thank you.

377
00:42:33.390 --> 00:42:34.010
Inderjit Dhillon: Okay?

378
00:42:34.590 --> 00:42:42.350
Inderjit Dhillon: Good question. But again, this is the reason why kind of we need you to have the linear algebra background.

379
00:42:43.180 --> 00:42:46.496
Inderjit Dhillon: And Anas had a question which was,

380
00:42:47.870 --> 00:42:54.180
Inderjit Dhillon: what W. Star was. This is my definition of W. Star. Right? The W. Star is now

381
00:42:55.380 --> 00:42:58.260
Inderjit Dhillon: the outcome of training this method.

382
00:42:59.640 --> 00:43:02.840
Inderjit Dhillon: So if I find if I take F of W.

383
00:43:03.500 --> 00:43:09.180
Inderjit Dhillon: And I want, if I find the minimizer of ffw, which is this.

384
00:43:10.770 --> 00:43:14.910
Inderjit Dhillon: So actually, you know. Maybe

385
00:43:16.770 --> 00:43:20.030
Inderjit Dhillon: I have completed a few things. Sorry.

386
00:43:27.530 --> 00:43:29.363
Inderjit Dhillon: I'll get better at this.

387
00:43:30.600 --> 00:43:33.690
Inderjit Dhillon: Okay, so so I set it equal to 0, right.

388
00:43:34.400 --> 00:43:37.339
Inderjit Dhillon: But but the gradient was just this right?

389
00:43:37.520 --> 00:43:43.300
Inderjit Dhillon: And so this is the value where gradient of

390
00:43:43.560 --> 00:43:48.629
Inderjit Dhillon: W. Of F, of W. Star equals it.

391
00:43:49.290 --> 00:43:56.040
Inderjit Dhillon: So basically, my, my W star, is this?

392
00:44:01.260 --> 00:44:04.850
Inderjit Dhillon: Okay? So just to repeat, let me just see.

393
00:44:05.150 --> 00:44:07.840
Inderjit Dhillon: can you guys see a cursor here or no?

394
00:44:09.290 --> 00:44:10.480
Inderjit Dhillon: No, right.

395
00:44:10.480 --> 00:44:10.950
Nilesh Gupta: No.

396
00:44:10.950 --> 00:44:11.843
Hormoz Shahrzad: No, and

397
00:44:12.500 --> 00:44:16.220
Inderjit Dhillon: Okay, I'll need to figure that out how to show you the closer.

398
00:44:18.490 --> 00:44:21.130
Inderjit Dhillon: Oh, you can see this right? You can see the cursor now.

399
00:44:21.130 --> 00:44:21.820
Hormoz Shahrzad: Yes.

400
00:44:22.980 --> 00:44:25.880
Inderjit Dhillon: Okay, somehow, I see 2 persons. Okay.

401
00:44:26.963 --> 00:44:30.099
Inderjit Dhillon: But close enough. Right? So so this is the gradient.

402
00:44:31.330 --> 00:44:34.089
Inderjit Dhillon: Okay, so if you take any W.

403
00:44:34.470 --> 00:44:35.890
Inderjit Dhillon: And this is the gradient.

404
00:44:36.250 --> 00:44:41.429
Inderjit Dhillon: What we want is that this should be set to 0 for us to get

405
00:44:41.580 --> 00:44:43.149
Inderjit Dhillon: minimum of F of W.

406
00:44:43.900 --> 00:44:49.950
Inderjit Dhillon: So my W. Star, which is, you know, not very well written.

407
00:44:50.420 --> 00:44:52.180
Inderjit Dhillon: This is W. Star.

408
00:44:53.430 --> 00:44:59.540
Inderjit Dhillon: Is the W. That particular W where the gradient equal to 0? Which means this.

409
00:45:07.020 --> 00:45:10.250
Inderjit Dhillon: Okay, does that make sense on us?

410
00:45:14.290 --> 00:45:15.220
Inderjit Dhillon: Okay, great.

411
00:45:16.920 --> 00:45:24.179
Inderjit Dhillon: Let's look a little bit closer at this. Right? I want my remind. Remember what X was.

412
00:45:24.330 --> 00:45:25.530
Inderjit Dhillon: what was X.

413
00:45:26.280 --> 00:45:29.109
Inderjit Dhillon: Let's see how big was. X

414
00:45:33.330 --> 00:45:44.800
Inderjit Dhillon: X was d plus one by N, okay, x was b plus one by end.

415
00:45:45.640 --> 00:45:48.260
Inderjit Dhillon: XX transpose.

416
00:45:48.460 --> 00:45:49.970
Inderjit Dhillon: Well, how big is this?

417
00:45:50.710 --> 00:45:53.739
Inderjit Dhillon: It's d plus one by d plus one.

418
00:45:56.880 --> 00:45:58.149
Inderjit Dhillon: What is W.

419
00:45:58.780 --> 00:46:00.290
Inderjit Dhillon: WS.

420
00:46:01.110 --> 00:46:02.310
Inderjit Dhillon: D. Plus one?

421
00:46:04.450 --> 00:46:09.069
Inderjit Dhillon: Okay, so if you look closely at this equation

422
00:46:10.350 --> 00:46:14.130
Inderjit Dhillon: Xx transpose is a d plus one by d plus one matrix.

423
00:46:14.880 --> 00:46:16.929
Inderjit Dhillon: And there are d plus one unknowns.

424
00:46:17.750 --> 00:46:23.280
Inderjit Dhillon: Okay, so this is d plus one linear equations.

425
00:46:27.180 --> 00:46:29.640
Inderjit Dhillon: and I have d plus one unknowns

426
00:46:38.510 --> 00:46:39.890
Inderjit Dhillon: perfect match. Right?

427
00:46:40.670 --> 00:46:46.589
Inderjit Dhillon: So that means that I can actually determine my W. Star uniquely.

428
00:46:49.450 --> 00:46:55.009
Inderjit Dhillon: although I haven't said some a few things right? When can I actually get a unique solution over here

429
00:46:55.970 --> 00:47:06.239
Inderjit Dhillon: again, linear algebra comes in right. I can write W. Star to be equal to

430
00:47:06.990 --> 00:47:13.730
Inderjit Dhillon: remember that Xx transpose is a matrix d plus one by d plus one matrix.

431
00:47:14.410 --> 00:47:18.330
Inderjit Dhillon: And under certain conditions it actually has an inverse.

432
00:47:20.320 --> 00:47:30.579
Inderjit Dhillon: And so I can write my W in closed form, as XX transpose inverse

433
00:47:30.720 --> 00:47:32.869
Inderjit Dhillon: times XWXY.

434
00:47:33.570 --> 00:47:35.310
Inderjit Dhillon: And this is okay.

435
00:47:35.970 --> 00:47:45.530
Inderjit Dhillon: If Xx transpose is invertible or non singular.

436
00:47:45.910 --> 00:47:51.620
Inderjit Dhillon: or it does not have a 0 eigenvalue and different characterizations.

437
00:47:52.750 --> 00:47:53.540
Inderjit Dhillon: Okay.

438
00:47:55.070 --> 00:47:59.800
Inderjit Dhillon: now, it turns out that this is not the best way to solve the problem. If you're doing it on a computer.

439
00:48:00.030 --> 00:48:02.419
Inderjit Dhillon: you don't actually want to take the inverse.

440
00:48:02.530 --> 00:48:11.300
Inderjit Dhillon: and after finding the inverse, multiply it by x 1, right? The right thing to do is actually to solve this.

441
00:48:11.970 --> 00:48:16.690
Inderjit Dhillon: Okay? And these are called the normal equation.

442
00:48:27.530 --> 00:48:36.920
Inderjit Dhillon: Okay? And we'll look towards that. We actually would want to solve the normal equations.

443
00:48:38.190 --> 00:48:41.579
Inderjit Dhillon: Okay, so I have now given you a derivation.

444
00:48:41.720 --> 00:48:51.389
Inderjit Dhillon: We've actually solved the problem right in some sense, as long as we know how to solve the normal equations which I'll talk a little bit about. We now know how to solve

445
00:48:51.660 --> 00:48:53.450
Inderjit Dhillon: the least squares. Regression point.

446
00:48:55.450 --> 00:48:57.210
Inderjit Dhillon: Okay? And what was the recipe?

447
00:48:57.960 --> 00:49:03.320
Inderjit Dhillon: We looked at a loss function which is the squared loss.

448
00:49:04.240 --> 00:49:08.069
Inderjit Dhillon: And we basically minimize it over the

449
00:49:08.410 --> 00:49:10.620
Inderjit Dhillon: training data that we are given.

450
00:49:12.600 --> 00:49:15.319
Inderjit Dhillon: And how did we do that. We took the.

451
00:49:15.620 --> 00:49:23.570
Inderjit Dhillon: We formulated it mathematically, and then we did some calculus and justified that the

452
00:49:23.950 --> 00:49:29.730
Inderjit Dhillon: I will basically put the gradient equals 0 right. And God.

453
00:49:31.270 --> 00:49:36.524
Inderjit Dhillon: Let me give you another way of thinking about this problem, right? Which is

454
00:49:37.590 --> 00:49:42.739
Inderjit Dhillon: more of a geometric way, and not as much algebraic. Okay.

455
00:49:43.950 --> 00:49:47.509
Inderjit Dhillon: this is actually the the way I like to think about the problem.

456
00:49:48.940 --> 00:49:51.650
Inderjit Dhillon: Okay, so let's talk a little bit about this geometric.

457
00:49:52.520 --> 00:49:57.410
Inderjit Dhillon: Okay, so remember that X transpose.

458
00:49:58.250 --> 00:50:10.449
Inderjit Dhillon: Okay, so remember, X was this matrix where each column was the training data with the one associated with it. So X transpose is one x, 1, transpose one x. 2, transpose

459
00:50:10.850 --> 00:50:18.969
Inderjit Dhillon: one XM. Transfers right, and if I write this as X bars.

460
00:50:19.290 --> 00:50:26.889
Inderjit Dhillon: then I have x, 1 bar, transpose x, 2 bar, transpose through Xn Bar transports.

461
00:50:29.040 --> 00:50:32.230
Inderjit Dhillon: Okay, and remember, my X transpose is

462
00:50:32.480 --> 00:50:35.369
Inderjit Dhillon: RN. By T. Plus one.

463
00:50:37.305 --> 00:50:44.640
Inderjit Dhillon: And what I want is that my X transpose W is approximately equal to what?

464
00:50:45.830 --> 00:50:48.719
Inderjit Dhillon: Okay, what is X transpose? W.

465
00:50:51.740 --> 00:50:59.250
Inderjit Dhillon: Well, X. Transpose. W is, a linear combination

466
00:51:03.530 --> 00:51:10.050
Inderjit Dhillon: of columns of X transpose.

467
00:51:11.340 --> 00:51:20.229
Inderjit Dhillon: Okay, in particular, it is W. Naught. Times. Column one plus w. 1

468
00:51:21.230 --> 00:51:28.900
Inderjit Dhillon: times. Column 2 of this matrix all the way to W.

469
00:51:29.320 --> 00:51:30.270
Inderjit Dhillon: D.

470
00:51:30.990 --> 00:51:33.730
Inderjit Dhillon: Times, column d. Plus one.

471
00:51:36.330 --> 00:51:42.979
Inderjit Dhillon: I can actually think of it as in a in a geometric way. Right? So I can basically say that, hey.

472
00:51:43.120 --> 00:51:48.560
Inderjit Dhillon: this is a linear these are linear combinations

473
00:51:49.530 --> 00:51:54.919
Inderjit Dhillon: right? And there are d plus one columns.

474
00:51:55.400 --> 00:52:00.810
Inderjit Dhillon: right? So it's a subspace of kind of dimension d plus one.

475
00:52:01.110 --> 00:52:10.840
Inderjit Dhillon: So I can think of it as that you know. Suppose, I say, that this is the column space

476
00:52:14.630 --> 00:52:19.749
Inderjit Dhillon: of X transpose, which is also known as the range space of X transpose.

477
00:52:23.880 --> 00:52:27.979
Inderjit Dhillon: If I take any linear combination. It'll give me some vector, over here.

478
00:52:28.400 --> 00:52:34.260
Inderjit Dhillon: X transpose. W, there's some arbitrary double, right?

479
00:52:34.500 --> 00:52:37.780
Inderjit Dhillon: But now, Y is a particular. Vector.

480
00:52:38.830 --> 00:52:45.210
Inderjit Dhillon: okay, so remember, y is rn, okay.

481
00:52:45.540 --> 00:52:47.819
Inderjit Dhillon: why may not lie in this subspace?

482
00:52:47.970 --> 00:52:54.360
Inderjit Dhillon: Okay for Y to lie in the subspace X transpose W must be equal to Y for some dumb.

483
00:52:55.340 --> 00:52:59.729
Inderjit Dhillon: But it may not be the case right? Because, remember, we are not insisting on equality.

484
00:53:00.410 --> 00:53:06.099
Inderjit Dhillon: So Y in general is not a vector. In this column space of X transpose.

485
00:53:06.580 --> 00:53:12.130
Inderjit Dhillon: But the optimal W. Star has the property, that

486
00:53:12.640 --> 00:53:19.540
Inderjit Dhillon: if I have X transpose W. Star, this, why.

487
00:53:20.610 --> 00:53:26.820
Inderjit Dhillon: the way to get this W. Star is to actually drop a perpendicular onto the space

488
00:53:28.760 --> 00:53:33.010
Inderjit Dhillon: because that perpendicular with kind of minimized by Pythagoras theorem

489
00:53:33.130 --> 00:53:36.889
Inderjit Dhillon: will actually minimize the distance to this linear space.

490
00:53:37.610 --> 00:53:38.310
Inderjit Dhillon: Okay?

491
00:53:39.030 --> 00:53:44.219
Inderjit Dhillon: So what you can say is that you know, for W. Star

492
00:53:44.350 --> 00:53:50.630
Inderjit Dhillon: to be an optimized to to be the optimal value, I should have the following, that X transpose. W

493
00:53:51.280 --> 00:54:00.960
Inderjit Dhillon: must be perpendicular, cool to. And remember, what is this vector over here?

494
00:54:01.360 --> 00:54:05.020
Inderjit Dhillon: This vector is y, minus x, transpose. W,

495
00:54:10.480 --> 00:54:19.110
Inderjit Dhillon: so X transpose W must be perpendicular to y minus X transpose W. Star for all of them.

496
00:54:20.450 --> 00:54:23.850
Inderjit Dhillon: Okay. And if that's the case, then it will be

497
00:54:25.450 --> 00:54:28.330
Inderjit Dhillon: it'll end up minimizing F of 12.

498
00:54:29.420 --> 00:54:35.829
Inderjit Dhillon: That means that W. Star would be argument of F of 2.

499
00:54:39.590 --> 00:54:46.350
Inderjit Dhillon: So remember when are 2 vectors orthogonal? Right? So for a to be perpendicular to B.

500
00:54:46.750 --> 00:54:49.850
Inderjit Dhillon: This means that a transpose B should be equal to 0.

501
00:54:51.180 --> 00:54:59.380
Inderjit Dhillon: If I say that I have X transpose W transpose times y minus x transpose W. Star

502
00:54:59.780 --> 00:55:03.370
Inderjit Dhillon: to be equal to 0 for all the

503
00:55:04.970 --> 00:55:10.050
Inderjit Dhillon: if I then, you know, do a little bit of algebra I have W transpose x

504
00:55:10.480 --> 00:55:13.599
Inderjit Dhillon: y. Minus x transpose W.

505
00:55:14.860 --> 00:55:17.440
Inderjit Dhillon: Star equal to 0 for all W.

506
00:55:25.270 --> 00:55:30.890
Inderjit Dhillon: And I have W transpose XY, minus XX, transpose

507
00:55:31.850 --> 00:55:34.910
Inderjit Dhillon: W. Star equal to 0 for all W.

508
00:55:35.600 --> 00:55:40.940
Inderjit Dhillon: And since this is supposed to be true for all W. That means that this right.

509
00:55:42.610 --> 00:55:44.810
Inderjit Dhillon: this quantity should be equal to 0,

510
00:55:45.440 --> 00:55:49.509
Inderjit Dhillon: which means that XX transpose W. Star

511
00:55:49.970 --> 00:55:52.490
Inderjit Dhillon: must be equal to XY.

512
00:55:56.980 --> 00:55:59.250
Inderjit Dhillon: So I did not do any calculus over here.

513
00:56:00.590 --> 00:56:07.280
Inderjit Dhillon: Right? Just use kind of Pythagoras theorem, some intuition about linear spaces, and so on. But I got the same solution.

514
00:56:10.730 --> 00:56:12.370
Inderjit Dhillon: same solution as over here.

515
00:56:16.900 --> 00:56:18.640
Inderjit Dhillon: So just trying to give you

516
00:56:19.190 --> 00:56:23.289
Inderjit Dhillon: geometric interpretation of what this W. Star is

517
00:56:24.460 --> 00:56:28.070
Inderjit Dhillon: right. So it is the coefficients or the vector

518
00:56:28.700 --> 00:56:33.080
Inderjit Dhillon: such that the residual Y minus X transpose. W. Star

519
00:56:33.220 --> 00:56:37.070
Inderjit Dhillon: is actually orthogonal to that entire space.

520
00:56:37.860 --> 00:56:38.650
Inderjit Dhillon: Okay.

521
00:56:39.670 --> 00:56:45.910
Inderjit Dhillon: so that's a different way of deriving it. Okay, so, but remember, finally, we get the same normal equations.

522
00:56:50.970 --> 00:56:53.230
Inderjit Dhillon: Okay, we get this again.

523
00:56:53.970 --> 00:56:59.970
Inderjit Dhillon: So not a surprise. Right? We are solving the same problem. We get the same solution. But this one was

524
00:57:01.300 --> 00:57:06.640
Inderjit Dhillon: derived from a geometric viewpoint.

525
00:57:13.350 --> 00:57:17.879
Inderjit Dhillon: And again reminding you, I have d plus one equations.

526
00:57:19.120 --> 00:57:23.020
Inderjit Dhillon: because Xx transpose is a d plus one by d plus one matrix.

527
00:57:23.290 --> 00:57:26.479
Inderjit Dhillon: And I have d plus one onwards.

528
00:57:32.280 --> 00:57:40.530
Inderjit Dhillon: And this has a unique solution. As long as Xx transpose is a non singular matrix

529
00:57:42.340 --> 00:57:49.590
Inderjit Dhillon: that may not always be the case. Right? It's not guaranteed that Xx transpose may be a non singular matrix. So, for example.

530
00:57:49.700 --> 00:57:58.690
Inderjit Dhillon: suppose the feature is repeated right, which means that if I look at the matrix x.

531
00:57:58.900 --> 00:58:01.149
Inderjit Dhillon: 2 rows are exactly identical.

532
00:58:02.190 --> 00:58:05.550
Inderjit Dhillon: In this case there will be linear dependence.

533
00:58:06.040 --> 00:58:08.510
Inderjit Dhillon: The rank of the matrix will be lower.

534
00:58:08.840 --> 00:58:14.659
Inderjit Dhillon: Xx. Transpose will not be singular, and there will not be a unique solution. In fact, there'll be an infinite number of solutions.

535
00:58:16.510 --> 00:58:23.870
Inderjit Dhillon: Okay? But in generic case, when you don't have linear dependence.

536
00:58:24.770 --> 00:58:26.940
Inderjit Dhillon: Okay, among the different features.

537
00:58:27.380 --> 00:58:33.060
Inderjit Dhillon: right? We will get a unique solution. So what we do need to do is 12

538
00:58:35.120 --> 00:58:41.310
Inderjit Dhillon: XX transpose times, W. Star equals. XY.

539
00:58:42.030 --> 00:58:46.850
Inderjit Dhillon: This is a linear system of equations.

540
00:58:48.370 --> 00:58:54.079
Inderjit Dhillon: I'm sure that all of you have studied this since all of you have taken a linear algebra course. Right?

541
00:58:54.580 --> 00:58:56.200
Inderjit Dhillon: So how do I solve this?

542
00:58:59.730 --> 00:59:01.090
Inderjit Dhillon: Any volunteers?

543
00:59:05.580 --> 00:59:12.290
Inderjit Dhillon: B, plus one diminary questions Xx transpose is a

544
00:59:13.190 --> 00:59:15.620
Inderjit Dhillon: d plus one by d plus one matrix.

545
00:59:16.380 --> 00:59:19.779
Inderjit Dhillon: And there are d plus one unknowns. Okay? So

546
00:59:20.610 --> 00:59:25.680
Inderjit Dhillon: I'm sure that all of you have looked at how to solve

547
00:59:25.820 --> 00:59:32.240
Inderjit Dhillon: such linear systems of equations. So the 1st method is to do dose and elimination

548
00:59:38.100 --> 00:59:45.610
Inderjit Dhillon: referrals XX transpose W. Star equals XY.

549
00:59:48.870 --> 00:59:51.249
Inderjit Dhillon: Now remember, these are prediction problems.

550
00:59:51.440 --> 00:59:56.620
Inderjit Dhillon: I said, N could be in the millions D could also actually be quite, quite large.

551
00:59:56.820 --> 01:00:00.730
Inderjit Dhillon: Right? So we need to quantify how expensive this is.

552
01:00:01.340 --> 01:00:06.350
Inderjit Dhillon: which means that how much time or how many operations does it take to solve this?

553
01:00:07.050 --> 01:00:11.430
Inderjit Dhillon: Okay, so what do we need to do? Well, to do this?

554
01:00:11.680 --> 01:00:20.110
Inderjit Dhillon: You 1st need to form the matrix Xx transpose.

555
01:00:21.520 --> 01:00:24.209
Inderjit Dhillon: Okay, let me call that matrix as a.

556
01:00:25.050 --> 01:00:27.090
Inderjit Dhillon: how many operations does that take?

557
01:00:28.900 --> 01:00:35.050
Inderjit Dhillon: But it's a d, by d plus one by N matrix. And I'm multiplying it

558
01:00:35.360 --> 01:00:39.930
Inderjit Dhillon: right? You can convince yourself that that'll take order, d square N operations

559
01:00:43.550 --> 01:00:46.341
Inderjit Dhillon: or a d square and floating point operations.

560
01:00:48.200 --> 01:00:54.590
Inderjit Dhillon: Then I need to form the right hand side. Which is this right? I've already formed this.

561
01:00:55.960 --> 01:01:02.510
Inderjit Dhillon: I need to form the right hand side

562
01:01:06.630 --> 01:01:09.790
Inderjit Dhillon: right, and let me call this XY equal to B,

563
01:01:11.450 --> 01:01:15.900
Inderjit Dhillon: okay, now, this is not very expensive. Right? This is all access.

564
01:01:16.050 --> 01:01:19.020
Inderjit Dhillon: D, by n, this only takes order.

565
01:01:19.850 --> 01:01:21.430
Inderjit Dhillon: Dn operations.

566
01:01:22.260 --> 01:01:30.130
Inderjit Dhillon: So that's not expensive. Now I have a I formed A, which is Xx. Transpose.

567
01:01:31.890 --> 01:01:35.700
Inderjit Dhillon: I have W. Star, which I need to solve for, and I have the right hand side.

568
01:01:36.370 --> 01:01:39.650
Inderjit Dhillon: So now this is in the form where you know use.

569
01:01:39.800 --> 01:01:45.859
Inderjit Dhillon: You've learned in your linear algebra course, how to kind of solve this right? And what does Gaussian elimination mean

570
01:01:46.080 --> 01:01:56.269
Inderjit Dhillon: that you take a, you factorize it as L of L times. U, okay, you do this decomposition.

571
01:01:59.430 --> 01:02:02.589
Inderjit Dhillon: L is a unit. Lower triangular matrix

572
01:02:02.790 --> 01:02:05.030
Inderjit Dhillon: u is an upper triangular matrix.

573
01:02:05.770 --> 01:02:10.310
Inderjit Dhillon: and this is also called Dawson illumination or led composition.

574
01:02:16.690 --> 01:02:21.090
Inderjit Dhillon: In this case we are actually in a slightly special case, right, because

575
01:02:21.210 --> 01:02:25.009
Inderjit Dhillon: my matrix has this form. A is equal to XX transpose.

576
01:02:25.430 --> 01:02:33.740
Inderjit Dhillon: So this is what's called a positive semi definite matrix.

577
01:02:34.900 --> 01:02:39.349
Inderjit Dhillon: Oh, my handwriting just goes, they wire sometimes.

578
01:02:41.220 --> 01:02:43.520
Inderjit Dhillon: Yeah, I think it's when I try to hurry up.

579
01:02:44.030 --> 01:02:49.800
Inderjit Dhillon: Okay, so it's a positive semi definite matrix.

580
01:02:51.690 --> 01:03:00.299
Inderjit Dhillon: When it's non singular, it's a positive, definite matrix. But it could be singular, right? And in this case I can actually write Lu decomposition

581
01:03:00.530 --> 01:03:03.559
Inderjit Dhillon: as what's called cholesky decomposition.

582
01:03:11.190 --> 01:03:15.129
Inderjit Dhillon: Okay, where U basically is. L transpose.

583
01:03:16.070 --> 01:03:16.910
Inderjit Dhillon: Okay.

584
01:03:17.310 --> 01:03:21.059
Inderjit Dhillon: And so let me just let you see what L is.

585
01:03:21.390 --> 01:03:23.629
Inderjit Dhillon: L is lower, triangular

586
01:03:27.740 --> 01:03:33.990
Inderjit Dhillon: U is sorry, and of course L. Transpose, then, is upper triangle.

587
01:03:43.430 --> 01:03:46.690
Inderjit Dhillon: So how does that help? How does Koleski decomposition help?

588
01:03:47.010 --> 01:03:50.029
Inderjit Dhillon: Well, if I factor ll transpose?

589
01:03:50.180 --> 01:03:53.570
Inderjit Dhillon: By the way, this does take to do this

590
01:03:54.040 --> 01:03:59.289
Inderjit Dhillon: takes a cubic number of operations cubic in the dimension.

591
01:04:02.080 --> 01:04:04.699
Inderjit Dhillon: but once I have ll transpose

592
01:04:06.370 --> 01:04:09.260
Inderjit Dhillon: I need to solve for W. Star.

593
01:04:09.530 --> 01:04:11.359
Inderjit Dhillon: I can do it in 2 steps

594
01:04:13.450 --> 01:04:17.090
Inderjit Dhillon: I can solve for L of Z equal to B,

595
01:04:17.910 --> 01:04:21.500
Inderjit Dhillon: and then L transpose W. Star equals.

596
01:04:23.460 --> 01:04:26.290
Inderjit Dhillon: Okay. And just think about what this is. This is a

597
01:04:26.500 --> 01:04:28.910
Inderjit Dhillon: L is a lower triangular matrix.

598
01:04:29.840 --> 01:04:33.929
Inderjit Dhillon: Z is a vector B is a vector

599
01:04:35.150 --> 01:04:41.410
Inderjit Dhillon: right? So the way I can solve it is basically the 1st equation I can solve for the 1st variable.

600
01:04:41.590 --> 01:04:45.399
Inderjit Dhillon: then I can solve using the second equation, and so on.

601
01:04:45.560 --> 01:04:47.870
Inderjit Dhillon: And this is called forward substitution.

602
01:04:55.480 --> 01:05:00.500
Inderjit Dhillon: Okay, and this is actually not too expensive. It's only order D squared. You can convince yourselves

603
01:05:01.020 --> 01:05:04.300
Inderjit Dhillon: right? Because you're basically just making one pass through this matrix.

604
01:05:04.590 --> 01:05:07.549
Inderjit Dhillon: And then when I come over here.

605
01:05:08.270 --> 01:05:14.180
Inderjit Dhillon: Then I get that. L transpose is an upper triangular matrix.

606
01:05:15.240 --> 01:05:21.510
Inderjit Dhillon: And I can basically use the last equation to get this W. Star d.

607
01:05:22.740 --> 01:05:24.799
Inderjit Dhillon: Then I know W. Star d.

608
01:05:25.130 --> 01:05:28.159
Inderjit Dhillon: I can use this last second last equation to get

609
01:05:28.450 --> 01:05:34.160
Inderjit Dhillon: W. Star D, minus one, and so on. So this is called backward substitution because I'm substituting backwards.

610
01:05:39.850 --> 01:05:42.299
Inderjit Dhillon: So I get order D square again.

611
01:05:43.900 --> 01:05:49.879
Inderjit Dhillon: Okay? So I can solve my normal equations

612
01:05:53.900 --> 01:05:55.959
Inderjit Dhillon: by Gaussian elimination.

613
01:05:58.070 --> 01:06:04.950
Inderjit Dhillon: In particular, koleski decomposition, followed by forward substitution, followed by backward substitution.

614
01:06:05.180 --> 01:06:09.890
Inderjit Dhillon: Right? And I can solve it in order.

615
01:06:11.250 --> 01:06:16.420
Inderjit Dhillon: D square n, which was to form the matrix last Dick.

616
01:06:16.990 --> 01:06:21.780
Inderjit Dhillon: The D square operations get subsumed by Dq, okay.

617
01:06:37.580 --> 01:06:44.889
Inderjit Dhillon: okay, so I've I've gone pretty quickly, right? And the reason I've gone pretty quickly in the last

618
01:06:45.270 --> 01:06:50.040
Inderjit Dhillon: 15min is because you've all told me that you've taken linear out

619
01:06:50.630 --> 01:06:54.450
Inderjit Dhillon: right? It's a prerequisite for the course, and so

620
01:06:56.510 --> 01:06:58.230
Inderjit Dhillon: is stuff that you already know.

621
01:06:59.110 --> 01:07:01.400
Inderjit Dhillon: So any questions or comments about that.

622
01:07:07.700 --> 01:07:12.050
Inderjit Dhillon: So typically when I have taught this course for 25 years.

623
01:07:12.360 --> 01:07:15.919
Inderjit Dhillon: typically, what ends up happening is when I ask students.

624
01:07:16.330 --> 01:07:20.239
Inderjit Dhillon: have you all taken linear algebra? Course, everybody's hand goes up

625
01:07:20.840 --> 01:07:28.120
Inderjit Dhillon: right. And then when I get into the details, everybody kind of starts saying, Oh, you know I don't really remember things right?

626
01:07:28.360 --> 01:07:33.180
Inderjit Dhillon: So what I will do is I will do a review of linear algebra

627
01:07:33.390 --> 01:07:36.610
Inderjit Dhillon: next time, but it'll be a very, very quick review.

628
01:07:36.770 --> 01:07:39.170
Inderjit Dhillon: Right? So there are things like.

629
01:07:39.540 --> 01:07:50.569
Inderjit Dhillon: you know. What are the positive, definite matrix? What are the Eigenvalues? What is the Svd of the matrix? And I'm going to assume that you know that right? It's it's not that the courses

630
01:07:51.040 --> 01:07:59.039
Inderjit Dhillon: totally rely reliant on that, but it's stuff that you know is very helpful to know when you're doing machine learning, especially if you want to understand

631
01:07:59.230 --> 01:08:03.569
Inderjit Dhillon: the theory or the mathematics behind many of the things that come up.

632
01:08:04.130 --> 01:08:11.479
Inderjit Dhillon: Okay? So again, like questions, comments, is this old hat? Should I not do the review?

633
01:08:12.640 --> 01:08:16.149
Inderjit Dhillon: Can I do a review of the linear algebra part next time.

634
01:08:17.109 --> 01:08:18.879
Hormoz Shahrzad: Review would be helpful.

635
01:08:20.220 --> 01:08:23.699
Inderjit Dhillon: Review review would be helpful. Okay, okay, good.

636
01:08:25.500 --> 01:08:30.339
Inderjit Dhillon: So now, I've given you only one way. By the way of solving this problem. Right? So.

637
01:08:32.220 --> 01:08:34.757
Inderjit Dhillon: I talked a little bit about

638
01:08:36.250 --> 01:08:40.030
Inderjit Dhillon: You know you get into trouble when the matrix is singular.

639
01:08:40.160 --> 01:08:42.740
Inderjit Dhillon: A is singular, right?

640
01:08:43.569 --> 01:08:47.430
Inderjit Dhillon: So you know, inverse.

641
01:08:49.130 --> 01:09:00.460
Inderjit Dhillon: Take off a or x, was it X transpose X or XX transpose. Xx transpose exists

642
01:09:01.979 --> 01:09:08.090
Inderjit Dhillon: only if A is non-singular.

643
01:09:10.140 --> 01:09:15.269
Inderjit Dhillon: That means that's all. The only time there's a unique solution is when it is non single.

644
01:09:16.240 --> 01:09:19.640
Inderjit Dhillon: But what happens if one feature is very close to another feature?

645
01:09:20.670 --> 01:09:26.020
Inderjit Dhillon: Right? There's something called the condition number of a matrix.

646
01:09:30.630 --> 01:09:31.410
Inderjit Dhillon: Okay?

647
01:09:31.819 --> 01:09:35.590
Inderjit Dhillon: So the condition number of a can be very large.

648
01:09:36.890 --> 01:09:41.199
Inderjit Dhillon: Okay? And that's equivalent to saying, if it is very large.

649
01:09:41.310 --> 01:09:44.180
Inderjit Dhillon: then A is close to singularity.

650
01:09:48.000 --> 01:09:52.139
Inderjit Dhillon: And then you get all sorts of problems with computing it on a computer.

651
01:09:52.350 --> 01:09:55.250
Inderjit Dhillon: right? Even though the solution is exactly unique.

652
01:09:55.430 --> 01:09:57.989
Inderjit Dhillon: you can run into kind of numerical issues.

653
01:09:58.230 --> 01:10:07.919
Inderjit Dhillon: Okay, it turns out that solving the normal equations is numerically not the best way of solving the problem.

654
01:10:09.290 --> 01:10:10.140
Inderjit Dhillon: Okay?

655
01:10:10.320 --> 01:10:18.330
Inderjit Dhillon: So solving normal equations, which is what I gave you above.

656
01:10:21.610 --> 01:10:27.020
Inderjit Dhillon: can yield large error.

657
01:10:30.330 --> 01:10:38.120
Inderjit Dhillon: When A is, we'll leave condition.

658
01:10:41.170 --> 01:10:43.390
Inderjit Dhillon: which means that the condition number is very large.

659
01:10:44.250 --> 01:10:51.430
Inderjit Dhillon: Okay, there are other methods of solving this problem.

660
01:10:52.590 --> 01:10:57.250
Inderjit Dhillon: Right? So remember, the original problem was what minimize

661
01:11:00.660 --> 01:11:04.219
Inderjit Dhillon: minimize this right minimize F of W, right?

662
01:11:04.660 --> 01:11:05.660
Inderjit Dhillon: So

663
01:11:09.300 --> 01:11:11.000
Inderjit Dhillon: there are other methods

664
01:11:14.720 --> 01:11:16.010
Inderjit Dhillon: which are better

665
01:11:20.240 --> 01:11:24.990
Inderjit Dhillon: when A is close to singular.

666
01:11:28.790 --> 01:11:34.279
Inderjit Dhillon: Okay, one of them is to use QR decomposition

667
01:11:40.350 --> 01:11:49.320
Inderjit Dhillon: of x, and the other is to use the singular value decomposition

668
01:12:00.730 --> 01:12:01.710
Inderjit Dhillon: offers.

669
01:12:03.410 --> 01:12:07.140
Inderjit Dhillon: Okay? And this is by far the best method.

670
01:12:09.140 --> 01:12:13.080
Inderjit Dhillon: I don't know. I guess I'm if I'm doing movie ratings, I would give it 5 stars.

671
01:12:13.860 --> 01:12:16.460
Inderjit Dhillon: Okay, but it is expensive.

672
01:12:17.520 --> 01:12:20.859
Inderjit Dhillon: It takes more competition to get the singular validity code.

673
01:12:22.480 --> 01:12:28.630
Inderjit Dhillon: Okay, so this is the most accurate

674
01:12:32.780 --> 01:12:36.680
Inderjit Dhillon: but more expensive.

675
01:12:44.270 --> 01:12:59.710
Inderjit Dhillon: Okay, so that leads us to, you know the end of this particular lecture. There was a reason, you know. I deliberately kind of made it a little fast paced in the beginning. For 1520min you would have been thinking, hey? Why is Professor going so slow? Right? But I think I've made up

676
01:12:59.970 --> 01:13:02.300
Inderjit Dhillon: towards the end of class right?

677
01:13:02.420 --> 01:13:10.729
Inderjit Dhillon: And one of the reasons I want to impress upon it is, there's a lot of linear algebra in machine learning. It'll really helps

678
01:13:10.830 --> 01:13:12.950
Inderjit Dhillon: to understand what is going on.

679
01:13:13.565 --> 01:13:17.409
Inderjit Dhillon: If you understand linear algebra like, I said, I will do a review.

680
01:13:17.510 --> 01:13:21.710
Inderjit Dhillon: But I want to emphasize again that that is a prerequisite for this course.

681
01:13:22.070 --> 01:13:30.120
Inderjit Dhillon: Right? So if you do not have the prerequisite, or you feel uncomfortable, please feel free to contact me or Nadesh.

682
01:13:30.960 --> 01:13:34.890
Inderjit Dhillon: Okay? Any questions before.

683
01:13:38.158 --> 01:13:45.930
Hormoz Shahrzad: One question. So if we have like, d plus one data point.

684
01:13:46.280 --> 01:13:50.660
Hormoz Shahrzad: and the matrix would be d plus one by d plus one right.

685
01:13:50.660 --> 01:13:52.108
Inderjit Dhillon: You mean the X data? Yes.

686
01:13:52.350 --> 01:13:52.880
Hormoz Shahrzad: Yes.

687
01:13:53.230 --> 01:14:05.520
Hormoz Shahrzad: So then, solving it with these methods are gonna give us like the exact like the overfitting scenario that you've talked about right.

688
01:14:05.520 --> 01:14:10.010
Inderjit Dhillon: Yeah, I mean, it'll give you the exact. So yeah, it'll give you equality. Yes.

689
01:14:10.990 --> 01:14:11.780
Hormoz Shahrzad: Okay.

690
01:14:11.780 --> 01:14:26.090
Inderjit Dhillon: But generally in prediction problems. That's not what you're looking for, right? Because what could happen is that they're actually, I mean, by the way. This is not the way to solve a regression problem, right? I haven't talked about something called regularization.

691
01:14:26.900 --> 01:14:27.310
Hormoz Shahrzad: Okay.

692
01:14:27.310 --> 01:14:29.270
Inderjit Dhillon: Which can actually lead to problems.

693
01:14:29.830 --> 01:14:37.439
Inderjit Dhillon: Right? So typically, you want to also add regularization. And we'll talk a little bit about ridge regularization and lasso regularization

694
01:14:37.700 --> 01:14:38.510
Inderjit Dhillon: right?

695
01:14:39.110 --> 01:14:43.919
Inderjit Dhillon: And so the goal is not to exactly make the training data set.

696
01:14:44.504 --> 01:14:50.180
Inderjit Dhillon: The prediction be exactly this. Same for the training data.

697
01:14:50.370 --> 01:14:56.649
Inderjit Dhillon: because what could happen is that maybe one of the training data actually, somehow, there's an error in it.

698
01:14:56.810 --> 01:14:58.319
Inderjit Dhillon: right? There's an outlier.

699
01:14:58.930 --> 01:15:03.550
Inderjit Dhillon: And if you try to fit it too. Well, then, you lead. Get get into problems.

700
01:15:05.100 --> 01:15:09.129
Hormoz Shahrzad: Yeah, yeah, I I I was just pointing it out that.

701
01:15:09.130 --> 01:15:13.439
Inderjit Dhillon: Yeah, yeah, no, no, yeah. That that point is well made. Yes, you're right.

702
01:15:14.560 --> 01:15:28.009
Hormoz Shahrzad: So therefore, basically, as you mentioned, we either bring in normalization, or we have some sort of a lower bound for the number of data points that we have like. If you have.

703
01:15:28.660 --> 01:15:32.090
Inderjit Dhillon: Yeah, yeah, not enough data. Because that might be.

704
01:15:33.450 --> 01:15:40.290
Inderjit Dhillon: yeah, you can actually have prediction problems where there are very, very few data points, and you are in very high dimensions.

705
01:15:40.820 --> 01:15:49.539
Inderjit Dhillon: Right? In that case the solution is not unique, but in there are certain cases like in compressed sensing, and so on, where you can actually get good solutions.

706
01:15:50.190 --> 01:16:03.149
Inderjit Dhillon: So remember, I mean, this is the 1st course in machine learning. I'm not going to go into detail in lectures, but but those are practical issues that come up in different prediction. Problems.

707
01:16:04.150 --> 01:16:04.910
Hormoz Shahrzad: Thank you.

708
01:16:06.430 --> 01:16:13.279
Inderjit Dhillon: Okay, well, thank you for listening. Monday is a holiday, and so I will see you all

709
01:16:14.140 --> 01:16:16.270
Inderjit Dhillon: next week on Wednesday.

710
01:16:16.540 --> 01:16:17.830
Inderjit Dhillon: Okay, thank you.

711
01:16:19.100 --> 01:16:19.789
Eddie: Thank you.

712
01:16:20.140 --> 01:16:20.810
Hormoz Shahrzad: Thanks.

713
01:16:21.230 --> 01:16:22.010
Chloe Chen: Thank you.

714
01:16:23.380 --> 01:16:24.220
Rishab Maheshwari: Thank you.

---- END OF LECTURE -------- START OF LECTURE 3 ----
WEBVTT

1
00:00:06.923 --> 00:00:11.762
Inderjit Dhillon: Okay, let me start before I start any questions.


2
00:00:13.183 --> 00:00:17.112
Inderjit Dhillon: And just to make sure everybody can hear me right. No issues.


3
00:00:18.033 --> 00:00:19.403
Hormoz Shahrzad: Yes, we can hear you.


4
00:00:19.553 --> 00:00:31.253
Inderjit Dhillon: Okay, awesome. Okay? So remember, last time we started talking about linear regression, we formulated the regression problem using least squares.


5
00:00:31.423 --> 00:00:40.963
Inderjit Dhillon: And then we saw that we got like a system of linear equations. And then we discussed how to potentially solve those linear equations right? And in the


6
00:00:41.083 --> 00:00:51.882
Inderjit Dhillon: in the, in that context, we saw there were various topics from linear algebra that were coming up. So today, I'll actually take a step aside and


7
00:00:52.113 --> 00:00:55.302
Inderjit Dhillon: do like a linear algebra background.


8
00:00:57.713 --> 00:01:02.642
Inderjit Dhillon: So think you can see my screen.


9
00:01:04.293 --> 00:01:18.293
Inderjit Dhillon: And so today I'll talk about then here, algebra background.


10
00:01:39.903 --> 00:01:40.683
Inderjit Dhillon: Okay.


11
00:01:42.683 --> 00:02:00.723
Inderjit Dhillon: okay. So you know, the the one of the actors in linear algebra. One of the main characters are these vectors. So I'll often call a D dimensional vector and say that it is X belongs to Rd, meaning that the elements of these vectors are real numbers.


12
00:02:01.053 --> 00:02:05.273
Inderjit Dhillon: So often we'll denote such a vector, as x, 1 x, 2


13
00:02:05.963 --> 00:02:09.372
Inderjit Dhillon: XD. So there are D numbers over here.


14
00:02:09.643 --> 00:02:25.463
Inderjit Dhillon: and then many times we may need to look at its transpose. And I'll donate the transpose by X transpose. And that becomes a row vector, so typically when I talk about a vector, it'll by default mean a


15
00:02:25.613 --> 00:02:27.363
Inderjit Dhillon: column. Vector.


16
00:02:27.623 --> 00:02:47.953
Inderjit Dhillon: and you know each, vector, I can represent it. So, for example, if I have Rd, of course, here it is, you know, 2 dimensional, but you have to think about it as being embedded in a d dimensional space. So each vector, can be represented by something like this. It has a magnitude.


17
00:02:48.093 --> 00:02:50.392
Inderjit Dhillon: and it has a particular direction.


18
00:02:50.553 --> 00:02:54.492
Inderjit Dhillon: And then, for example, if I have, I can do vector, operations.


19
00:02:54.833 --> 00:03:01.502
Inderjit Dhillon: So if I have XI have y, then basically, I have.


20
00:03:01.663 --> 00:03:11.552
Inderjit Dhillon: you know, I can stick Y over here, and then I will have x plus y being a vector okay, just through vector addition.


21
00:03:12.913 --> 00:03:18.423
Inderjit Dhillon: Now, like I said, vectors have a direction as well as a magnitude.


22
00:03:18.673 --> 00:03:22.892
Inderjit Dhillon: The magnitude is often called a vector, norm


23
00:03:25.223 --> 00:03:32.823
Inderjit Dhillon: in particular. When we did linear regression, we saw that there was a 2 norm which we talked about, but in general.


24
00:03:33.013 --> 00:03:36.063
Inderjit Dhillon: a vector norm is going to be a function


25
00:03:36.603 --> 00:03:46.623
Inderjit Dhillon: denoted by, you know, 2 bars on the outside which takes a D dimensional vector and maps it onto


26
00:03:48.733 --> 00:03:50.423
Inderjit Dhillon: non-negative numbers.


27
00:03:50.913 --> 00:04:02.833
Inderjit Dhillon: Okay, so think about it as the magnitude of a vector right, it can be 0, or it can be something which is positive. Now, vector norms can have some abstract properties that they should satisfy.


28
00:04:03.213 --> 00:04:06.582
Inderjit Dhillon: Right? So there are 3 properties in particular.


29
00:04:06.803 --> 00:04:16.883
Inderjit Dhillon: like, I said before that a vector norm should, the is kind of capturing the magnitude. So it's greater than equal to 0. Okay? And


30
00:04:17.423 --> 00:04:26.223
Inderjit Dhillon: the 0 value can only be obtained from the 0. Vector so X norm of X equal to 0


31
00:04:26.843 --> 00:04:30.523
Inderjit Dhillon: if and only if X equal to 0.


32
00:04:31.163 --> 00:04:40.102
Inderjit Dhillon: Okay, now remember that just to make sure that you know this is a D dimensional vector whereas this vector over this


33
00:04:40.233 --> 00:04:43.222
Inderjit Dhillon: over here is a scalar.


34
00:04:43.473 --> 00:04:54.423
Inderjit Dhillon: So I will not always put a bar over here to denote a vector but it should be clear from context, right? So only the 0 vector, has norm equal to 0.


35
00:04:54.593 --> 00:04:58.223
Inderjit Dhillon: If I take a vector and I multiply it by Alpha.


36
00:04:58.653 --> 00:05:05.333
Inderjit Dhillon: the second property that all vector norms should satisfy is called homogeneity, which means that


37
00:05:05.803 --> 00:05:16.252
Inderjit Dhillon: when I multiply it by a scalar alpha, which could be negative, or it could be positive. Right? Then the magnitude, the vector norm basically becomes


38
00:05:16.463 --> 00:05:20.482
Inderjit Dhillon: multiplied by the absolute value of alpha.


39
00:05:21.043 --> 00:05:26.093
Inderjit Dhillon: So this 1st one was, you know, non-negativity.


40
00:05:30.913 --> 00:05:34.183
Inderjit Dhillon: And this second property is called homogeneity.


41
00:05:38.053 --> 00:05:43.263
Inderjit Dhillon: And the 3rd property, which is very important, is called the triangle inequality.


42
00:05:43.533 --> 00:05:48.432
Inderjit Dhillon: which means that if I take the vector x plus y.


43
00:05:48.713 --> 00:05:55.823
Inderjit Dhillon: then the magnitude, the norm of X plus y will actually be less than equal to the norm of X


44
00:05:56.263 --> 00:06:01.573
Inderjit Dhillon: plus the normal. Y, so these are properties that we kind of expect.


45
00:06:01.733 --> 00:06:04.192
Inderjit Dhillon: you know, in the Euclidean space.


46
00:06:05.237 --> 00:06:16.493
Inderjit Dhillon: But you can define generally a vector norm as a function from Rd to R plus, which


47
00:06:16.803 --> 00:06:26.803
Inderjit Dhillon: basically obeys these properties in class. The other day we looked at in particular, we looked at the tuner.


48
00:06:28.133 --> 00:06:41.632
Inderjit Dhillon: Okay? So the 2 norm is so remember that X is at Rd is in Rd, so the numbers can be, you know, components of the vector, can be negative or positive. So the 2 norm is going to be. You basically take the


49
00:06:42.013 --> 00:06:48.132
Inderjit Dhillon: some square x 1 square plus x 2 square of the absolute values


50
00:06:49.603 --> 00:06:52.402
Inderjit Dhillon: XD square. And then I take a square root.


51
00:06:52.733 --> 00:06:56.382
Inderjit Dhillon: Okay, so this is known as the L. 2 norm.


52
00:06:58.233 --> 00:07:02.502
Inderjit Dhillon: Anybody else want to tell me some other norms that they might know of.


53
00:07:04.423 --> 00:07:07.222
Hormoz Shahrzad: So l, 1 norm can be just


54
00:07:07.753 --> 00:07:12.912
Hormoz Shahrzad: so like the sum of absolute values of the components of the vector.


55
00:07:12.913 --> 00:07:22.532
Inderjit Dhillon: Exactly so. An l 1 norm is going to be, you know, very similar to what is over here, except that we will not square, and we will not take the square root.


56
00:07:23.153 --> 00:07:28.362
Inderjit Dhillon: So it'll just be the summation of the absolute values


57
00:07:32.873 --> 00:07:38.923
Inderjit Dhillon: in machine learning. Generally, we don't really need to go to complex numbers. But these properties generalize. If


58
00:07:39.113 --> 00:07:53.014
Inderjit Dhillon: the vector is a complex, vector which means that every component is a complex number. And then you know the absolute value sign is the magnitude of that complex number. Okay,


59
00:07:53.653 --> 00:07:58.192
Inderjit Dhillon: So besides x, 1 norm and 2 norm, any other norm.


60
00:08:01.423 --> 00:08:04.062
Hormoz Shahrzad: Infinity, nor which would be the only.


61
00:08:04.063 --> 00:08:07.252
Inderjit Dhillon: So the infinity norm. And what would be the infinity norm.


62
00:08:08.556 --> 00:08:15.363
Hormoz Shahrzad: It's the maximum you know the absolute value of the components.


63
00:08:15.843 --> 00:08:20.752
Inderjit Dhillon: So it's the maximum absolute value. Right? So I have d components. I look at


64
00:08:20.923 --> 00:08:26.203
Inderjit Dhillon: the maximum absolute value among them, and that's called the infinity norm.


65
00:08:28.093 --> 00:08:34.823
Inderjit Dhillon: And all these 3 norms are general, are special cases of the general P. Norm.


66
00:08:35.863 --> 00:08:37.873
Inderjit Dhillon: So I can actually define A. P. Norm.


67
00:08:38.203 --> 00:08:45.593
Inderjit Dhillon: where I take the pith power of every component.


68
00:08:45.993 --> 00:08:50.823
Inderjit Dhillon: So think about P. Equal to one p. Equal to 2 p. Equal to 3 p. Equal, 4, and so on.


69
00:08:53.603 --> 00:08:56.823
Inderjit Dhillon: And then I take the sorry I.


70
00:09:01.933 --> 00:09:04.572
Inderjit Dhillon: And then I take the pith root of it.


71
00:09:05.723 --> 00:09:12.483
Inderjit Dhillon: Okay, so this is in general, the lp, norm, okay.


72
00:09:12.643 --> 00:09:25.962
Inderjit Dhillon: mostly in machine learning, we will end up using. And we've already seen that right that we've used the L 2 norm. But we'll see, even when we talk about regression that we will use the l 1. Normals.


73
00:09:26.203 --> 00:09:31.318
Inderjit Dhillon: Okay. l, 1. Norm ends up leading to sparsity in the


74
00:09:31.913 --> 00:09:34.393
Inderjit Dhillon: coefficients of the model that we find.


75
00:09:34.543 --> 00:09:39.673
Inderjit Dhillon: Okay, so that's very quick review of vector norms.


76
00:09:40.331 --> 00:09:41.794
Inderjit Dhillon: Like, I said,


77
00:09:42.413 --> 00:09:59.943
Inderjit Dhillon: vector has a direction and a magnitude. Sometimes you may need to look at the direction of a vector and so what you can do is you can say that, you know, if X hat is the direction of A, vector, I can get it by looking at X divided by


78
00:10:00.393 --> 00:10:01.643
Inderjit Dhillon: normal fix.


79
00:10:01.953 --> 00:10:14.303
Inderjit Dhillon: Okay, so that is the unit vector in the direction, all thanks.


80
00:10:14.843 --> 00:10:21.982
Inderjit Dhillon: And what do I mean by unit. I just mean that the norm of this vector is one.


81
00:10:23.113 --> 00:10:28.942
Inderjit Dhillon: Okay? Now, clearly, you know, like, I said, you have different norms.


82
00:10:29.253 --> 00:10:35.993
Inderjit Dhillon: Right? So the this might have. If I divide by x 1, it'll have unit


83
00:10:36.383 --> 00:10:42.813
Inderjit Dhillon: l. 1 norm. If I look at X 2, it'll have unit L, 2 norm, okay.


84
00:10:43.033 --> 00:11:04.153
Inderjit Dhillon: and the L 2 norm unit vector which has L 2 norm equal to one is kind of important because it plays a role in things called projections and projectors. And as we saw, like geometrically, the least squares problem is just a projection of your


85
00:11:04.473 --> 00:11:10.643
Inderjit Dhillon: target vector B onto kind of the range space of this matrix. X,


86
00:11:10.783 --> 00:11:13.769
Inderjit Dhillon: okay, so let's look a little bit at


87
00:11:14.283 --> 00:11:24.003
Inderjit Dhillon: vectors that are unit L, 2 norm, right? So if I now take X hat X hat transpose.


88
00:11:24.813 --> 00:11:29.402
Inderjit Dhillon: Okay, now, what is excited, exact, transpose right? This is a


89
00:11:30.873 --> 00:11:39.143
Inderjit Dhillon: X side, is a column vector, Xi transpose is a row. Vector.


90
00:11:40.093 --> 00:11:44.083
Inderjit Dhillon: right? So this is actually a d, by d matrix.


91
00:11:44.923 --> 00:11:52.863
Inderjit Dhillon: So x hat X hat transpose is not just a vector also, but it is a dy matrix.


92
00:11:54.573 --> 00:12:03.063
Inderjit Dhillon: It is a special dy d matrix because it is rank one. It basically has only d parameters. But


93
00:12:03.273 --> 00:12:14.603
Inderjit Dhillon: you can interpret this in a geometric fashion. And so X hat X hat transpose where X hat is a unit, vector is an orthogonal


94
00:12:16.523 --> 00:12:27.113
Inderjit Dhillon: projector on to the vector X or accept.


95
00:12:28.623 --> 00:12:42.833
Inderjit Dhillon: Okay, because they are, you know, the orthogonal projector doesn't really depend upon the direction, the magnitude of the vector okay? So just so that you kind of understand what I'm saying, right? So if I have X hat


96
00:12:43.593 --> 00:12:45.333
Inderjit Dhillon: X head transpose.


97
00:12:45.893 --> 00:12:50.433
Inderjit Dhillon: And I then project a vector Y onto this.


98
00:12:50.933 --> 00:13:00.543
Inderjit Dhillon: So that means that I multiply Y over here. I can write it as X hat. Times X hat transpose y.


99
00:13:03.443 --> 00:13:09.972
Inderjit Dhillon: So remember, this is now just the inner product between X hat and y.


100
00:13:10.492 --> 00:13:12.443
Inderjit Dhillon: right? So this is just a scalar.


101
00:13:14.653 --> 00:13:15.533
Inderjit Dhillon: Okay?


102
00:13:15.722 --> 00:13:20.383
Inderjit Dhillon: And this is like we said before, is a unit. Vector.


103
00:13:23.043 --> 00:13:31.603
Inderjit Dhillon: Okay, so geometrically, what is happening is that, suppose I have X over here.


104
00:13:32.033 --> 00:13:39.493
Inderjit Dhillon: Okay, then I normalize it to have unit L 2 norm.


105
00:13:39.613 --> 00:13:45.422
Inderjit Dhillon: which means that I'm actually putting this X hat on the surface of a


106
00:13:46.923 --> 00:13:54.723
Inderjit Dhillon: sphere generally. Here, of course, I'm in 2 dimension. I'm doing it in 2 dimensions. So it's a circle. So X hat is


107
00:13:55.483 --> 00:13:56.713
Inderjit Dhillon: this, vector


108
00:14:00.603 --> 00:14:19.973
Inderjit Dhillon: okay? And now, if I have some vector. X over here. Now, you have to, just, you know, use your imagination a little bit right? Because we are trying to visualize D dimensional space. But we are, of course, only drawing on 2 dimensional, you know, piece of paper, or in this case, an ipad.


109
00:14:20.053 --> 00:14:31.343
Inderjit Dhillon: Okay, so you can think about like Y as not necessarily being within this plane, it can actually be outside this plane. It could be inside the plane also. Right?


110
00:14:31.673 --> 00:14:41.493
Inderjit Dhillon: And then you kind of when I say, projected, you drop a perpendicular down onto X, okay.


111
00:14:41.663 --> 00:14:52.353
Inderjit Dhillon: and this magnitude is X hat. Transpose Y, and this, vector.


112
00:14:52.753 --> 00:14:57.273
Inderjit Dhillon: so remember, a vector, has magnitude and direction.


113
00:14:57.763 --> 00:15:04.643
Inderjit Dhillon: So this vector, maybe I can take advantage of the fact that I have different colors over here.


114
00:15:05.453 --> 00:15:10.493
Inderjit Dhillon: So this vector, over here is


115
00:15:11.143 --> 00:15:18.972
Inderjit Dhillon: well, its magnitude is X transpose. Xi transpose Y, and its direction is accept.


116
00:15:20.603 --> 00:15:24.903
Inderjit Dhillon: Okay? And I can always, you know, write the scalar later.


117
00:15:25.103 --> 00:15:28.523
Inderjit Dhillon: So it is X head. Times X head. Transpose.


118
00:15:29.593 --> 00:15:35.093
Inderjit Dhillon: Why, which is the same as X hat X hat. Transpose times.


119
00:15:37.293 --> 00:15:39.692
Inderjit Dhillon: Okay, so that is kind of the geometric.


120
00:15:39.843 --> 00:15:40.873
Inderjit Dhillon: Oh,


121
00:15:42.393 --> 00:15:59.113
Inderjit Dhillon: meaning of this. And you basically have now an orthogonal projector onto the vector and you can actually generalize it when you have not only just one vector but you have multiple vectors, and then it becomes orthogonal projector onto a particular subspace.


122
00:15:59.453 --> 00:16:04.012
Inderjit Dhillon: Okay, and then,


123
00:16:05.403 --> 00:16:14.662
Inderjit Dhillon: let me kind of, you know, just talk a little bit about inner products. So if I have X hat, transpose y right. That was the magnitude of the projection.


124
00:16:14.893 --> 00:16:19.013
Inderjit Dhillon: That's the same as X divided by norm of X


125
00:16:20.163 --> 00:16:23.302
Inderjit Dhillon: transpose y, that's the same as


126
00:16:23.703 --> 00:16:31.983
Inderjit Dhillon: X transpose y divided by norm of X. And remember over here that norm of X is actually fixed


127
00:16:32.103 --> 00:16:33.323
Inderjit Dhillon: to be the tuner.


128
00:16:34.983 --> 00:16:49.702
Inderjit Dhillon: Okay? Orthogonal projector only makes sense in this. When I have this 2 norm, right? So I can actually even write this as X transpose y. And remember, the 2 norm is sum of squares. Right? So actually, I can write this as


129
00:16:50.023 --> 00:16:52.943
Inderjit Dhillon: X transpose X and square root of this.


130
00:16:55.513 --> 00:17:00.063
Inderjit Dhillon: Okay, but we'll kind of just leave it over here right over this part.


131
00:17:01.473 --> 00:17:11.593
Inderjit Dhillon: So in general, if I have X transpose Y, there is a important inequality


132
00:17:11.973 --> 00:17:15.153
Inderjit Dhillon: called the Cauchy-schwartz inequality which says that


133
00:17:15.363 --> 00:17:20.742
Inderjit Dhillon: norm of X transpose Y is less than equal to norm of x times, norm of y.


134
00:17:21.583 --> 00:17:29.843
Inderjit Dhillon: This is the Kashi Schwartz inequality.


135
00:17:33.773 --> 00:17:41.323
Inderjit Dhillon: Okay? And so I can actually take, you know, X transpose y


136
00:17:42.503 --> 00:17:46.553
Inderjit Dhillon: if I move the right hand side. And again, remember, this is also


137
00:17:47.093 --> 00:18:06.252
Inderjit Dhillon: this is Cauchy-schwartz inequality. This is the 2 norm. So many times when I, when we omit the 2 norm, we will actually, it will be implicit that it is the 2 norm. Okay? So when I don't write anything in a subscript, then, you know, it'll be we'll assume it to be the 2 norm, unless I explicitly say so. Otherwise. Okay.


138
00:18:06.393 --> 00:18:12.332
Inderjit Dhillon: So now, if I take X transpose y divided by norm of X, divided by norm of Y


139
00:18:12.603 --> 00:18:15.503
Inderjit Dhillon: by the Cauchy-schwartz inequality over here.


140
00:18:15.633 --> 00:18:20.673
Inderjit Dhillon: Right? This is. This is like taking this and dividing by this.


141
00:18:20.823 --> 00:18:24.852
Inderjit Dhillon: So it's clear that this is less than equal to one.


142
00:18:25.403 --> 00:18:29.423
Inderjit Dhillon: And what that means is that X transpose y


143
00:18:29.663 --> 00:18:37.363
Inderjit Dhillon: divided by norm of X, divided by norm of Y lies between plus one and minus one.


144
00:18:38.193 --> 00:18:47.113
Inderjit Dhillon: Okay? And you can see that you can interpret this to be the cost Theta, where, if you have


145
00:18:47.453 --> 00:18:52.132
Inderjit Dhillon: X and y, then this is the angle, theta.


146
00:18:54.083 --> 00:18:58.513
Inderjit Dhillon: Okay, maybe I can make a line over here


147
00:18:59.763 --> 00:19:02.802
Inderjit Dhillon: and said, this is exactly the same thing as this data.


148
00:19:03.953 --> 00:19:10.743
Inderjit Dhillon: Okay, so now we have the angle between 2 vectors X and Y,


149
00:19:11.903 --> 00:19:25.713
Inderjit Dhillon: and we are saying that X transpose Y, divided by norm of X, divided by norm of Y, can be interpreted as cost of theta right? You can actually see it over here. Right? This is theta, you know that cost is this divided by this?


150
00:19:26.063 --> 00:19:29.513
Inderjit Dhillon: And this is normal. Y, okay?


151
00:19:30.043 --> 00:19:37.562
Inderjit Dhillon: And so so what happens when X and y are perpendicular to each other.


152
00:19:39.603 --> 00:19:45.632
Inderjit Dhillon: Right? You also have probably learned that when X and y are perpendicular to each other, X and y.


153
00:19:46.463 --> 00:19:50.473
Inderjit Dhillon: or perpendicular to each other


154
00:19:55.393 --> 00:20:02.293
Inderjit Dhillon: is equivalent to saying that X transpose y equal to 0.


155
00:20:03.603 --> 00:20:09.492
Inderjit Dhillon: Okay, theta equal to 90 degrees.


156
00:20:12.543 --> 00:20:13.493
Inderjit Dhillon: Okay?


157
00:20:13.613 --> 00:20:20.772
Inderjit Dhillon: So you know, like, I said, vectors from a very component, a very important component in linear algebra.


158
00:20:20.923 --> 00:20:27.983
Inderjit Dhillon: But the main actor, the main character in linear algebra is a matrix.


159
00:20:28.873 --> 00:20:33.572
Inderjit Dhillon: Okay. So if I have a matrix A, and I say, this is RM. By n.


160
00:20:35.483 --> 00:20:41.752
Inderjit Dhillon: this means that it is. You know, if I kind of look at it. I can write it as


161
00:20:42.433 --> 00:20:50.253
Inderjit Dhillon: A as a 1, 1, a. 1, 2. I have basically M and numbers a, 1 n.


162
00:20:51.083 --> 00:20:54.213
Inderjit Dhillon: Am. One am 2 a.


163
00:20:56.813 --> 00:21:02.123
Inderjit Dhillon: But it's much more than just us array of numbers.


164
00:21:02.283 --> 00:21:03.113
Inderjit Dhillon: Right?


165
00:21:14.793 --> 00:21:17.852
Inderjit Dhillon: Okay, so what does the matrix do?


166
00:21:17.983 --> 00:21:19.763
Inderjit Dhillon: Right? So matrix.


167
00:21:19.883 --> 00:21:29.233
Inderjit Dhillon: And this is why it's called linear algebra, and it's not, you know. In some sense it is the same as learning about matrices


168
00:21:29.643 --> 00:21:36.192
Inderjit Dhillon: that a is actually a linear transformation from our end.


169
00:22:19.153 --> 00:22:20.923
Inderjit Dhillon: Okay, I guess.


170
00:22:25.913 --> 00:22:29.882
Inderjit Dhillon: I guess my connection dropped. Did everybody else's connection drop, or just mine.


171
00:22:31.923 --> 00:22:33.893
Nilesh Gupta: Oh, it was just yours!


172
00:22:34.103 --> 00:22:35.902
Inderjit Dhillon: This one interesting?


173
00:22:36.313 --> 00:22:41.863
Inderjit Dhillon: Okay? And then I see some things in the chat. Is that just because I dropped off for a little bit?


174
00:22:43.033 --> 00:22:47.702
Inderjit Dhillon: Okay, okay, so, and I guess I need to reshare my screen. Probably right?


175
00:22:47.943 --> 00:22:48.723
Nilesh Gupta: Okay.


176
00:22:48.993 --> 00:22:51.572
Inderjit Dhillon: Okay. Sorry about that. I don't know what happened.


177
00:23:01.753 --> 00:23:05.382
Inderjit Dhillon: I see. So right now it's saying sharing is not turned on Nilesh


178
00:23:06.684 --> 00:23:13.943
Inderjit Dhillon: so I don't know if I'm logged in twice, or send request.


179
00:23:14.493 --> 00:23:18.082
Chloe Chen: Think someone was someone else, was made host by Zoom. So.


180
00:23:18.083 --> 00:23:19.942
Nilesh Gupta: Oh, I see! It might have.


181
00:23:20.868 --> 00:23:21.793
Inderjit Dhillon: Okay.


182
00:23:21.973 --> 00:23:22.703
Nilesh Gupta: Yeah.


183
00:23:22.703 --> 00:23:24.573
Inderjit Dhillon: So can you make me the.


184
00:23:24.853 --> 00:23:26.552
Nilesh Gupta: Yeah, I think I just did. Can you try?


185
00:23:26.553 --> 00:23:32.053
Inderjit Dhillon: Yeah, yeah. So now let me just protect my


186
00:23:32.713 --> 00:23:37.903
Inderjit Dhillon: ipad screen. I have to do this again. Sorry about this.


187
00:23:38.903 --> 00:23:40.083
Inderjit Dhillon: Oh.


188
00:23:47.873 --> 00:23:50.502
Inderjit Dhillon: screen sharing. Zoom.


189
00:23:57.193 --> 00:24:02.453
Inderjit Dhillon: Okay, okay. I think we are back.


190
00:24:03.513 --> 00:24:04.153
Hormoz Shahrzad: Yes.


191
00:24:04.463 --> 00:24:20.772
Inderjit Dhillon: Okay, so a matrix M, by N will map n dimensional vectors to M dimensional vectors. And why is it called a linear transformation? Well, a linear. It's because if I take now a linear combination.


192
00:24:25.663 --> 00:24:27.733
Inderjit Dhillon: if I take a linear combination


193
00:24:30.743 --> 00:24:31.453
Inderjit Dhillon: of


194
00:24:32.653 --> 00:24:41.703
Inderjit Dhillon: x and y, so if I take, if I have 2 vectors, and I take a linear combination, alpha x plus beta y.


195
00:24:42.033 --> 00:24:45.123
Inderjit Dhillon: Then what ends up happening is that


196
00:24:45.453 --> 00:24:51.462
Inderjit Dhillon: this is linear. So this will happen that it will be Alf a times alpha x


197
00:24:51.953 --> 00:24:57.022
Inderjit Dhillon: plus beta y, and that will be equal to alpha times a times x


198
00:24:57.493 --> 00:25:00.073
Inderjit Dhillon: plus beta times a times. Y,


199
00:25:01.563 --> 00:25:08.422
Inderjit Dhillon: okay, so basically, you get again, Alpha and Beta over here, right of the


200
00:25:08.583 --> 00:25:11.383
Inderjit Dhillon: image or linear transformation of X


201
00:25:11.813 --> 00:25:22.283
Inderjit Dhillon: and Beta times, the linear transformation of Y, okay, so many times again, we try to visualize it right? Can't really do it in n dimensional.


202
00:25:22.503 --> 00:25:28.763
Inderjit Dhillon: you know, if it's high dimensional space. But let's just represent it. So suppose I have X,


203
00:25:29.473 --> 00:25:31.982
Inderjit Dhillon: and remember what space was. X in


204
00:25:33.183 --> 00:25:42.923
Inderjit Dhillon: X was a vector, in rm, so it's M, no, it's rn, okay.


205
00:25:43.373 --> 00:25:52.013
Inderjit Dhillon: so it's rn, so it's n dimensional space. There are N numbers over here. I do a times X,


206
00:25:53.083 --> 00:25:56.993
Inderjit Dhillon: and this will get transformed into.


207
00:25:57.783 --> 00:26:00.893
Inderjit Dhillon: Let's say this, vector, a times X,


208
00:26:01.173 --> 00:26:04.972
Inderjit Dhillon: and this will be in M dimensional space.


209
00:26:05.773 --> 00:26:12.263
Inderjit Dhillon: Okay, so really, that's what a matrix is about. It's a linear transformation. It's not just


210
00:26:12.793 --> 00:26:16.193
Inderjit Dhillon: an Mn. Array of numbers.


211
00:26:16.333 --> 00:26:24.173
Inderjit Dhillon: It's an Mn. Array of numbers, and there's a particular rule of multiplying it with vectors, and that gives us the linear transformation.


212
00:26:25.523 --> 00:26:36.363
Inderjit Dhillon: So what are are things that you should know about matrices? So just like vectors have a magnitude.


213
00:26:37.023 --> 00:26:42.293
Inderjit Dhillon: matrices can also have magnitudes. So we define matrix norms.


214
00:26:46.243 --> 00:26:52.893
Inderjit Dhillon: So if I have a and this belongs to this is the Mn matrix.


215
00:26:53.283 --> 00:26:56.262
Inderjit Dhillon: So remember my norm.


216
00:26:56.703 --> 00:27:05.183
Inderjit Dhillon: I designate it like this, and this will be a function which maps a matrix, 2.


217
00:27:05.943 --> 00:27:08.963
Inderjit Dhillon: A non-negative real number.


218
00:27:09.863 --> 00:27:13.623
Inderjit Dhillon: And what properties should a matrix norm satisfy?


219
00:27:13.893 --> 00:27:18.992
Inderjit Dhillon: Again, there will be 3 properties very much analogous to vector norms.


220
00:27:20.353 --> 00:27:25.773
Inderjit Dhillon: So norm of A is greater than equal to 0 and


221
00:27:26.293 --> 00:27:30.493
Inderjit Dhillon: the norm of a equals 0 if and only if


222
00:27:32.583 --> 00:27:40.472
Inderjit Dhillon: a is the 0. Matrix. Okay? Like, I said before, this is non-negativity


223
00:27:42.743 --> 00:27:48.173
Inderjit Dhillon: homogeneity. I take a vector sorry. I take a matrix, A,


224
00:27:48.863 --> 00:27:56.042
Inderjit Dhillon: multiply it by alpha. So remember that means that I'm multiplying each component each of the Mn numbers by Alpha.


225
00:27:56.343 --> 00:28:02.072
Inderjit Dhillon: And this is going to be equal to absolute value of alpha times. The norm of a


226
00:28:03.333 --> 00:28:05.923
Inderjit Dhillon: okay. So this is homogeneity.


227
00:28:11.193 --> 00:28:17.943
Inderjit Dhillon: Okay? And then the 3, rd again, is very similar, which is the triangle inequality.


228
00:28:18.753 --> 00:28:25.462
Inderjit Dhillon: The norm norm of a plus B is less than equal to norm of a plus norm of big.


229
00:28:27.533 --> 00:28:30.442
Inderjit Dhillon: Okay? And that's the final inequality.


230
00:28:39.053 --> 00:28:43.533
Inderjit Dhillon: Okay, so and can anybody tell me a particular matrix norm.


231
00:28:47.143 --> 00:28:50.732
Inderjit Dhillon: So this is a property that any matrix norm should satisfy.


232
00:28:52.523 --> 00:29:00.272
Inderjit Dhillon: In the case of vectors, we saw that you had the l 1 norm, you had the L. 2 norm. You had the l infinity norm, and these were all special cases of the


233
00:29:00.673 --> 00:29:02.923
Inderjit Dhillon: the Lp. Norm.


234
00:29:04.583 --> 00:29:06.652
Inderjit Dhillon: What about? In the case of matrices.


235
00:29:10.983 --> 00:29:16.383
Inderjit Dhillon: I can take what's called the Frobenius norm, right? So it's often denoted as


236
00:29:19.763 --> 00:29:23.693
Inderjit Dhillon: a subscript F for frobe news.


237
00:29:23.973 --> 00:29:30.593
Inderjit Dhillon: And it's basically the sum of squares of all the Mn. Numbers, and then you take the square root.


238
00:29:31.363 --> 00:29:38.902
Inderjit Dhillon: So summation I equal to one to M. Summation J. Is equal to one to N.


239
00:29:39.853 --> 00:29:49.713
Inderjit Dhillon: A, IJ square, okay. And this is called the for many as well.


240
00:29:55.423 --> 00:30:00.453
Inderjit Dhillon: Any other norm matrix store.


241
00:30:07.772 --> 00:30:13.323
Hormoz Shahrzad: Similarly, we can have the the summation of the absolute values.


242
00:30:13.323 --> 00:30:21.203
Inderjit Dhillon: Yeah, so I can actually have, I can essentially treat this matrix. Mn, matrix, m, by N, matrix


243
00:30:21.883 --> 00:30:24.862
Inderjit Dhillon: as an m times, n, vector, right?


244
00:30:25.113 --> 00:30:25.723
Hormoz Shahrzad: Right.


245
00:30:25.943 --> 00:30:27.383
Inderjit Dhillon: Of all the numbers.


246
00:30:28.003 --> 00:30:32.062
Inderjit Dhillon: So forget about its property. It's, you know, used as a linear transformation.


247
00:30:32.163 --> 00:30:35.292
Inderjit Dhillon: But I can just unfold it into


248
00:30:35.823 --> 00:30:45.932
Inderjit Dhillon: an Mn dimensional vector, and then I can basically use all the norms right? Because if you look at the property over here, none of them is actually talking about.


249
00:30:46.173 --> 00:31:06.883
Inderjit Dhillon: For example, matrix multiplication, there's a separate property that some that you know it's submultiplicative when multiplying. But generally, if we just use these 3 as the properties of a matrix norm, then I can basically take any vector, norm and use it.


250
00:31:08.223 --> 00:31:09.163
Inderjit Dhillon: Okay?


251
00:31:09.373 --> 00:31:13.973
Inderjit Dhillon: So as a result, I can have, you know, summation of


252
00:31:14.793 --> 00:31:20.233
Inderjit Dhillon: I equal to one to J. Sorry, I equal to one to M.


253
00:31:20.713 --> 00:31:25.192
Inderjit Dhillon: J. Equal to 1, 2 n, and I can just take


254
00:31:26.103 --> 00:31:31.193
Inderjit Dhillon: okay, and this is a valid matrix. Now.


255
00:31:35.483 --> 00:31:39.082
Inderjit Dhillon: any other norms that you might be familiar with, or you might know about.


256
00:31:41.053 --> 00:31:44.813
Inderjit Dhillon: What about trying to think of the matrix as a linear transformation.


257
00:31:45.183 --> 00:31:47.973
Inderjit Dhillon: Are there any norms that can come up? Come up


258
00:31:48.373 --> 00:31:50.343
Inderjit Dhillon: but thinking of it that way?


259
00:31:57.333 --> 00:32:03.173
Inderjit Dhillon: Okay, so here is one way to define matrix norms. Let's look in particular at the 2 norm.


260
00:32:06.513 --> 00:32:07.433
Inderjit Dhillon: Okay.


261
00:32:08.883 --> 00:32:17.252
Inderjit Dhillon: okay? And and you'll see why I've written the subscript, too. So I can think about the matrix. So remember the matrix, A


262
00:32:17.703 --> 00:32:21.073
Inderjit Dhillon: is a linear transformation, right? It takes any X


263
00:32:21.523 --> 00:32:24.613
Inderjit Dhillon: and changes it to a times. X.


264
00:32:25.193 --> 00:32:28.712
Inderjit Dhillon: Now think about its action on every possible. X,


265
00:32:29.613 --> 00:32:37.683
Inderjit Dhillon: okay? And if I think about it, I can say, okay, well, this is a particular vector, n.


266
00:32:39.773 --> 00:32:44.833
Inderjit Dhillon: RMA, times, X is a vector, in RM.


267
00:32:46.163 --> 00:32:48.022
Inderjit Dhillon: That has a particular norm.


268
00:32:48.963 --> 00:32:51.062
Inderjit Dhillon: So I can look at the norm of it.


269
00:32:53.613 --> 00:32:56.193
Inderjit Dhillon: Let's just confine ourselves to the 2 norm.


270
00:32:56.733 --> 00:33:00.122
Inderjit Dhillon: And I can ask the question, how much does it magnify


271
00:33:00.393 --> 00:33:02.492
Inderjit Dhillon: or change the magnitude of X.


272
00:33:03.763 --> 00:33:07.243
Inderjit Dhillon: So for every X, it maps it onto a Times X,


273
00:33:09.913 --> 00:33:14.233
Inderjit Dhillon: and I can look at the relative norm of


274
00:33:15.879 --> 00:33:23.012
Inderjit Dhillon: the relative number, which is a times x, the norm of that divided by X. And now I can take


275
00:33:23.263 --> 00:33:30.892
Inderjit Dhillon: all access, and I can ask the question, What is the maximum among all exits?


276
00:33:32.603 --> 00:33:35.742
Inderjit Dhillon: Okay? And when I do this in the 2 dorm.


277
00:33:37.173 --> 00:33:40.343
Inderjit Dhillon: this is called the matrix to norm.


278
00:33:42.593 --> 00:33:47.603
Inderjit Dhillon: Okay? And I can actually think of it another way, because, oh.


279
00:33:48.043 --> 00:33:50.802
Inderjit Dhillon: I can actually write this as Max


280
00:33:51.783 --> 00:33:57.183
Inderjit Dhillon: of Norm of X is less than equal to one of a times. X.


281
00:33:59.173 --> 00:34:03.083
Inderjit Dhillon: So geometrically this is the following right? So if I take my


282
00:34:05.753 --> 00:34:09.922
Inderjit Dhillon: let me actually go a little slower. Sorry. I think I'm going too fast.


283
00:34:10.693 --> 00:34:15.623
Inderjit Dhillon: Let me redraw that again. So suppose I take access.


284
00:34:16.043 --> 00:34:20.203
Inderjit Dhillon: Okay, remember that this is our M.


285
00:34:20.903 --> 00:34:26.993
Inderjit Dhillon: And I look at all. The X's, which satisfy this, that norm of X is less than equal to one.


286
00:34:27.203 --> 00:34:30.333
Inderjit Dhillon: Well, there are all these vectors within the unit sphere.


287
00:34:36.633 --> 00:34:39.393
Inderjit Dhillon: Okay, so X is some vector, over here.


288
00:34:40.943 --> 00:34:43.643
Inderjit Dhillon: Okay? And the 2 norm of X is


289
00:34:44.873 --> 00:34:47.813
Inderjit Dhillon: less than equal to one. Okay, this is what.


290
00:34:48.973 --> 00:34:53.002
Inderjit Dhillon: And this is saying, okay. Now, if I take the image of this


291
00:34:54.903 --> 00:35:04.963
Inderjit Dhillon: right, some vectors will become here. Some vectors will become like this. Some vectors will become like this, and I look at the collection of all the vectors.


292
00:35:05.423 --> 00:35:07.663
Inderjit Dhillon: and I will look at the maximum.


293
00:35:16.263 --> 00:35:23.593
Inderjit Dhillon: and that will be the what's sometimes called the operator norm of this matrix because A is thought of as a


294
00:35:23.803 --> 00:35:27.873
Inderjit Dhillon: operator right? Operating on the vector X


295
00:35:29.583 --> 00:35:48.183
Inderjit Dhillon: okay? So well, it turns out. And I'm I don't think I'm going to go into detail. This is something I'm just going to give to you as a fact. Remember, this is a this is, I'm reviewing a lot of material right? So this is a material that you would have learned, you know, in a full course, or as part of a full course in linear algebra. But I'm just doing a quick review.


296
00:35:48.383 --> 00:35:58.453
Inderjit Dhillon: This turns out to be Sigma, one, which is the maximum singular value


297
00:36:03.403 --> 00:36:04.872
Inderjit Dhillon: of the matrix.


298
00:36:05.293 --> 00:36:15.223
Inderjit Dhillon: So you probably have this kind of question. Right? Well, what is the what are, what are singular values, and what does it mean to be to have the maximum singular value? Right? So we will come to that. Okay.


299
00:36:15.453 --> 00:36:26.383
Inderjit Dhillon: But similar to this, I can basically have. So like, I said, these are operator norms operator


300
00:36:29.523 --> 00:36:30.593
Inderjit Dhillon: norms.


301
00:36:33.923 --> 00:36:37.272
Inderjit Dhillon: So just have just as I have one. I can have


302
00:36:38.543 --> 00:36:41.743
Inderjit Dhillon: so sorry. Just like I have Operator 2 Norm, I can have


303
00:36:41.973 --> 00:36:46.702
Inderjit Dhillon: operator, one norm, which is, you know, just following


304
00:36:47.043 --> 00:36:52.733
Inderjit Dhillon: Max of, you know, X not equal to 0 normal fix.


305
00:36:52.883 --> 00:36:54.832
Inderjit Dhillon: Do I deploy normal? Fax?


306
00:36:55.643 --> 00:36:57.948
Inderjit Dhillon: Okay? So remember, I can't have


307
00:36:58.823 --> 00:37:07.632
Inderjit Dhillon: Obviously, it doesn't make sense to have X equal to 0. Right? Because then I can't define the relative value. Okay, so similarly, I can take that, you know, it's maximum


308
00:37:07.963 --> 00:37:13.003
Inderjit Dhillon: of norm of x, 1 is less than equal to one norm of X.


309
00:37:14.343 --> 00:37:17.252
Inderjit Dhillon: Okay. And similarly, I can have the infinity norm.


310
00:37:18.383 --> 00:37:24.523
Inderjit Dhillon: which is maximum of X not equal to 0 norm of ax


311
00:37:24.773 --> 00:37:27.862
Inderjit Dhillon: infinity divided by normal. Phi, x.


312
00:37:30.163 --> 00:37:37.253
Inderjit Dhillon: Okay, so this is the operator, the operator.


313
00:37:38.813 --> 00:37:48.013
Inderjit Dhillon: 2 norm, the operator, one norm, and the operator infinity norms. And it turns out, if you do an analysis.


314
00:37:48.143 --> 00:37:52.782
Inderjit Dhillon: that these are actually not that complicated quantities


315
00:37:53.093 --> 00:38:01.412
Inderjit Dhillon: they end up being like, I think, one norm, or the the infinity norm, I forget which one it is. You look at all the columns.


316
00:38:02.013 --> 00:38:07.823
Inderjit Dhillon: and you end up. The norm is the maximum among all column norms.


317
00:38:08.273 --> 00:38:13.983
Inderjit Dhillon: and the other one is the maximum among all donors of the matrix. Okay?


318
00:38:18.953 --> 00:38:22.903
Inderjit Dhillon: okay. And so, similarly, I have maximum of


319
00:38:23.483 --> 00:38:28.163
Inderjit Dhillon: normal effects. Infinity is less than equal to one normal effects.


320
00:38:28.353 --> 00:38:34.053
Inderjit Dhillon: Infinity. Okay? And again, I can say that you know this is asking the question.


321
00:38:34.183 --> 00:38:39.263
Inderjit Dhillon: and if I have norm of x, 1 norm less than equal to one. It actually is


322
00:38:39.913 --> 00:38:41.983
Inderjit Dhillon: this set of vectors.


323
00:38:44.793 --> 00:38:50.182
Inderjit Dhillon: These are the vectors where the one norm is less than equal to one.


324
00:38:50.593 --> 00:38:51.323
Inderjit Dhillon: Okay?


325
00:38:51.683 --> 00:38:56.653
Inderjit Dhillon: And then infinity. Norm is, you know, I want the maximum to be less than equal to one.


326
00:38:57.203 --> 00:39:04.833
Inderjit Dhillon: So it's actually this square infinity. Norm is less than equal to one.


327
00:39:06.023 --> 00:39:11.033
Inderjit Dhillon: And the corresponding operator norm is, how much magnification does that give?


328
00:39:13.653 --> 00:39:14.473
Inderjit Dhillon: Okay?


329
00:39:15.773 --> 00:39:21.803
Inderjit Dhillon: You can actually like, do mix and match. Right? You can actually take a PQ.


330
00:39:22.383 --> 00:39:26.423
Inderjit Dhillon: And you can say, in one space.


331
00:39:27.453 --> 00:39:32.363
Inderjit Dhillon: I'm gonna look at the ex, the P. Norm.


332
00:39:38.683 --> 00:39:41.703
Inderjit Dhillon: and in the other space. I look at the kernel.


333
00:39:42.963 --> 00:39:49.583
Inderjit Dhillon: Okay, but this is kind of esoteric, you know, we're not really going to talk about it. Right? So mostly.


334
00:39:50.183 --> 00:39:54.643
Inderjit Dhillon: you know, in line, in, in, in machine learning.


335
00:39:54.893 --> 00:39:58.722
Inderjit Dhillon: matrix norms will not come up that much right. But


336
00:39:58.923 --> 00:40:16.073
Inderjit Dhillon: when you do some analysis and so on about, you know, maybe, how the regression is doing, or you need to do, or even in classification, then typically you'll use the Frobenius norm, or maybe the to norm. Okay, so don't need to worry too much about it.


337
00:40:16.523 --> 00:40:22.193
Inderjit Dhillon: There is an important aspect, which is that of


338
00:40:23.963 --> 00:40:30.933
Inderjit Dhillon: This actually comes up in and I think I already mentioned it to you before. Right? It comes up in linear regression, which is


339
00:40:31.993 --> 00:40:38.793
Inderjit Dhillon: the concept of a matrix being oh.


340
00:40:39.433 --> 00:40:46.103
Inderjit Dhillon: either low rank or singular right? If it's a square matrix, you say the matrix is not invertible.


341
00:40:46.393 --> 00:40:48.933
Inderjit Dhillon: or it is singular.


342
00:40:49.233 --> 00:40:58.762
Inderjit Dhillon: and then, if it is rectangular, you can have, you know, less than the full rank, and so on. So there is an important quantity which is called the condition number


343
00:40:59.383 --> 00:41:13.343
Inderjit Dhillon: of a matrix Kappa, okay. And this is called the condition number of Matrix A,


344
00:41:13.833 --> 00:41:18.572
Inderjit Dhillon: and I'll just give you the formula for the condition number of a. It is the norm of a


345
00:41:19.483 --> 00:41:21.653
Inderjit Dhillon: time, the norm of a inverse.


346
00:41:22.873 --> 00:41:25.942
Inderjit Dhillon: So it's kind of already assuming it's a square matrix.


347
00:41:26.343 --> 00:41:29.633
Inderjit Dhillon: Okay? And in particular, when I have the 2 norm.


348
00:41:30.963 --> 00:41:39.492
Inderjit Dhillon: if I use the operator to norm to measure the norm of a or norm of inverse, it becomes sigma, one divided by Sigma. N,


349
00:41:40.733 --> 00:41:47.263
Inderjit Dhillon: okay, so this is the maximum singular value


350
00:41:51.313 --> 00:41:56.362
Inderjit Dhillon: of a, and this is the minimum singular values.


351
00:42:10.623 --> 00:42:14.473
Inderjit Dhillon: So this is equal to norm of a the 2 norm of a.


352
00:42:14.863 --> 00:42:20.632
Inderjit Dhillon: and this is equal to the to norm of a inverse.


353
00:42:24.273 --> 00:42:29.082
Inderjit Dhillon: Okay, let me just take a little pause any questions so far.


354
00:42:29.313 --> 00:42:34.872
Inderjit Dhillon: Remember, this is a very very quick review, like I said before, you know, we will upload the


355
00:42:35.053 --> 00:42:41.843
Inderjit Dhillon: video lectures, and also the class notes will be available to you. And then, you know, I think in this.


356
00:42:42.103 --> 00:42:51.183
Inderjit Dhillon: in this I'll also have like some oh, oh, notes! In addition to


357
00:42:51.673 --> 00:42:56.153
Inderjit Dhillon: you know my handwritten notes, I'll also have, like some types of notes that I will upload


358
00:42:57.353 --> 00:42:58.963
Inderjit Dhillon: any questions so far.


359
00:43:02.393 --> 00:43:09.152
Inderjit Dhillon: Okay, let's move on to some other concepts that may come across


360
00:43:09.553 --> 00:43:15.683
Inderjit Dhillon: in machine learning. Okay? So the next big kind of you know, we talked a little bit about singular values.


361
00:43:16.053 --> 00:43:25.783
Inderjit Dhillon: right? So the singular value decomposition is a really important concept in linear algebra. Right?


362
00:43:26.103 --> 00:43:27.383
Inderjit Dhillon: So.


363
00:43:27.936 --> 00:43:31.562
Inderjit Dhillon: Let's talk a little bit about Eigenvalues and Eigenvectors.


364
00:43:46.543 --> 00:43:53.452
Inderjit Dhillon: Anybody wants to tell me what an eigenvalue an Eigenvector of a is of a matrix, a.


365
00:43:58.763 --> 00:44:01.993
Inderjit Dhillon: So see, all of you have told me that you've done linear algebra right?


366
00:44:15.013 --> 00:44:16.363
Inderjit Dhillon: No volunteers.


367
00:44:16.973 --> 00:44:17.863
Inderjit Dhillon: Okay.


368
00:44:18.473 --> 00:44:24.483
Haoran Niu: So it'll be the value of ax equals to lambda X


369
00:44:24.693 --> 00:44:39.833
Haoran Niu: and lambda would be the eigenvalue, and for the Eigenvector like, if we using the eigenvalue, get the corresponding matrix, and then for each column it will be the Eigenvector.


370
00:44:40.243 --> 00:44:45.823
Inderjit Dhillon: Okay, okay. So 1st of all, the matrix has to be square.


371
00:44:46.493 --> 00:44:49.752
Inderjit Dhillon: because what we are saying is that I have a times X,


372
00:44:50.573 --> 00:44:57.263
Inderjit Dhillon: and that is equal to lambda. X, okay, X's


373
00:44:57.493 --> 00:45:00.912
Inderjit Dhillon: not equal to 0, because, of course, if X equal to 0, then


374
00:45:01.023 --> 00:45:06.733
Inderjit Dhillon: a times X will also be 0 lambda is called the Eigenvalue.


375
00:45:10.543 --> 00:45:13.733
Inderjit Dhillon: and X is called the Eigenvector.


376
00:45:17.373 --> 00:45:18.253
Inderjit Dhillon: Okay.


377
00:45:18.633 --> 00:45:24.402
Inderjit Dhillon: And remember, why does a have to be a square matrix? Well, because a times X.


378
00:45:24.593 --> 00:45:26.782
Inderjit Dhillon: Remember, if a is m, by n.


379
00:45:28.203 --> 00:45:39.112
Inderjit Dhillon: it is a linear transformation from N dimensional vector to M dimensional. Vector but here, I'm insisting that a times X equals Lambda X. So its dimensions of the left hand side have to be equal to


380
00:45:39.273 --> 00:45:45.952
Inderjit Dhillon: the dimensions of the right hand side. So a has to be a square matrix.


381
00:45:47.903 --> 00:45:48.863
Inderjit Dhillon: Okay.


382
00:45:49.823 --> 00:45:57.602
Inderjit Dhillon: now, what does if I look at a times X equal to lambda X. What does that imply? Well, that implies that ax


383
00:45:57.703 --> 00:46:08.182
Inderjit Dhillon: minus lambda x equal 0, which means that a minus lambda I times x equal to 0,


384
00:46:09.383 --> 00:46:17.182
Inderjit Dhillon: which means that this matrix a minus lambda. I so remember a minus lambda. I is also a matrix


385
00:46:17.893 --> 00:46:18.753
Inderjit Dhillon: right?


386
00:46:18.923 --> 00:46:21.792
Inderjit Dhillon: And lambda of these values such that


387
00:46:22.413 --> 00:46:32.743
Inderjit Dhillon: if I look at the Eigenvector, then a minus lambda, I times x equal to 0, which means that x belongs to the null space of a minus lambda I,


388
00:46:33.013 --> 00:46:49.293
Inderjit Dhillon: and that a minus lambda I is singular, and x belongs to its null space.


389
00:46:54.023 --> 00:46:56.783
Inderjit Dhillon: And what that means is that


390
00:46:58.383 --> 00:47:08.173
Inderjit Dhillon: If a matrix is singular, one characterization of it is that its determinant equals 0


391
00:47:09.273 --> 00:47:14.343
Inderjit Dhillon: determinant of a minus lambda. IS, 0,


392
00:47:14.903 --> 00:47:19.592
Inderjit Dhillon: okay. And if you know, just think about it. Right? A is an M by N matrix.


393
00:47:20.093 --> 00:47:26.633
Inderjit Dhillon: and I have a 1, 1, 8, 2, 2, 8, 3, 3.


394
00:47:27.563 --> 00:47:34.202
Inderjit Dhillon: Remember, it's an N by N matrix. And then I have a 1, 2, a 1, 3,


395
00:47:34.873 --> 00:47:41.723
Inderjit Dhillon: a 1 n, and one and 2, and so on.


396
00:47:42.073 --> 00:47:48.323
Inderjit Dhillon: Right? If I look at a minus lambda, I I'm basically taking all the diagonal elements


397
00:47:49.043 --> 00:47:50.882
Inderjit Dhillon: and subtracting lambda from it.


398
00:47:52.383 --> 00:48:00.053
Inderjit Dhillon: And what I'm saying is that these lambdas are numbers such that this determinant is 0.


399
00:48:01.023 --> 00:48:07.872
Inderjit Dhillon: Okay, so remember that this determinant is now a


400
00:48:09.263 --> 00:48:19.962
Inderjit Dhillon: if you expand it out. If you remember the formula for indeterminant, it's a polynomial all they know. Meal


401
00:48:20.903 --> 00:48:22.802
Inderjit Dhillon: of degree. M.


402
00:48:26.483 --> 00:48:27.543
Inderjit Dhillon: In lambda.


403
00:48:29.073 --> 00:48:35.512
Inderjit Dhillon: Okay, so that means that if a is N. By N.


404
00:48:37.103 --> 00:48:42.783
Inderjit Dhillon: Then he has, and eigenvalues.


405
00:48:46.983 --> 00:48:54.532
Inderjit Dhillon: Okay. So if A is 5 by 5, it has 5 eigenvalues. If A is 10 by 10 it has 10 Eigenvalues.


406
00:48:55.293 --> 00:49:18.002
Inderjit Dhillon: They may be similar, I mean they may. Some of the Eigenvalues might be identical, but you say that it has N eigenvalues. Okay? And then one of the questions is, you know, what about the Eigenvectors? So I'm not going to go into detail. There's a very rich theory about something called the Jordan canonical form, and so on. Where, you know, a matrix may not have a full set of Eigenvectors.


407
00:49:18.123 --> 00:49:31.523
Inderjit Dhillon: It always has n eigenvalues, but it actually may not have n eigenvectors. Those situations don't really come up in machine learning, in machine learning. Typically, we encounter, you know, A to B,


408
00:49:31.933 --> 00:49:41.123
Inderjit Dhillon: you know, in many cases to be symmetric, especially if we need to talk about its spectrum. Right? So, for example, if you recall when we talked about regression.


409
00:49:41.253 --> 00:49:43.635
Inderjit Dhillon: we ended up with the matrices.


410
00:49:44.373 --> 00:49:50.763
Inderjit Dhillon: we ended up. We had this feature matrix, which is X. But what came up in the solution was Xx transports.


411
00:49:50.893 --> 00:49:56.663
Inderjit Dhillon: And if you think about Xx, transpose that has a special property in particular, it is


412
00:49:56.903 --> 00:50:06.862
Inderjit Dhillon: symmetric. And then, as we will talk a little bit even right now, it's actually special in the sense that it is also symmetric, positive, definite.


413
00:50:07.093 --> 00:50:09.073
Inderjit Dhillon: So let's talk a little bit about


414
00:50:10.473 --> 00:50:18.603
Inderjit Dhillon: a symmetric matrix. So if A is a transpose. Which means that a is a symmetric matrix.


415
00:50:26.153 --> 00:50:34.253
Inderjit Dhillon: Okay, then there's actually a very special property that that holds. Okay.


416
00:50:34.713 --> 00:50:37.784
Inderjit Dhillon: So remember that if I have a


417
00:50:38.723 --> 00:50:45.293
Inderjit Dhillon: polynomial, even a degree, 2 polynomial right, which is where the coefficients are real numbers.


418
00:50:45.453 --> 00:50:54.102
Inderjit Dhillon: My Eigen, my, the roots of the polynomial. If you take, for example, a quadratic. The roots of a polynomial can actually be complex.


419
00:50:55.043 --> 00:50:57.462
Inderjit Dhillon: They could be complex numbers, right?


420
00:50:57.673 --> 00:51:09.412
Inderjit Dhillon: But it turns out that if a is symmetric, in fact, there's a larger class of matrices called normal matrices. But we won't discuss those every symmetric matrix. It's what's called a normal matrix.


421
00:51:09.673 --> 00:51:15.392
Inderjit Dhillon: If a asymmetric, then all of its eigenvalues


422
00:51:20.353 --> 00:51:29.333
Inderjit Dhillon: of a oral, so no complex numbers, and then it has


423
00:51:30.853 --> 00:51:40.723
Inderjit Dhillon: a very another important property, which is that it has a full


424
00:51:41.813 --> 00:51:45.093
Inderjit Dhillon: set. So remember, there are n eigenvalues.


425
00:51:45.543 --> 00:51:55.363
Inderjit Dhillon: and what I'm saying is that there are. It has a full set, which is n eigenvectors that have a further property that they are mutually. Orthogon.


426
00:52:07.113 --> 00:52:11.773
Inderjit Dhillon: Okay, so what does that mean? Let's regroup. Let's think about it.


427
00:52:12.953 --> 00:52:15.402
Inderjit Dhillon: If A is equal to a transpose.


428
00:52:16.523 --> 00:52:23.152
Inderjit Dhillon: I have N Eigenvectors, so a. v 1 equal to lambda one, or let me write it.


429
00:52:23.333 --> 00:52:25.413
Inderjit Dhillon: because, remember, Lambda is a scalar.


430
00:52:25.573 --> 00:52:31.922
Inderjit Dhillon: so I'll actually write it as v. 1 lambda, one


431
00:52:34.103 --> 00:52:38.052
Inderjit Dhillon: a. V. 2 is equal to v. 2 lambda, 2.


432
00:52:39.983 --> 00:52:45.153
Inderjit Dhillon: And then I have a times Vn. Is equal to Vn. Lambda.


433
00:52:47.983 --> 00:52:49.943
Inderjit Dhillon: Right? So I have. And what


434
00:52:50.303 --> 00:52:55.542
Inderjit Dhillon: what this kind of theorem or result is saying, that if I have a symmetric matrix.


435
00:52:55.743 --> 00:53:00.993
Inderjit Dhillon: then I have v. 1 lambda, one v. 2 lambda, 2 Vn. Lambda, N.


436
00:53:01.963 --> 00:53:05.482
Inderjit Dhillon: And that lambda I belongs to all.


437
00:53:05.953 --> 00:53:08.212
Inderjit Dhillon: We don't have to go to complex numbers.


438
00:53:09.183 --> 00:53:10.363
Inderjit Dhillon: Further.


439
00:53:12.253 --> 00:53:23.622
Inderjit Dhillon: we can show that there exist Vi and vj. Or v. 1 through vn. Such that they're all mutually orthogonal. So that means Vi transpose vj.


440
00:53:24.353 --> 00:53:31.013
Inderjit Dhillon: When I say orthogonal, that means the angle between them is 90 degrees, or these vectors are


441
00:53:31.133 --> 00:53:34.122
Inderjit Dhillon: perpendicular to each other, and we saw that


442
00:53:34.403 --> 00:53:38.683
Inderjit Dhillon: being perpendicular means that the inner product is 0.


443
00:53:42.763 --> 00:53:45.782
Inderjit Dhillon: And of course you can always normalize the vectors.


444
00:53:46.703 --> 00:53:51.002
Inderjit Dhillon: such that their norm is one l. 2 norms.


445
00:53:52.563 --> 00:53:54.792
Inderjit Dhillon: What that means is that if I take.


446
00:53:57.383 --> 00:54:00.472
Inderjit Dhillon: if I assemble, all the Eigenvectors


447
00:54:04.533 --> 00:54:07.693
Inderjit Dhillon: as columns in a particular matrix.


448
00:54:09.293 --> 00:54:16.492
Inderjit Dhillon: Okay, then, if I take V transpose V, that equals I,


449
00:54:18.063 --> 00:54:27.112
Inderjit Dhillon: that means V transpose. V is the identity matrix. And you can convince yourself right that the Ij element of the matrix


450
00:54:27.483 --> 00:54:31.183
Inderjit Dhillon: V transpose V is going to be VI transpose vj.


451
00:54:31.813 --> 00:54:38.703
Inderjit Dhillon: and that's 0. If I not equal to J, and this one, if I equal to J, so V. Transpose V equals I.


452
00:54:39.053 --> 00:54:43.892
Inderjit Dhillon: So that means that V transposes the inverse of V. The inverse is unique.


453
00:54:44.153 --> 00:54:48.513
Inderjit Dhillon: So it turns out that Vv transpose also equal to


454
00:54:50.223 --> 00:54:55.203
Inderjit Dhillon: okay. And I can actually now write this all of these


455
00:54:56.573 --> 00:55:00.702
Inderjit Dhillon: very compactly, in the following way, that I can write a


456
00:55:01.193 --> 00:55:06.242
Inderjit Dhillon: like, I said, if I assemble v. 1, v. 2, into a matrix.


457
00:55:07.923 --> 00:55:17.262
Inderjit Dhillon: then I can write these Eigenvalue and Eigenvector equations in the following way, that I'm writing right now.


458
00:55:25.583 --> 00:55:33.263
Inderjit Dhillon: Okay, so if I call this matrix, I've already called this matrix V, this is V,


459
00:55:34.513 --> 00:55:38.193
Inderjit Dhillon: and this matrix. I call as capital lambda.


460
00:55:39.843 --> 00:55:47.372
Inderjit Dhillon: Okay, so this is I can basically write it as a times. B is equal to V. Lamb.


461
00:55:51.263 --> 00:55:54.793
Inderjit Dhillon: And since V is orthogonal. If I multiply it


462
00:55:55.373 --> 00:56:03.293
Inderjit Dhillon: on the right hand, side by V. Transpose both sides, left, right, left, side by V. Transpose, and right side by V. Transpose


463
00:56:03.923 --> 00:56:07.853
Inderjit Dhillon: vv, transpose is the identity. So this implies


464
00:56:08.233 --> 00:56:12.873
Inderjit Dhillon: that a is equal to V lambda. V transpose.


465
00:56:13.873 --> 00:56:17.493
Inderjit Dhillon: So I can basically take any real symmetric matrix.


466
00:56:18.543 --> 00:56:25.363
Inderjit Dhillon: And I can decompose it, I can factor it using its eigenvalue. Decomposition.


467
00:56:32.993 --> 00:56:36.982
Inderjit Dhillon: Okay, as a equal to V. Lambda. V. Transpose.


468
00:56:39.543 --> 00:56:42.632
Inderjit Dhillon: And V has the property that V is orthogonal.


469
00:56:43.173 --> 00:56:46.773
Inderjit Dhillon: Lambda is, remember that this is a diagonal matrix.


470
00:56:47.963 --> 00:56:50.202
Inderjit Dhillon: This is a diagonal matrix.


471
00:56:54.553 --> 00:56:57.833
Inderjit Dhillon: and this is called an orthogonal matrix.


472
00:57:07.453 --> 00:57:10.243
Inderjit Dhillon: This is called the eigenvalue. Decomposition


473
00:57:10.413 --> 00:57:17.163
Inderjit Dhillon: of this matrix. A. So this is oh, square matrix.


474
00:57:18.873 --> 00:57:21.462
Inderjit Dhillon: And then there are some special matrices.


475
00:57:21.643 --> 00:57:26.853
Inderjit Dhillon: right? Which are called not only just symmetric matrices.


476
00:57:27.003 --> 00:57:32.533
Inderjit Dhillon: but they have a further property, which is that


477
00:57:33.133 --> 00:57:37.072
Inderjit Dhillon: all the Eigenvalues are greater than equal to 0.


478
00:57:37.413 --> 00:57:45.403
Inderjit Dhillon: Okay? And these matrices, or what's called the positive definite matrices. Okay, so


479
00:57:46.076 --> 00:57:53.752
Inderjit Dhillon: a. And I'll tell you. I'll tell you a little bit later why, I'm talking about positive, definite matrices. Right? So a is


480
00:57:54.363 --> 00:57:58.533
Inderjit Dhillon: positive semidefinite.


481
00:58:02.373 --> 00:58:10.393
Inderjit Dhillon: if well, the basic definition of a positive semi-definite is, you know, that the all the quadratic forms.


482
00:58:10.903 --> 00:58:13.383
Inderjit Dhillon: or greater than equal to 0


483
00:58:14.063 --> 00:58:17.162
Inderjit Dhillon: for Alex. And many times we say that


484
00:58:17.593 --> 00:58:19.993
Inderjit Dhillon: we use this, you know


485
00:58:20.923 --> 00:58:27.143
Inderjit Dhillon: to denote a positive, definite matrix. Okay, similarly, a is positive


486
00:58:28.063 --> 00:58:31.642
Inderjit Dhillon: instead of semidefinite. We say it is positive, definite.


487
00:58:32.213 --> 00:58:36.932
Inderjit Dhillon: If X transpose ax is strictly greater than 0.


488
00:58:41.693 --> 00:58:42.593
Inderjit Dhillon: Okay?


489
00:58:42.813 --> 00:58:50.973
Inderjit Dhillon: And you know, the reason I'm talking about these matrices is that a alright, he is


490
00:58:52.293 --> 00:59:02.283
Inderjit Dhillon: positive. So so these 2 are equivalent. So let me just see, this is equivalent


491
00:59:05.513 --> 00:59:17.413
Inderjit Dhillon: equivalent to saying, lived all it's eigenvalues.


492
00:59:20.373 --> 00:59:22.413
Inderjit Dhillon: or greater than equal to 6.


493
00:59:23.233 --> 00:59:29.683
Inderjit Dhillon: So all the Eigenvalues. Not only are they real, but they are non-negative.


494
00:59:33.553 --> 00:59:34.423
Inderjit Dhillon: Okay?


495
00:59:45.183 --> 00:59:52.372
Inderjit Dhillon: And similarly, you know, A is positive, definite is equivalent to saying that all its eigenvalues are


496
00:59:52.613 --> 00:59:53.802
Inderjit Dhillon: 0 1 0.


497
00:59:58.943 --> 01:00:13.293
Inderjit Dhillon: So if you recall right when we did the the material on regression, then we had, you know, that X transpose y is nearly equal to


498
01:00:13.433 --> 01:00:16.622
Inderjit Dhillon: the what do we call it? B,


499
01:00:18.773 --> 01:00:23.432
Inderjit Dhillon: or X transpose W is nearly equal to y.


500
01:00:23.723 --> 01:00:28.912
Inderjit Dhillon: and then we saw that this was Xx transpose W. If we solve


501
01:00:29.913 --> 01:00:33.792
Inderjit Dhillon: X transpose W minus y minimum


502
01:00:34.453 --> 01:00:41.762
Inderjit Dhillon: over W square, right? This implied that this would be


503
01:00:42.803 --> 01:00:54.273
Inderjit Dhillon: X times y. Right. And if you look at this matrix, okay, this matrix is positive, semi-definite.


504
01:00:55.963 --> 01:01:04.733
Inderjit Dhillon: So even though you might think that, hey, you know, why are we talking about positive semi-definite matrices and eigenvalues, and so on.


505
01:01:04.963 --> 01:01:07.733
Inderjit Dhillon: These squares regression in the particular case


506
01:01:07.893 --> 01:01:23.293
Inderjit Dhillon: where you right away see that you get a positive semidefinite. And what ends up happening is that when you have any matrix which is of this form, gg, transpose or Xx transpose right. If a is Gg, transpose.


507
01:01:23.443 --> 01:01:27.072
Inderjit Dhillon: then, if I look at the quadratic form, X transpose ax.


508
01:01:27.383 --> 01:01:33.063
Inderjit Dhillon: I can write it as X transpose. GG. Transpose X, and I can get, as


509
01:01:33.213 --> 01:01:40.862
Inderjit Dhillon: this is equal to Z transpose z equals 0 where z is equal to G transpose X


510
01:01:41.313 --> 01:01:44.173
Inderjit Dhillon: right? And this is the sum of squares.


511
01:01:44.453 --> 01:01:48.462
Inderjit Dhillon: I'm sorry I didn't mean to say 0. But this is greater than equal 0.


512
01:01:48.733 --> 01:02:03.923
Inderjit Dhillon: So any matrix of this form, any matrix of this form is positive.


513
01:02:06.483 --> 01:02:07.532
Inderjit Dhillon: So I mean, definitely.


514
01:02:12.573 --> 01:02:21.933
Inderjit Dhillon: okay, so in particular, Xx, transpose over here to matrix definitely.


515
01:02:23.003 --> 01:02:23.813
Inderjit Dhillon: Okay.


516
01:02:24.493 --> 01:02:33.322
Inderjit Dhillon: Now, let me kind of finish this by talking a little bit about F. 3D, or the singular value decomposition.


517
01:02:33.523 --> 01:02:35.723
Inderjit Dhillon: Okay, so


518
01:02:38.813 --> 01:02:46.112
Inderjit Dhillon: this is material that should be taught in all linear algebra courses. But


519
01:02:46.803 --> 01:02:50.083
Inderjit Dhillon: it's possible that some of you might not have seen this before.


520
01:02:50.523 --> 01:02:56.233
Inderjit Dhillon: I think the material that I talked about before Eigenvalues Eigenvectors


521
01:02:56.433 --> 01:03:01.933
Inderjit Dhillon: that everybody has seen before, and ideally, you should know what a singular value. Decomposition is


522
01:03:02.603 --> 01:03:05.143
Inderjit Dhillon: so. Remember the eigenvalue, decomposition.


523
01:03:09.613 --> 01:03:20.002
Inderjit Dhillon: decomposition exist only for requires a to be square


524
01:03:22.123 --> 01:03:35.493
Inderjit Dhillon: right? And we saw that if a is square and it is symmetric, then it has real icon. Well.


525
01:03:36.303 --> 01:03:39.633
Inderjit Dhillon: and it has orthogonal eigenvectors.


526
01:03:40.403 --> 01:03:47.823
Inderjit Dhillon: So real square, symmetric matrices


527
01:03:52.963 --> 01:03:57.932
Inderjit Dhillon: have this eigenvalue decomposition.


528
01:04:04.143 --> 01:04:06.683
Inderjit Dhillon: which is, that a is equal to


529
01:04:08.862 --> 01:04:11.102
Inderjit Dhillon: what did I say? A is equal to


530
01:04:12.593 --> 01:04:15.362
Inderjit Dhillon: over here? V. Lambda. We trans.


531
01:04:17.523 --> 01:04:20.043
Inderjit Dhillon: V. Lambda. V transpose.


532
01:04:21.353 --> 01:04:32.083
Inderjit Dhillon: Okay, so square, real square symmetric matrices have this eigenvalue, decomposition, singular value. Decomposition, or Svd.


533
01:04:34.662 --> 01:04:37.153
Inderjit Dhillon: Exists for all matrices.


534
01:04:37.333 --> 01:04:41.793
Inderjit Dhillon: Excess for all medicines.


535
01:04:43.943 --> 01:04:50.403
Inderjit Dhillon: So suppose A is a general M. By N. Matrix, okay.


536
01:04:51.303 --> 01:04:58.683
Inderjit Dhillon: Svd of a is that a is, it can be written as U.


537
01:04:59.133 --> 01:05:01.803
Inderjit Dhillon: Sigma read transcripts.


538
01:05:02.933 --> 01:05:14.452
Inderjit Dhillon: Okay, so let me say what you is. Okay, that you and we are orthogonal.


539
01:05:17.782 --> 01:05:19.673
Inderjit Dhillon: And Sigma is.


540
01:05:20.003 --> 01:05:26.553
Inderjit Dhillon: But okay, so let me write it over here. A is


541
01:05:27.362 --> 01:05:31.563
Inderjit Dhillon: U will be u 1. So remember a is M by N.


542
01:05:32.693 --> 01:05:36.442
Inderjit Dhillon: So this is going to be m by M.


543
01:05:37.063 --> 01:05:40.753
Inderjit Dhillon: So u. 1 u, 2


544
01:05:42.053 --> 01:05:53.653
Inderjit Dhillon: Then I will have sigma, one sigma, 2 through sigma.


545
01:05:56.353 --> 01:05:59.304
Inderjit Dhillon: So what will. What could happen is that?


546
01:06:00.103 --> 01:06:03.743
Inderjit Dhillon: you know I can either have. M is greater than equal to.


547
01:06:04.333 --> 01:06:12.853
Inderjit Dhillon: And okay, in which case I will have, you know there will be wow


548
01:06:13.723 --> 01:06:16.992
Inderjit Dhillon: and singular values. Right? So this is M.


549
01:06:17.573 --> 01:06:19.303
Inderjit Dhillon: By M.


550
01:06:20.603 --> 01:06:22.723
Inderjit Dhillon: So I'll have Sigma N.


551
01:06:23.643 --> 01:06:31.682
Inderjit Dhillon: And then I'll have zeros over here. So remember, these are all zeros, and then this matrix is


552
01:06:32.213 --> 01:06:38.643
Inderjit Dhillon: v. 1, transpose v. 2, transpose v, and transfers.


553
01:06:39.713 --> 01:06:44.683
Inderjit Dhillon: But this is n by end. This is m by n right.


554
01:06:44.803 --> 01:06:49.473
Inderjit Dhillon: and this is U. This is sigma, a message we transpose.


555
01:06:50.423 --> 01:06:58.103
Inderjit Dhillon: and the fact that U and V are orthogonal means UU transpose equals u transpose u equals, i


556
01:06:59.513 --> 01:07:11.033
Inderjit Dhillon: vv. Transpose equal to V transpose. V is equal to I right, and sigma is like.


557
01:07:13.823 --> 01:07:17.053
Inderjit Dhillon: and typically these sigmas are arranged in


558
01:07:17.593 --> 01:07:21.363
Inderjit Dhillon: decreasing order. Sigma, one is greater than equal to Sigma. 2


559
01:07:21.773 --> 01:07:23.803
Inderjit Dhillon: is greater than equal to Sigma can.


560
01:07:24.223 --> 01:07:29.303
Inderjit Dhillon: Okay, and these are greater than equal to 0.


561
01:07:31.883 --> 01:07:39.343
Inderjit Dhillon: So Svd is kind of, you know, very kind of important right? And


562
01:07:41.843 --> 01:07:50.123
Inderjit Dhillon: it is kind of like a fundamental decomposition of any matrix. Nice thing is, it exists for


563
01:07:50.243 --> 01:07:51.663
Inderjit Dhillon: all matrices.


564
01:07:52.993 --> 01:07:53.833
Inderjit Dhillon: Okay?


565
01:07:54.183 --> 01:08:06.101
Inderjit Dhillon: So you know, I've probably gone a little bit more than I needed to in terms of like the review of linear algebra but you know this should be hopefully like,


566
01:08:06.623 --> 01:08:10.442
Inderjit Dhillon: good kind of refresher for you all.


567
01:08:12.223 --> 01:08:12.933
Inderjit Dhillon: And


568
01:08:15.613 --> 01:08:19.833
Inderjit Dhillon: So let me know if you have any questions on the material that we've seen so far.


569
01:08:25.143 --> 01:08:28.172
Inderjit Dhillon: I see, like a chat message. Let me just show.


570
01:08:28.953 --> 01:08:30.573
Inderjit Dhillon: Was there a question over here.


571
01:08:33.323 --> 01:08:40.052
Inderjit Dhillon: Oh, somebody just wrote for me in Islam. Sorry about when I'm writing. I not don't necessarily see the see the chat.


572
01:08:45.383 --> 01:08:46.313
Inderjit Dhillon: Okay?


573
01:08:47.213 --> 01:08:54.879
Inderjit Dhillon: So with that kind of you know. The kind of the view of linear algebra is over. Of course, when we come to


574
01:08:55.950 --> 01:08:58.633
Inderjit Dhillon: when we come back to teaching.


575
01:08:58.793 --> 01:09:10.193
Inderjit Dhillon: looking at regression, looking at classification. There will be times when we will come back to this and what it means, and then, of course, I will talk about it even at that point.


576
01:09:12.083 --> 01:09:15.903
Inderjit Dhillon: So any questions so far or about today's lecture.


577
01:09:23.363 --> 01:09:26.543
Inderjit Dhillon: Okay? Well, then, I'm going to kind of stop sharing.


578
01:09:26.973 --> 01:09:35.402
Inderjit Dhillon: And if there are no more, no questions, and we'll conclude this


579
01:09:36.476 --> 01:09:41.183
Inderjit Dhillon: and next time, when we meet on Monday


580
01:09:41.293 --> 01:09:44.323
Inderjit Dhillon: we will talk more about regression.


581
01:09:44.583 --> 01:09:48.592
Inderjit Dhillon: What I gave you is was a very simplified version of regression.


582
01:09:48.923 --> 01:09:59.773
Inderjit Dhillon: But when we look at practical issues in predicting a real valued number, then there are.


583
01:09:59.913 --> 01:10:03.482
Inderjit Dhillon: you know, other issues that come up. For example.


584
01:10:03.703 --> 01:10:20.203
Inderjit Dhillon: you know, there is something called regularization. So we'll talk a little bit about regularization next time, different kinds of regularization Ridge regularization, Lassau regularization. And typically, they're very important. If you want to get like a good predictive method.


585
01:10:20.983 --> 01:10:23.126
Inderjit Dhillon: Okay, okay, so


586
01:10:23.903 --> 01:10:30.640
Inderjit Dhillon: again like, I said. If there are no more questions, then we'll call it a day, and I will see you all on


587
01:10:31.093 --> 01:10:31.983
Inderjit Dhillon: Monday.


588
01:10:32.453 --> 01:10:33.213
Inderjit Dhillon: Good.


589
01:10:34.253 --> 01:10:35.083
Hormoz Shahrzad: Thank you.


590
01:10:35.693 --> 01:10:36.533
Chloe Chen: Thank you.


591
01:10:38.693 --> 01:10:39.633
Inderjit Dhillon: Bye-bye.



---- END OF LECTURE -------- START OF LECTURE 4 ----
WEBVTT

1
00:00:00.039 --> 00:00:04.656
Inderjit Dhillon: Okay, great. So well, welcome to this class.


2
00:00:05.909 --> 00:00:31.868
Inderjit Dhillon: I'm going to continue from where we left off last week. If you remember, I. We did linear regression. Then we said, Okay, well, we need to review some of our linear algebra. And that's what we did last time. And now I'm going to continue, spend a little bit more time on linear algebra review, and then go back to the problem of


3
00:00:32.129 --> 00:00:36.648
Inderjit Dhillon: regression. So before I start, are there any questions for me?


4
00:00:40.519 --> 00:00:53.319
Inderjit Dhillon: Okay? So we'd have placed the lecture notes and the video on canvas. So all of you should have access. And we'll basically keep on uploading them after each lecture.


5
00:00:54.489 --> 00:01:01.579
Inderjit Dhillon: Okay, so let me start just reviewing the things that we did last time.


6
00:01:01.749 --> 00:01:06.289
Inderjit Dhillon: So we talked about vectors.


7
00:01:07.789 --> 00:01:14.028
Inderjit Dhillon: each vector has a norm or a size and a direction size is called norms.


8
00:01:14.229 --> 00:01:21.519
Inderjit Dhillon: We talked about different kinds of vector norms in general, the Lp norms.


9
00:01:22.269 --> 00:01:32.359
Inderjit Dhillon: Then we talked about, you know, orthogonal projector onto just one line which can be generalized to subspaces.


10
00:01:32.679 --> 00:01:41.879
Inderjit Dhillon: talked a little bit about Cauchy-schwartz inequality. Basically the angles between 2 vectors. The cosine theta can be written as


11
00:01:42.099 --> 00:01:53.669
Inderjit Dhillon: X transpose y divided by 2 norm of x 2 norm of y, that's the angle between the or the cosine of the angle between the Vectors X and Y,


12
00:01:53.869 --> 00:02:01.809
Inderjit Dhillon: and then we moved on to talk about matrices. And M. By N. Matrix is a linear transformation


13
00:02:02.059 --> 00:02:07.638
Inderjit Dhillon: from n dimensional vectors to M dimensional vectors.


14
00:02:08.029 --> 00:02:13.688
Inderjit Dhillon: Again, matrices have sizes or norms.


15
00:02:13.939 --> 00:02:20.639
Inderjit Dhillon: There are straightforward extensions of vector norms to matrices where we think of


16
00:02:20.789 --> 00:02:36.819
Inderjit Dhillon: an M by N. Matrix as an M. Times, n dimensional. Vector so from that we get corresponding lp, norms, including the square root of the sum of squares of the absolute value that's called the Frobenius norm


17
00:02:37.549 --> 00:02:41.629
Inderjit Dhillon: right? And then, similarly, like the


18
00:02:42.499 --> 00:02:50.549
Inderjit Dhillon: some of absolute values which corresponds to the l 1 vector norm. But then we saw that oh.


19
00:02:51.239 --> 00:02:56.008
Inderjit Dhillon: matrices have other kinds of norms which are called operator norms. Right? So


20
00:02:57.999 --> 00:03:03.689
Inderjit Dhillon: if you think of them as linear transformations. Then you can look at how much they


21
00:03:04.129 --> 00:03:11.848
Inderjit Dhillon: change the size of vectors from the domain to the range. So if I take an X


22
00:03:12.129 --> 00:03:14.458
Inderjit Dhillon: and I apply a to it.


23
00:03:14.749 --> 00:03:24.438
Inderjit Dhillon: how big or small, is 8 times x compared to x, and that we can measure in different norms. So if we measure it in 2 norm.


24
00:03:24.539 --> 00:03:27.768
Inderjit Dhillon: then we get the maximum singular value


25
00:03:28.009 --> 00:03:33.069
Inderjit Dhillon: of the matrix a. And similarly, we can get, you know, the


26
00:03:33.439 --> 00:03:40.109
Inderjit Dhillon: one operator norm of a matrix and the infinity operator norm


27
00:03:41.029 --> 00:03:52.748
Inderjit Dhillon: matrices have when there's a nontrivial null space, and I'll talk a little bit about that today. Then matrices are non singular. Singular.


28
00:03:53.949 --> 00:03:55.199
Inderjit Dhillon: So


29
00:03:55.519 --> 00:04:03.729
Inderjit Dhillon: one way to measure the closeness of a matrix to singularity is something called the condition number of a matrix


30
00:04:04.615 --> 00:04:16.728
Inderjit Dhillon: and that can come into play when you're talking about regression, and I'll probably mention that a little bit later. Then we started talking about, you know, square matrices and some of the properties they have.


31
00:04:16.939 --> 00:04:23.508
Inderjit Dhillon: So when you talk about square matrices, you have the concept of Eigenvalues and Eigenvectors.


32
00:04:23.969 --> 00:04:31.929
Inderjit Dhillon: The Eigenvalues are basically the roots of the characteristic polynomial which is a polynomial of degree. N,


33
00:04:32.059 --> 00:04:42.468
Inderjit Dhillon: and as a result, an n, by N. Matrix has n eigenvalues in general. Even if a matrix is real, then it can have real as well as complex eigenvalues.


34
00:04:42.609 --> 00:05:03.168
Inderjit Dhillon: But then there's a special class of matrices called normal matrices, where all the Eigenvalues are guaranteed to be real and symmetric matrices are a special subclass of normal matrices. So symmetric matrices, which is mostly what we encounter in machine learning.


35
00:05:03.269 --> 00:05:19.348
Inderjit Dhillon: When we talk about Eigenvalues and the Eigenvectors. They are special, the symmetric matrices, the Eigenvalues are always real, but not only that, they also have a full set.


36
00:05:20.439 --> 00:05:25.528
Inderjit Dhillon: which is, you know, if an if a matrix is N, by N, the full set of size. N,


37
00:05:25.709 --> 00:05:35.179
Inderjit Dhillon: and they have n mutually orthogonal eigenvectors. And that's definitely not true of all matrices. Okay?


38
00:05:35.489 --> 00:05:43.058
Inderjit Dhillon: So if you are looking at a symmetric matrix, we can actually get what's called eigenvalue decomposition.


39
00:05:43.429 --> 00:05:47.289
Inderjit Dhillon: because a times V is equal to


40
00:05:47.659 --> 00:05:53.149
Inderjit Dhillon: v times. Lambda lambda is diagonal. V is consultant.


41
00:05:53.449 --> 00:05:58.168
Inderjit Dhillon: and this is called the Eigenvalue decomposition. But of course it only exists, for


42
00:05:58.719 --> 00:06:13.908
Inderjit Dhillon: this kind of decomposition only exists for normal matrices. In our case, symmetric matrices. There are special classes of subclasses, of symmetric matrices, where not only are the eigenvalues


43
00:06:14.139 --> 00:06:22.399
Inderjit Dhillon: real, but they're also greater than equal to 0. Those are called positive semi-definite matrices. Okay?


44
00:06:22.639 --> 00:06:28.858
Inderjit Dhillon: And then, towards the end of the last lecture, we talked a little bit about singular value, decomposition.


45
00:06:29.499 --> 00:06:35.839
Inderjit Dhillon: eigenvalue, decomposition requires the matrix to be square. And really we talked about


46
00:06:36.818 --> 00:06:46.789
Inderjit Dhillon: the eigenvalue decomposition of a real symmetric matrix. But the singular value decomposition actually exists for all matrices.


47
00:06:47.809 --> 00:07:12.978
Inderjit Dhillon: And you know there's an interesting saying about the singular value decomposition. When you take a matrix, you can do different kinds of decompositions of it, eigenvalue, decomposition for real symmetric matrix. And there's the Jordan canonical decomposition. Then you can do lu decomposition from gaussian elimination


48
00:07:13.169 --> 00:07:22.438
Inderjit Dhillon: and somebody remarked that the singular value. Decomposition is like the Rolls Royce, of all matrix decompositions


49
00:07:22.969 --> 00:07:25.428
Inderjit Dhillon: as well as the Swiss army knife


50
00:07:25.649 --> 00:07:31.278
Inderjit Dhillon: of all matrix decompositions. So that means that it's very rich, theoretically.


51
00:07:31.479 --> 00:07:36.199
Inderjit Dhillon: and in some sense quite, quite intriguing and beautiful.


52
00:07:36.319 --> 00:07:42.549
Inderjit Dhillon: That's why it's the Rolls Royce. It's also extremely useful in a lot of competitions.


53
00:07:42.729 --> 00:07:46.179
Inderjit Dhillon: And as a result it's also the Swiss army knife.


54
00:07:47.200 --> 00:07:49.769
Inderjit Dhillon: So that was a kind of quick recap.


55
00:07:50.319 --> 00:07:55.309
Inderjit Dhillon: And so today let me start off with the singular value decomposition.


56
00:07:55.419 --> 00:07:57.908
Inderjit Dhillon: And so today I will talk about


57
00:08:00.969 --> 00:08:02.238
Inderjit Dhillon: sorry. 1min.


58
00:08:03.709 --> 00:08:09.678
Inderjit Dhillon: Today I'll talk about Svd oops. Sorry


59
00:08:15.409 --> 00:08:20.019
Inderjit Dhillon: today I will talk about Svd and regression.


60
00:08:21.639 --> 00:08:26.039
Inderjit Dhillon: so I will tie tie up these 2 different topics.


61
00:08:28.259 --> 00:08:33.479
Inderjit Dhillon: Okay, so let's review what the Svd is.


62
00:08:33.879 --> 00:08:38.659
Inderjit Dhillon: We touched upon this at the end of oh.


63
00:08:39.459 --> 00:08:50.079
Inderjit Dhillon: the last lecture. Okay, it's the singular value decomposition.


64
00:08:52.149 --> 00:08:55.948
Inderjit Dhillon: Hence the acronym. SVD.


65
00:08:56.119 --> 00:09:03.589
Inderjit Dhillon: Okay, so let's in like I said, it exists for any matrix. So let a be any general matrix.


66
00:09:04.789 --> 00:09:06.118
Inderjit Dhillon: real matrix.


67
00:09:06.499 --> 00:09:16.219
Inderjit Dhillon: M. By N, as I kind of said, you don't really get complex matrices in machine learning. So we'll always talk about


68
00:09:16.339 --> 00:09:17.739
Inderjit Dhillon: real matrices.


69
00:09:18.049 --> 00:09:19.668
Inderjit Dhillon: So the Svd.


70
00:09:19.849 --> 00:09:26.349
Inderjit Dhillon: As we discussed last time, is a decomposition where I can express U as equal to


71
00:09:26.549 --> 00:09:29.838
Inderjit Dhillon: U. Sigma v. Transpose.


72
00:09:30.699 --> 00:09:37.219
Inderjit Dhillon: And here what we have is that UNV.


73
00:09:37.359 --> 00:09:38.279
Inderjit Dhillon: R.


74
00:09:39.629 --> 00:09:41.769
Inderjit Dhillon: Orthogonal matrices.


75
00:09:49.089 --> 00:09:57.038
Inderjit Dhillon: What do I mean by orthogonal matrices? I mean that U transpose U is equal to i.


76
00:09:57.919 --> 00:10:01.349
Inderjit Dhillon: and V transpose V equals? I.


77
00:10:01.999 --> 00:10:10.139
Inderjit Dhillon: There are 2 ways you can define an Svd, so first, st we'll define an Svd. Where U. And V. Are actually square matrices.


78
00:10:10.469 --> 00:10:28.808
Inderjit Dhillon: and because this says that you know, the inverse of U is U transpose, or the left inverse of U is u transpose, and the matrix is square. The inverse is unique. This means that UU transpose is also equal to the identity.


79
00:10:31.769 --> 00:10:40.249
Inderjit Dhillon: Okay, so U transpose U equals. UU, transpose is the identity. When U is orthogonal and square.


80
00:10:41.759 --> 00:10:53.058
Inderjit Dhillon: And then you have Sigma, which is a real and diagonal matrix.


81
00:10:57.589 --> 00:11:00.269
Inderjit Dhillon: Okay? And actually, it's also non-negative.


82
00:11:02.419 --> 00:11:04.368
Inderjit Dhillon: So it's a diagonal matrix


83
00:11:04.709 --> 00:11:13.349
Inderjit Dhillon: with non-negative values on the diagonal. Okay, so let me, for now, you know, let me draw this out.


84
00:11:13.639 --> 00:11:17.668
Inderjit Dhillon: So let me 1st assume that M is greater than equal to N,


85
00:11:17.809 --> 00:11:21.439
Inderjit Dhillon: so let me draw an Mn matrix


86
00:11:22.279 --> 00:11:26.439
Inderjit Dhillon: where M is bigger than N, so this is M.


87
00:11:26.839 --> 00:11:30.369
Inderjit Dhillon: And columns, and I have this matrix A,


88
00:11:30.879 --> 00:11:34.959
Inderjit Dhillon: and what the Svd says is that there's a decomposition.


89
00:11:35.419 --> 00:11:48.109
Inderjit Dhillon: Well, I have it equal to U. So this is the matrix U, this is M. By M,


90
00:11:48.709 --> 00:11:55.339
Inderjit Dhillon: and then I have a matrix, which is, you know, M.


91
00:11:56.649 --> 00:12:11.459
Inderjit Dhillon: By N, and that is Sigma, okay? And then my V transpose is N by. And


92
00:12:13.679 --> 00:12:19.399
Inderjit Dhillon: so note that you and we are square matrices. Okay?


93
00:12:19.789 --> 00:12:25.598
Inderjit Dhillon: And I can think about you as having the columns u 1.


94
00:12:25.729 --> 00:12:33.198
Inderjit Dhillon: You, too, and we'll go up till let me just make a distinction that there is UN


95
00:12:35.169 --> 00:12:40.749
Inderjit Dhillon: UN plus one, do you, M.


96
00:12:41.339 --> 00:12:45.349
Inderjit Dhillon: Each of the ui belongs to RM.


97
00:12:46.079 --> 00:12:52.278
Inderjit Dhillon: And then the similar values are, was there a question? Sorry?


98
00:12:54.349 --> 00:12:59.879
Inderjit Dhillon: Okay, if there's a question, just kind of always just interrupt me, because I sometimes don't see the chat.


99
00:13:00.539 --> 00:13:04.469
Inderjit Dhillon: I don't think there's a question. But let me know if there is


100
00:13:05.799 --> 00:13:15.069
Inderjit Dhillon: okay. Sorry. I just had heard a beep. So that's why I thought there was a question, okay? And so the matrix sigma, capital sigma.


101
00:13:15.369 --> 00:13:23.219
Inderjit Dhillon: These are little Sigma sigma, one sigma, 2 lowercase sigma sigma, n.


102
00:13:24.609 --> 00:13:28.128
Inderjit Dhillon: these values are zeros. And so are these values.


103
00:13:29.429 --> 00:13:31.759
Inderjit Dhillon: Okay, so it's a diagonal matrix.


104
00:13:32.359 --> 00:13:41.118
Inderjit Dhillon: And then I have v, 1, transpose v, 2, transpose B and transpose.


105
00:13:41.639 --> 00:13:47.438
Inderjit Dhillon: These are the rows of V transpose or the columns of V,


106
00:13:47.779 --> 00:13:55.718
Inderjit Dhillon: so these, this matrix U consists of left singular vectors.


107
00:14:00.039 --> 00:14:03.719
Inderjit Dhillon: Okay. So sometimes it's called the matrix of left singular vectors.


108
00:14:04.209 --> 00:14:13.779
Inderjit Dhillon: So maybe it's better if I make the arrow to be pointing from here.


109
00:14:15.429 --> 00:14:25.418
Inderjit Dhillon: Okay, so these vectors u 1 through UN UN plus one through are called left singular vectors because they appear on the left of the decomposition.


110
00:14:25.619 --> 00:14:31.408
Inderjit Dhillon: So similarly, these are the right singular vectors.


111
00:14:38.358 --> 00:14:44.539
Inderjit Dhillon: Okay, and note that U transpose U equals. I,


112
00:14:45.429 --> 00:14:50.719
Inderjit Dhillon: that means that if I take Ui transpose Uj.


113
00:14:52.338 --> 00:14:58.429
Inderjit Dhillon: that is equal to one, if I equal to J and 0, otherwise.


114
00:14:59.959 --> 00:15:03.949
Inderjit Dhillon: Okay. So that means these vectors are mutually orthogonal.


115
00:15:04.569 --> 00:15:10.349
Inderjit Dhillon: Okay? And they basically form an orthogonal basis for RM.


116
00:15:11.249 --> 00:15:18.789
Inderjit Dhillon: And since it is square, let me actually explicitly write it because this is important.


117
00:15:24.949 --> 00:15:29.869
Inderjit Dhillon: since you is square


118
00:15:33.209 --> 00:15:41.969
Inderjit Dhillon: and U transpose U equals I. This implies that UU transpose equals. I.


119
00:15:42.399 --> 00:15:47.498
Inderjit Dhillon: So what you have is U transpose U equal UU transpose


120
00:15:47.739 --> 00:15:58.989
Inderjit Dhillon: equals. I. And note, this is only because U is square. Okay? And we'll come to situations where we might actually encounter orthogonal orthogonal matrices that are not square.


121
00:15:59.159 --> 00:15:59.989
Inderjit Dhillon: Okay?


122
00:16:00.329 --> 00:16:08.098
Inderjit Dhillon: So similarly, V transpose v equals. I, again, it's an orthogonal matrix.


123
00:16:08.999 --> 00:16:15.898
Inderjit Dhillon: Vi transpose. Vj, is equal to one if I equal today


124
00:16:16.219 --> 00:16:19.629
Inderjit Dhillon: and a 0 FI not equal to J,


125
00:16:20.249 --> 00:16:33.539
Inderjit Dhillon: okay. And clearly over here you have I and J, or between one and N, and over here


126
00:16:36.119 --> 00:16:41.339
Inderjit Dhillon: I and J, or between one and M,


127
00:16:42.889 --> 00:16:48.069
Inderjit Dhillon: okay? And then the all important singular values.


128
00:16:48.169 --> 00:16:53.289
Inderjit Dhillon: These are the singular values over here these values.


129
00:17:00.309 --> 00:17:04.278
Inderjit Dhillon: Okay, so let me kind of just say singular values.


130
00:17:09.029 --> 00:17:17.899
Inderjit Dhillon: Typically, they are arranged in decreasing order. So the convention is typically that we write it. As Sigma, one is greater than equal to Sigma. 2


131
00:17:18.699 --> 00:17:23.819
Inderjit Dhillon: is greater than equal to Sigma, N, which is greater than equal to 0.


132
00:17:24.579 --> 00:17:30.399
Inderjit Dhillon: Okay, so, Sigma, I belongs to R, plus.


133
00:17:31.089 --> 00:17:34.328
Inderjit Dhillon: That means they are non-negative numbers.


134
00:17:34.859 --> 00:17:38.129
Inderjit Dhillon: And typically they are arranged in decreasing order.


135
00:17:44.339 --> 00:17:50.698
Inderjit Dhillon: Okay, so that's the singular value decomposition. And let's talk a little bit more about the singular value decomposition.


136
00:17:51.849 --> 00:17:58.199
Inderjit Dhillon: if A is equal to U. Sigma v. Transpose. So if I have this decomposition.


137
00:17:58.989 --> 00:18:08.069
Inderjit Dhillon: this means that if I multiply each side, oh, sorry the right. Then both left hand, side and right hand side by the matrix. V.


138
00:18:08.639 --> 00:18:14.498
Inderjit Dhillon: Then I get a times V is equal to U. Sigma V transpose V,


139
00:18:15.089 --> 00:18:18.668
Inderjit Dhillon: and since I know that V transpose V is the identity.


140
00:18:18.839 --> 00:18:22.439
Inderjit Dhillon: this means that this is equal to use signal.


141
00:18:24.069 --> 00:18:26.929
Inderjit Dhillon: Okay? So that means that a times V


142
00:18:28.089 --> 00:18:31.039
Inderjit Dhillon: is equal to U times. Sigma.


143
00:18:31.509 --> 00:18:35.298
Inderjit Dhillon: okay, you can cast your mind back to this. Not remember.


144
00:18:35.479 --> 00:18:40.009
Inderjit Dhillon: it's starting to look a little bit like eigenvalue decomposition. But it's different.


145
00:18:40.749 --> 00:18:47.739
Inderjit Dhillon: Okay? Because here what we have is a times. Vi. If I look at the Ith column of both sides


146
00:18:48.059 --> 00:18:56.269
Inderjit Dhillon: a times. Vi is equal to Sigma. I times ui, okay, for


147
00:18:58.839 --> 00:19:03.219
Inderjit Dhillon: one is less than equal to. I is less than equal to N,


148
00:19:10.429 --> 00:19:11.359
Inderjit Dhillon: okay.


149
00:19:11.659 --> 00:19:14.788
Inderjit Dhillon: And now notice one thing over here. Okay.


150
00:19:15.199 --> 00:19:18.688
Inderjit Dhillon: I have this matrix, a, which is M, by N,


151
00:19:19.579 --> 00:19:24.089
Inderjit Dhillon: so in this case the number of columns is smaller than the number of rows.


152
00:19:24.549 --> 00:19:27.359
Inderjit Dhillon: But this matrix is M by M.


153
00:19:28.599 --> 00:19:31.079
Inderjit Dhillon: This matrix is M. By N.


154
00:19:31.719 --> 00:19:33.589
Inderjit Dhillon: If I look at the last


155
00:19:33.899 --> 00:19:37.469
Inderjit Dhillon: NM. Minus N columns of U,


156
00:19:37.629 --> 00:19:40.208
Inderjit Dhillon: and if you think about matrix multiplication.


157
00:19:40.939 --> 00:19:45.979
Inderjit Dhillon: then you'll see that this part gets multiplied by this part.


158
00:19:46.629 --> 00:19:49.499
Inderjit Dhillon: and this part UN plus one through


159
00:19:50.049 --> 00:19:55.979
Inderjit Dhillon: is actually multiplied by the 0 matrix this part and that's actually 0.


160
00:19:57.099 --> 00:20:02.019
Inderjit Dhillon: So really, the way you can write this decomposition is a


161
00:20:02.349 --> 00:20:05.248
Inderjit Dhillon: is equal to the 1st part of u


162
00:20:05.879 --> 00:20:08.579
Inderjit Dhillon: times the upper part of Sigma


163
00:20:09.419 --> 00:20:15.118
Inderjit Dhillon: and the V. Is left unchanged in the case that M is greater than equal to N.


164
00:20:15.479 --> 00:20:24.749
Inderjit Dhillon: So sometimes people will call this the full. Svd, okay.


165
00:20:25.889 --> 00:20:36.069
Inderjit Dhillon: And when you have the full Svd, there is something called either the thin or reduced Svd.


166
00:20:41.229 --> 00:20:43.139
Inderjit Dhillon: okay. And that's the following.


167
00:20:45.489 --> 00:20:55.719
Inderjit Dhillon: thin or reduced Svd is A is equal to just to, so that you are clear. That this is a different U and Sigma, I'll just write it as U hat


168
00:20:56.339 --> 00:20:57.639
Inderjit Dhillon: sigma hat.


169
00:20:58.109 --> 00:20:59.758
Inderjit Dhillon: We have transpose.


170
00:21:00.749 --> 00:21:01.679
Inderjit Dhillon: Okay?


171
00:21:01.919 --> 00:21:05.902
Inderjit Dhillon: And in this case, AI sorry.


172
00:21:13.049 --> 00:21:19.189
Inderjit Dhillon: In this case you had belongs to M. By M.


173
00:21:19.509 --> 00:21:25.439
Inderjit Dhillon: Sorry. M. By N, okay. So U hat is actually rectangular matrix.


174
00:21:25.729 --> 00:21:32.789
Inderjit Dhillon: It's still orthogonal, because U transpose U hat. Transpose U hat is the identity.


175
00:21:33.989 --> 00:21:39.839
Inderjit Dhillon: Sigma hat belongs to this is now a square matrix. N, by N


176
00:21:41.209 --> 00:21:44.219
Inderjit Dhillon: and V hat is also a square matrix.


177
00:21:46.959 --> 00:21:51.978
Inderjit Dhillon: and this is V hat. Transpose V hat equals identity.


178
00:21:54.039 --> 00:22:02.159
Inderjit Dhillon: Okay? And this is a now, a square diagonal matrix.


179
00:22:06.259 --> 00:22:10.559
Inderjit Dhillon: And really, I'm not doing anything new. If you know about the full Svd.


180
00:22:10.739 --> 00:22:20.889
Inderjit Dhillon: the thin Svd actually just follows, because what you can do is you can take this part of you.


181
00:22:21.549 --> 00:22:23.388
Inderjit Dhillon: And this is just U hat.


182
00:22:24.349 --> 00:22:28.849
Inderjit Dhillon: Okay? So the first, st the M. By N. Matrix.


183
00:22:29.139 --> 00:22:34.169
Inderjit Dhillon: With the 1st n left. Singular vectors forms you had


184
00:22:34.799 --> 00:22:42.429
Inderjit Dhillon: now the important thing to realize, and this comes up when you are kind of manipulating formulae around. And that's 1 of the reasons why I want to


185
00:22:43.150 --> 00:22:46.338
Inderjit Dhillon: stress. It is that in general.


186
00:22:46.669 --> 00:22:53.959
Inderjit Dhillon: if M is not equal to n right? And then well.


187
00:22:54.789 --> 00:22:59.548
Inderjit Dhillon: you had U hat transpose is not the identity.


188
00:23:00.539 --> 00:23:02.659
Inderjit Dhillon: So I can't just reverse this


189
00:23:03.669 --> 00:23:05.808
Inderjit Dhillon: because you had is not square.


190
00:23:06.509 --> 00:23:11.718
Inderjit Dhillon: On the other hand, when M is greater than or equal to N. And V hat is square.


191
00:23:12.059 --> 00:23:20.949
Inderjit Dhillon: then we had is square, which means that V hat transpose is the left inverse.


192
00:23:21.169 --> 00:23:25.999
Inderjit Dhillon: When you have a square matrix, the left inverse is the same as the right. Inverse.


193
00:23:26.179 --> 00:23:33.209
Inderjit Dhillon: As a result, you actually have that V hat V hat transpose is equal to the identity.


194
00:23:34.599 --> 00:23:35.419
Inderjit Dhillon: Okay?


195
00:23:40.849 --> 00:23:42.559
Inderjit Dhillon: Any questions so far.


196
00:23:46.139 --> 00:23:56.228
Inderjit Dhillon: Okay, it's a very important decomposition, and it can come up in different scenarios, and we'll see that, you know there's 1 particular way of regularizing


197
00:23:59.319 --> 00:24:10.978
Inderjit Dhillon: called Bridge for regression called Bridge Regression, and we will see that by knowing the Svd. We can actually get a much better and much more interesting interpretation


198
00:24:11.219 --> 00:24:13.849
Inderjit Dhillon: of what Ridge regression does.


199
00:24:16.032 --> 00:24:24.259
Hormoz Shahrzad: Question. Sorry. So what significance of you over the you had? Because it seems like all those


200
00:24:24.589 --> 00:24:32.859
Hormoz Shahrzad: you know, the other vectors, you know greater than N to M are basically


201
00:24:33.949 --> 00:24:39.108
Hormoz Shahrzad: getting getting to 0 by by the multiplication. So.


202
00:24:39.109 --> 00:24:40.219
Inderjit Dhillon: Exactly.


203
00:24:40.949 --> 00:24:44.569
Hormoz Shahrzad: And so why are we, even, you know, care about


204
00:24:45.279 --> 00:24:47.718
Hormoz Shahrzad: creating the you rather than you had.


205
00:24:48.249 --> 00:24:55.028
Inderjit Dhillon: Yeah, so you'll see that sometimes it'll be easier to use you hat. But but I will actually come to a characterization


206
00:24:55.329 --> 00:25:04.579
Inderjit Dhillon: of the the entire U, so you know, when you're doing a 1st course in linear algebra, you might have studied things like


207
00:25:04.699 --> 00:25:07.019
Inderjit Dhillon: the column space of a matrix.


208
00:25:07.619 --> 00:25:10.059
Inderjit Dhillon: The row space of the matrix.


209
00:25:10.429 --> 00:25:12.559
Inderjit Dhillon: the null space of a matrix.


210
00:25:12.859 --> 00:25:14.979
Inderjit Dhillon: the null space of its transpose.


211
00:25:15.199 --> 00:25:20.654
Inderjit Dhillon: So we'll actually see that the vectors UN plus one through


212
00:25:21.749 --> 00:25:27.649
Inderjit Dhillon: they are actually in the. They form a basis of the right null space of a.


213
00:25:29.219 --> 00:25:29.899
Hormoz Shahrzad: Let me see.


214
00:25:30.049 --> 00:25:33.048
Inderjit Dhillon: Okay, so it's just good to carry it around so that


215
00:25:33.509 --> 00:25:41.148
Inderjit Dhillon: you know the full singular value decomposition just gives. You, you know, a holistic picture of the entire matrix.


216
00:25:41.579 --> 00:25:42.279
Hormoz Shahrzad: Thank you.


217
00:25:42.279 --> 00:25:47.139
Inderjit Dhillon: In many cases you'd only need to use the thin Svd.


218
00:25:48.669 --> 00:25:49.988
Hormoz Shahrzad: I see. Thank you.


219
00:25:50.329 --> 00:25:55.468
Inderjit Dhillon: But whenever you're doing it, you should just make sure that you realize which one you're using.


220
00:25:55.779 --> 00:25:58.538
Inderjit Dhillon: Because, like I said, you know, in some cases.


221
00:25:58.739 --> 00:25:59.409
Hormoz Shahrzad: Correct.


222
00:25:59.889 --> 00:26:09.968
Inderjit Dhillon: UU. Transpose is equal to identity, and in some other cases it may not be. And we'll actually see that regression is just doing the operation of UU transpose


223
00:26:10.199 --> 00:26:14.009
Inderjit Dhillon: it's actually UU transpose is actually an orthogonal projector.


224
00:26:14.269 --> 00:26:16.678
Inderjit Dhillon: And it's actually projecting onto the


225
00:26:16.989 --> 00:26:26.678
Inderjit Dhillon: the A particular space defined by the matrix of the data with all its features. And again, I'll come to that very soon.


226
00:26:28.059 --> 00:26:28.819
Hormoz Shahrzad: Thank you.


227
00:26:28.819 --> 00:26:29.549
Inderjit Dhillon: Okay.


228
00:26:30.459 --> 00:26:42.249
Inderjit Dhillon: okay, so let's let's do a little bit of like manipulation with these with the Svd. So if I have the Svd of a. And let's use the reduced Svd.


229
00:26:42.849 --> 00:26:48.819
Inderjit Dhillon: A hat is equal to U hat sigma hat we had transpose.


230
00:26:49.119 --> 00:26:53.268
Inderjit Dhillon: Well, this implies that a transpose what is a transpose.


231
00:26:54.309 --> 00:27:01.179
Inderjit Dhillon: Well, it is u hat sigma hat v hat transpose, transpose.


232
00:27:01.609 --> 00:27:03.599
Inderjit Dhillon: and if you know the rule, for


233
00:27:03.849 --> 00:27:06.769
Inderjit Dhillon: you know, taking the transpose of a product


234
00:27:07.079 --> 00:27:09.798
Inderjit Dhillon: right? So in general, if you have


235
00:27:10.629 --> 00:27:17.188
Inderjit Dhillon: XY. Transpose where x and y are matrices, this is y transpose x transpose.


236
00:27:17.899 --> 00:27:27.179
Inderjit Dhillon: So as a result, I have V hat transpose, transpose, which is v hat sigma hat transpose. But it's


237
00:27:27.709 --> 00:27:34.179
Inderjit Dhillon: square and symmetric sigma hat. So it'll be the same as sigma hat.


238
00:27:35.169 --> 00:27:36.478
Inderjit Dhillon: And then I have this.


239
00:27:37.649 --> 00:27:45.579
Inderjit Dhillon: Okay, so this is V hat Sigma hat you had transpose.


240
00:27:46.309 --> 00:27:50.099
Inderjit Dhillon: So if I do a transpose times a.


241
00:27:50.909 --> 00:27:53.689
Inderjit Dhillon: so a has this as Svd


242
00:27:54.799 --> 00:27:58.478
Inderjit Dhillon: and the Svd of a transpose is actually over here.


243
00:28:01.189 --> 00:28:04.539
Inderjit Dhillon: Okay, so obviously, it's clear that when you have the Svd of


244
00:28:06.299 --> 00:28:12.478
Inderjit Dhillon: a trans a a, then you actually have the Svd of Hat.


245
00:28:13.089 --> 00:28:14.459
Inderjit Dhillon: Sorry it transfers.


246
00:28:14.609 --> 00:28:21.018
Inderjit Dhillon: Okay, so a transpose A is equal to V hat sigma hat.


247
00:28:21.929 --> 00:28:25.078
Inderjit Dhillon: You had transpose right times.


248
00:28:25.779 --> 00:28:28.499
Inderjit Dhillon: You had Sigma hat.


249
00:28:28.989 --> 00:28:30.549
Inderjit Dhillon: We have transports.


250
00:28:32.619 --> 00:28:37.529
Inderjit Dhillon: Okay? So you'll see that I have U Hat transpose U hat over here.


251
00:28:38.019 --> 00:28:48.339
Inderjit Dhillon: This is the identity. So as a result this is, we had sigma hat square times. V. Transpose


252
00:28:49.399 --> 00:28:52.909
Inderjit Dhillon: we have transpose, and if you think about it.


253
00:28:53.609 --> 00:28:57.568
Inderjit Dhillon: remember that a transpose a what is the size of a transpose a.


254
00:28:58.129 --> 00:29:03.118
Inderjit Dhillon: Well, remember a is M. By n.


255
00:29:04.019 --> 00:29:07.108
Inderjit Dhillon: so a transpose a is actually n, by N.


256
00:29:07.109 --> 00:29:08.069
Hormoz Shahrzad: One m.


257
00:29:08.959 --> 00:29:09.949
Hormoz Shahrzad: Anybody.


258
00:29:10.409 --> 00:29:11.444
Inderjit Dhillon: Sorry, did I?


259
00:29:11.789 --> 00:29:12.679
Hormoz Shahrzad: Emily.


260
00:29:13.019 --> 00:29:17.169
Inderjit Dhillon: Yeah, a is m, by n.


261
00:29:17.999 --> 00:29:22.949
Inderjit Dhillon: so a transpose a belongs to end by end.


262
00:29:23.119 --> 00:29:25.318
Inderjit Dhillon: So a transpose a square matrix.


263
00:29:25.809 --> 00:29:28.178
Inderjit Dhillon: And then what do you see over here? Well.


264
00:29:28.439 --> 00:29:31.698
Inderjit Dhillon: you actually have the eigenvalue decomposition


265
00:29:41.409 --> 00:29:43.398
Inderjit Dhillon: of a transport section.


266
00:29:45.589 --> 00:29:56.928
Inderjit Dhillon: So you can actually look at this and realize that the singular values are basically square roots of the eigenvalue. The singular values of A are the square roots of the eigenvalues of


267
00:29:57.169 --> 00:29:58.339
Inderjit Dhillon: a transposit.


268
00:30:00.089 --> 00:30:04.099
Inderjit Dhillon: Okay, similarly, if I do a a transpose.


269
00:30:04.889 --> 00:30:13.349
Inderjit Dhillon: Then I get u hat sigma hat V hat. Transpose we had sigma hat.


270
00:30:14.279 --> 00:30:19.389
Inderjit Dhillon: You had transpose v hat times V is identity.


271
00:30:19.539 --> 00:30:25.989
Inderjit Dhillon: and so I get U hat sigma hat squared, you had transpose.


272
00:30:26.289 --> 00:30:27.639
Inderjit Dhillon: And what is that?


273
00:30:28.589 --> 00:30:35.238
Inderjit Dhillon: That's again. Now, the Eigenvalue decomposition, right of a a transpose.


274
00:30:38.389 --> 00:30:43.849
Inderjit Dhillon: So given a you can see that a transpose, a


275
00:30:44.629 --> 00:30:49.599
Inderjit Dhillon: and a transpose, a transpose, a, and a transpose, actually kind of linked together.


276
00:30:51.579 --> 00:30:58.509
Inderjit Dhillon: They have the same eigenvalues.


277
00:30:59.159 --> 00:31:05.579
Inderjit Dhillon: but their Eigenvectors are different. Right? A transpose a has Eigenvectors we had


278
00:31:06.409 --> 00:31:10.619
Inderjit Dhillon: and a a transpose? Has Eigenvectors as U hat?


279
00:31:12.839 --> 00:31:13.829
Inderjit Dhillon: Okay?


280
00:31:14.059 --> 00:31:18.839
Inderjit Dhillon: And you can say say also, remember that we talked about positive, definite matrices.


281
00:31:19.609 --> 00:31:28.839
Inderjit Dhillon: A transpose, a and a a transpose are positive, semidefinite matrices.


282
00:31:34.128 --> 00:31:38.149
Inderjit Dhillon: Okay? And that becomes clear from here, right? Because there's a square over here.


283
00:31:38.599 --> 00:31:44.898
Inderjit Dhillon: Okay? So that means their Eigenvalues Eigenvalues.


284
00:31:46.869 --> 00:31:49.678
Inderjit Dhillon: or greater than equal to 6.


285
00:31:53.579 --> 00:31:56.409
Inderjit Dhillon: Okay, so that's when I when I said, You know the


286
00:31:56.909 --> 00:32:13.388
Inderjit Dhillon: singular value. Decomposition is the Rolls Royce of all matrix decompositions. You know this is one example, right? It has very kind of interesting properties, right? That a transpose and a transpose A are really linked together through the Svd.


287
00:32:13.699 --> 00:32:16.908
Inderjit Dhillon: Now let's look at even a more really beautiful


288
00:32:17.049 --> 00:32:24.729
Inderjit Dhillon: part of the Svd, right? So remember that. Let's think of M is greater than equal to N.


289
00:32:25.889 --> 00:32:30.308
Inderjit Dhillon: A. Belongs to M. By N.


290
00:32:30.779 --> 00:32:39.228
Inderjit Dhillon: Okay, and let me now write the Svd. Okay, a. Is equal to U. Sigma V. Transpose


291
00:32:40.019 --> 00:32:46.019
Inderjit Dhillon: I have a transpose is equal to V. Sigma. Transpose U transpose.


292
00:32:46.139 --> 00:32:50.269
Inderjit Dhillon: Sorry, not Sigma hand. So here, by the way, I'm doing the full. Svd, okay.


293
00:32:55.309 --> 00:32:59.479
Inderjit Dhillon: so U is square and V is square, and Sigma is actually rectangle.


294
00:33:00.419 --> 00:33:04.019
Inderjit Dhillon: Now, in both cases I can take a times. V


295
00:33:04.399 --> 00:33:07.288
Inderjit Dhillon: is equal to U times sigma.


296
00:33:07.699 --> 00:33:15.129
Inderjit Dhillon: Similarly, I can take a transpose times U by post, multiplying both sides by matrix U,


297
00:33:15.499 --> 00:33:23.108
Inderjit Dhillon: I have v. Sigma transpose, and this means that a if I look at the each column.


298
00:33:23.549 --> 00:33:31.989
Inderjit Dhillon: A of Vi is equal to UI sigma I, for I


299
00:33:33.039 --> 00:33:39.518
Inderjit Dhillon: between one and N. And similarly, I have a transpose. UI


300
00:33:40.399 --> 00:33:45.168
Inderjit Dhillon: is equal to. Well, let's here. Actually, I need to be a little bit careful, right?


301
00:33:45.419 --> 00:33:51.439
Inderjit Dhillon: A transpose. VVUI is equal to VI.


302
00:33:52.329 --> 00:34:00.859
Inderjit Dhillon: Sigma, i, for I between one and one and n.


303
00:34:02.829 --> 00:34:05.178
Inderjit Dhillon: because there are only N. V's.


304
00:34:06.329 --> 00:34:11.439
Inderjit Dhillon: and then, for the rest, a transpose ui equals 0.


305
00:34:12.589 --> 00:34:19.078
Inderjit Dhillon: Okay, n plus one is less than equal to. I is less than equal to N.


306
00:34:26.319 --> 00:34:29.468
Inderjit Dhillon: Remember, I talked about the Svd as.


307
00:34:30.859 --> 00:34:37.689
Inderjit Dhillon: oh, sorry a matrix as doing a linear transformation. So if I have an M by N, matrix, a


308
00:34:38.589 --> 00:34:42.969
Inderjit Dhillon: basically is a map. Linear map represents a linear map.


309
00:34:43.459 --> 00:34:47.049
Inderjit Dhillon: Linear transformation for Rn to rm.


310
00:34:47.499 --> 00:34:51.098
Inderjit Dhillon: so this is X to a times. X.


311
00:34:51.999 --> 00:35:01.869
Inderjit Dhillon: Now, what is a if I know the Svd of a this is U. Sigma V transpose times X.


312
00:35:03.269 --> 00:35:07.628
Inderjit Dhillon: So now I can break a times X into 3 parts. Actually.


313
00:35:07.939 --> 00:35:14.469
Inderjit Dhillon: I can 1st say it's V transpose times X, and the result is, Sigma.


314
00:35:15.189 --> 00:35:19.658
Inderjit Dhillon: Result is then transformed by Sigma, which is then transformed by U,


315
00:35:27.589 --> 00:35:31.978
Inderjit Dhillon: okay, and what is the point in writing this? Right? Well.


316
00:35:33.629 --> 00:35:38.839
Inderjit Dhillon: here's the really interesting part, right? Which is that if I take.


317
00:35:39.209 --> 00:35:41.348
Inderjit Dhillon: So if I kind of draw a picture.


318
00:35:43.469 --> 00:35:47.478
Inderjit Dhillon: okay? And I kind of think of this as rn.


319
00:35:50.129 --> 00:35:55.848
Inderjit Dhillon: right? And then we know that multiplying A


320
00:35:56.189 --> 00:36:06.069
Inderjit Dhillon: or a acting on our end as a linear transformation transforms it into RM.


321
00:36:09.159 --> 00:36:10.139
Inderjit Dhillon: Okay.


322
00:36:11.319 --> 00:36:17.199
Inderjit Dhillon: And what we just saw, for example, is like over here


323
00:36:17.659 --> 00:36:21.459
Inderjit Dhillon: that a times V is equal to U times sigma.


324
00:36:22.289 --> 00:36:26.468
Inderjit Dhillon: So, for example, if I have v 1,


325
00:36:26.669 --> 00:36:33.679
Inderjit Dhillon: the unit, vector right? So it's on the unit sphere.


326
00:36:34.499 --> 00:36:36.558
Inderjit Dhillon: And remember, the V's are orthogonal.


327
00:36:36.669 --> 00:36:41.269
Inderjit Dhillon: So v, 2 is going to be somewhere here, right?


328
00:36:41.789 --> 00:36:49.029
Inderjit Dhillon: That's going to be transformed into something like depending upon a.


329
00:36:49.499 --> 00:36:53.268
Inderjit Dhillon: This could be Sigma one u 1.


330
00:36:54.419 --> 00:36:58.368
Inderjit Dhillon: And and you know, suppose this is the unit square.


331
00:36:58.859 --> 00:37:05.959
Inderjit Dhillon: then you know it could be magnified or it could be the norm could be actually smaller than one.


332
00:37:06.289 --> 00:37:15.169
Inderjit Dhillon: And then, you know, maybe Sigma to U 2 is something like this right? That's what this says.


333
00:37:16.479 --> 00:37:23.748
Inderjit Dhillon: right? But based on the Svd, I can actually decompose it. Further, right? I can say that a times X


334
00:37:24.199 --> 00:37:28.239
Inderjit Dhillon: is essentially 3 transformations. V transpose times x


335
00:37:28.519 --> 00:37:35.849
Inderjit Dhillon: sigma times. The result. Times u times the result. So I can write this as that. First, st


336
00:37:39.499 --> 00:37:41.979
Inderjit Dhillon: there's an action of V transpose.


337
00:37:43.999 --> 00:37:45.639
Inderjit Dhillon: And what does that give us?


338
00:37:52.199 --> 00:38:00.339
Inderjit Dhillon: Where is v 1 mapped onto? Let me try to maybe use a different color.


339
00:38:05.679 --> 00:38:08.629
Inderjit Dhillon: Well, before I do that.


340
00:38:10.409 --> 00:38:19.738
Inderjit Dhillon: Well, what? So when I'm asking, what? What is V transpose? Times v, 1.


341
00:38:21.639 --> 00:38:24.089
Inderjit Dhillon: And V transpose times, v, 2,


342
00:38:28.929 --> 00:38:37.619
Inderjit Dhillon: because I'm looking wanted. So you see, the image of the vector v, 1, and of v, 2. Sorry, I think I, you wrote U 2 over here.


343
00:38:38.719 --> 00:38:39.949
Inderjit Dhillon: Change that.


344
00:38:45.399 --> 00:38:51.959
Inderjit Dhillon: So let's look at it right? Let let EI


345
00:38:53.039 --> 00:39:00.499
Inderjit Dhillon: be the ith column of the identity. So you know, e, 1 is 1 0 0 0,


346
00:39:01.279 --> 00:39:07.128
Inderjit Dhillon: and EI is, you know, it's ith column of the identity. So just 1 1


347
00:39:08.419 --> 00:39:10.078
Inderjit Dhillon: in the 8th place


348
00:39:12.479 --> 00:39:18.068
Inderjit Dhillon: and zeros, otherwise. Okay, so ei is the ith column of identity. Let me write that


349
00:39:18.509 --> 00:39:27.368
Inderjit Dhillon: EI is ielts column of the identity matrix.


350
00:39:29.589 --> 00:39:31.429
Inderjit Dhillon: So what is v times? Ei?


351
00:39:31.739 --> 00:39:35.359
Inderjit Dhillon: Well, it just picks out the I had column.


352
00:39:37.809 --> 00:39:45.709
Inderjit Dhillon: Okay, if I now multiply both sides by V transpose I get v transpose VEI


353
00:39:46.119 --> 00:39:48.669
Inderjit Dhillon: is equal to v transpose VI.


354
00:39:49.849 --> 00:39:55.668
Inderjit Dhillon: But v transpose v is identity, so ei is equal to v transpose. VI,


355
00:40:03.939 --> 00:40:13.588
Inderjit Dhillon: okay, so what that means is that we transpose v, 1 is e, 1, and this is e, 2,


356
00:40:14.519 --> 00:40:18.889
Inderjit Dhillon: which means that this is e 1.


357
00:40:19.019 --> 00:40:23.849
Inderjit Dhillon: It's basically along the axis on the unit sphere.


358
00:40:24.259 --> 00:40:25.648
Inderjit Dhillon: This is E, 2.


359
00:40:26.839 --> 00:40:28.599
Inderjit Dhillon: This is on the unit sphere.


360
00:40:31.439 --> 00:40:34.389
Inderjit Dhillon: And what this map we transpose is doing


361
00:40:37.639 --> 00:40:51.478
Inderjit Dhillon: is that v, 1 is mapped onto Yvonne and v, 2 is mapped onto E, 2.


362
00:40:52.869 --> 00:40:55.718
Inderjit Dhillon: Okay, so that's the 1st part of the linear transform.


363
00:40:58.769 --> 00:41:01.949
Inderjit Dhillon: The second part is, remember.


364
00:41:07.069 --> 00:41:11.629
Inderjit Dhillon: use Sigma V transpose. So that second part is Sigma.


365
00:41:11.809 --> 00:41:15.549
Inderjit Dhillon: Okay? So if I do, Sigma.


366
00:41:15.789 --> 00:41:23.299
Inderjit Dhillon: But what is Sigma, it's just actually stretching the the each each dimension.


367
00:41:25.059 --> 00:41:33.149
Inderjit Dhillon: Okay, so e, 1 is going to be mapped onto sigma, one e, 1.


368
00:41:43.209 --> 00:41:46.248
Inderjit Dhillon: Okay, so the unit sphere is somewhere over here.


369
00:41:47.399 --> 00:41:53.499
Inderjit Dhillon: Okay? And Sigma 2. So VE, 2 is mapped to Sigma 2 e, 2.


370
00:41:53.959 --> 00:41:57.989
Inderjit Dhillon: Is that clear? Because Sigma is a diagonal matrix. Right?


371
00:41:58.319 --> 00:42:02.079
Inderjit Dhillon: So sigma times e, 1 is equal to


372
00:42:02.399 --> 00:42:07.638
Inderjit Dhillon: sigma one times e, 1, and the matrix sigma times e, 2


373
00:42:07.859 --> 00:42:12.898
Inderjit Dhillon: is equal to sigma 2 times e, 2. So it's just stretching of the axis.


374
00:42:13.689 --> 00:42:20.979
Inderjit Dhillon: And then, finally, we have the 3rd part, which is you.


375
00:42:21.709 --> 00:42:27.909
Inderjit Dhillon: And this is the action of you.


376
00:42:28.729 --> 00:42:33.419
Inderjit Dhillon: Okay? And remember that u times, ei, what is that?


377
00:42:33.849 --> 00:42:42.719
Inderjit Dhillon: It's just picking up the Ith column of the matrix? U, so it's just ui, so


378
00:42:43.549 --> 00:42:45.208
Inderjit Dhillon: this is what is happening.


379
00:42:45.959 --> 00:42:48.158
Inderjit Dhillon: Let me get back the same color.


380
00:42:49.139 --> 00:42:54.729
Inderjit Dhillon: So e 1 gets mapped to sigma, one e, 1


381
00:42:55.629 --> 00:42:59.258
Inderjit Dhillon: e, 2 gets mapped to sigma to E, 2.


382
00:43:00.519 --> 00:43:10.189
Inderjit Dhillon: Okay, and then sigma one e, 1 gets mapped to Sigma one u 1 and Sigma 2 e, 2


383
00:43:10.489 --> 00:43:14.329
Inderjit Dhillon: gets mapped to Sigma to Youtube.


384
00:43:16.159 --> 00:43:20.342
Inderjit Dhillon: Okay. So again, like a very really interesting


385
00:43:22.069 --> 00:43:26.219
Inderjit Dhillon: way to think about a linear transform, right? That


386
00:43:26.499 --> 00:43:40.498
Inderjit Dhillon: you think of just a linear transform of mapping things vectors onto other vectors, but you can actually break it down to where it. And and one thing I, you know, did not mention explicitly that this is Rn.


387
00:43:41.029 --> 00:43:42.678
Inderjit Dhillon: And this is our app.


388
00:43:46.319 --> 00:43:47.229
Inderjit Dhillon: Okay?


389
00:43:48.889 --> 00:43:56.938
Inderjit Dhillon: And then I think Hormos had asked me, Okay, what is the point of carrying around these vectors? UR plus one through


390
00:43:57.839 --> 00:44:07.599
Inderjit Dhillon: So again, a very beautiful characterization of the singular value, decomposition is the following right? So if matrix A,


391
00:44:07.739 --> 00:44:10.169
Inderjit Dhillon: I haven't spoken about rank so far.


392
00:44:10.859 --> 00:44:14.068
Inderjit Dhillon: but if matrix A has rank R,


393
00:44:14.359 --> 00:44:20.188
Inderjit Dhillon: okay, and R has to be less than equal to minimum of mn.


394
00:44:20.939 --> 00:44:26.269
Inderjit Dhillon: and if M is greater than or equal to N, it means that r is less than equal to n.


395
00:44:26.659 --> 00:44:32.809
Inderjit Dhillon: so R is the rank of the matrix, and that's exactly equal to


396
00:44:33.749 --> 00:44:36.598
Inderjit Dhillon: the number of non-zero singular values.


397
00:44:37.979 --> 00:44:46.008
Inderjit Dhillon: That's really the almost numerically. The best way to determine the rank of a matrix is to actually do an Svd, okay.


398
00:44:46.139 --> 00:44:48.449
Inderjit Dhillon: so and so let me write it out.


399
00:44:48.619 --> 00:44:52.358
Inderjit Dhillon: This means that Sigma one is greater than equal to Sigma 2


400
00:44:53.279 --> 00:44:57.939
Inderjit Dhillon: is greater than equal to Sigma R. And this is greater than 0.


401
00:44:58.769 --> 00:45:06.759
Inderjit Dhillon: And what that means is that Sigma r plus one equals sigma r plus 2 to Sigma, n.


402
00:45:07.039 --> 00:45:08.409
Inderjit Dhillon: equal 0.


403
00:45:10.449 --> 00:45:11.299
Inderjit Dhillon: Okay?


404
00:45:12.239 --> 00:45:15.639
Inderjit Dhillon: And then if I think about a my matrix a.


405
00:45:16.849 --> 00:45:21.818
Inderjit Dhillon: Then I have a is Rn, 2 RM.


406
00:45:23.429 --> 00:45:28.418
Inderjit Dhillon: This is its maps. v. 1, as we saw to


407
00:45:28.649 --> 00:45:43.089
Inderjit Dhillon: u, 1 sigma, one v. 2, 2, u, 2, sigma, 2, we are to UR. Sigma, r.


408
00:45:45.189 --> 00:45:50.808
Inderjit Dhillon: and VR, plus one. It actually maps to 0 because the rank is.


409
00:45:51.869 --> 00:45:58.459
Inderjit Dhillon: And similarly, we are those 4 VN


410
00:46:00.499 --> 00:46:03.808
Inderjit Dhillon: is mapped to 0. Note that r may be equal to.


411
00:46:04.519 --> 00:46:08.818
Inderjit Dhillon: and in which case there are no, there's no vector


412
00:46:09.009 --> 00:46:12.019
Inderjit Dhillon: VR plus one through. Vn, okay?


413
00:46:12.529 --> 00:46:20.809
Inderjit Dhillon: And then we have a transpose which maps RM, 2 or m.


414
00:46:21.419 --> 00:46:29.449
Inderjit Dhillon: and I have that u, 1 is mapped to v. 1 sigma, one


415
00:46:30.469 --> 00:46:34.278
Inderjit Dhillon: u, 2 is mapped to v. 2 sigma, 2


416
00:46:35.779 --> 00:46:40.339
Inderjit Dhillon: UR is mapped to the R. Sigma r


417
00:46:41.189 --> 00:46:45.169
Inderjit Dhillon: and then UR plus one is mapped to 0,


418
00:46:45.349 --> 00:46:51.596
Inderjit Dhillon: and we are assuming that m is greater than equal to N, for now so UR plus one through


419
00:46:52.399 --> 00:46:53.888
Inderjit Dhillon: is mapped to 0.


420
00:46:55.689 --> 00:47:07.579
Inderjit Dhillon: So the beautiful thing that the Svd does okay it is. It provides a very special provides.


421
00:47:07.819 --> 00:47:10.208
Inderjit Dhillon: Sorry my R is a little funky.


422
00:47:10.539 --> 00:47:13.149
Inderjit Dhillon: It provides an orthogonal basis


423
00:47:17.919 --> 00:47:24.529
Inderjit Dhillon: for the 4 fundamental


424
00:47:28.289 --> 00:47:33.258
Inderjit Dhillon: subspaces of A.


425
00:47:34.319 --> 00:47:42.798
Inderjit Dhillon: And what are the 4 so fundamental subspaces? And some of you might have seen this in your in your linear algebra class. It's the


426
00:47:43.119 --> 00:47:52.669
Inderjit Dhillon: column space of the matrix, A, which is often denoted as the range space of a.


427
00:47:53.549 --> 00:47:57.199
Inderjit Dhillon: the other is the rule space of the matrix.


428
00:47:57.339 --> 00:48:05.988
Inderjit Dhillon: and you probably saw right that the row rank in in your in your undergraduate linear algebra course, that the row rank is the same as the column rank


429
00:48:06.209 --> 00:48:15.339
Inderjit Dhillon: right? And it just falls out of the Svd. Right? So it's the I can write the range space of a transpose.


430
00:48:15.969 --> 00:48:18.188
Inderjit Dhillon: then I have the null space.


431
00:48:20.789 --> 00:48:28.889
Inderjit Dhillon: which is null space of a I have no space off.


432
00:48:29.059 --> 00:48:35.538
Inderjit Dhillon: A monthly transpose is equal to null space of a transpits.


433
00:48:36.369 --> 00:48:42.129
Inderjit Dhillon: I didn't need to get all fancy over here. So, okay.


434
00:48:42.779 --> 00:48:47.359
Inderjit Dhillon: and this is the the basis of this space


435
00:48:47.869 --> 00:48:51.929
Inderjit Dhillon: is u, 1 u, 2 through UR.


436
00:48:53.889 --> 00:49:00.819
Inderjit Dhillon: The basis of this is v. 1 v. 2, through VR, but since


437
00:49:01.489 --> 00:49:10.708
Inderjit Dhillon: the row, space and the column space have the same dimension, and that's the same as the rank of the matrix. There are R. Vectors in any orthogonal basis.


438
00:49:11.179 --> 00:49:21.198
Inderjit Dhillon: and then the null space is VR plus one VR plus 2 through Vn.


439
00:49:22.139 --> 00:49:26.909
Inderjit Dhillon: and then the null space of a transpose is UR plus one


440
00:49:27.479 --> 00:49:32.199
Inderjit Dhillon: UR plus 2. And since we are assuming that m is greater than equal to N,


441
00:49:32.539 --> 00:49:33.414
Inderjit Dhillon: I have.


442
00:49:37.609 --> 00:49:45.719
Inderjit Dhillon: Okay, so like, I said, the Svd is a really important matrix decomposition.


443
00:49:46.159 --> 00:49:51.969
Inderjit Dhillon: And many times it's known as the Rolls. Royce.


444
00:49:54.719 --> 00:49:59.489
Inderjit Dhillon: Okay, but it's also incredibly incredibly useful computationally.


445
00:49:59.759 --> 00:50:05.929
Inderjit Dhillon: So that's why it is called, as the Swiss Army Knife.


446
00:50:07.579 --> 00:50:17.128
Inderjit Dhillon: Very useful in many many different applications, including in machine learning.


447
00:50:28.359 --> 00:50:32.388
Inderjit Dhillon: Okay? And let me do. Let me give one more property of


448
00:50:33.182 --> 00:50:42.018
Inderjit Dhillon: the Svd. And then we'll move on to regression. Okay? And this again, like a very again, a very, very important property of an Svd.


449
00:50:42.299 --> 00:50:45.799
Inderjit Dhillon: So if a belongs to is an M. By N. Matrix.


450
00:50:46.979 --> 00:50:51.218
Inderjit Dhillon: we've seen that you know the truncated or thin. Svd.


451
00:50:55.839 --> 00:50:57.329
Inderjit Dhillon: okay, where?


452
00:50:58.140 --> 00:51:04.878
Inderjit Dhillon: And we let let's actually consider the truncated Svd, right? Which is, let K. Be any number.


453
00:51:05.089 --> 00:51:13.309
Inderjit Dhillon: Okay, for any K where K. Belongs to


454
00:51:16.559 --> 00:51:24.148
Inderjit Dhillon: Min, mn, okay, let me write A. K. The truncated Svd.


455
00:51:24.689 --> 00:51:29.079
Inderjit Dhillon: As A is equal to UK. Sigma K.


456
00:51:29.709 --> 00:51:31.399
Inderjit Dhillon: Vk transports.


457
00:51:32.919 --> 00:51:41.039
Inderjit Dhillon: Okay? And Uk is RM. By K.


458
00:51:42.549 --> 00:51:44.109
Inderjit Dhillon: It's orthogonal.


459
00:51:44.819 --> 00:51:46.509
Inderjit Dhillon: Alright. Okay.


460
00:51:48.699 --> 00:51:52.268
Inderjit Dhillon: Transpose Uk is my identity.


461
00:51:53.099 --> 00:52:04.208
Inderjit Dhillon: Okay, Sigma K is K, by K, okay, and Vk is


462
00:52:08.179 --> 00:52:09.428
Inderjit Dhillon: K, by M.


463
00:52:13.529 --> 00:52:20.898
Inderjit Dhillon: Vk, transpose. Vk, so all I've done is the following right? So all I've done is


464
00:52:24.869 --> 00:52:27.598
Inderjit Dhillon: I'm saying that you can take any cake.


465
00:52:28.749 --> 00:52:35.469
Inderjit Dhillon: Okay? And I can basically just take the 1st K columns or the 1st K left. Singular vectors.


466
00:52:35.799 --> 00:52:42.169
Inderjit Dhillon: The 1st case, singular values and crucial over here is that the singular values are decreasing.


467
00:52:43.139 --> 00:52:47.068
Inderjit Dhillon: and then I can take the 1st K rows of V transpose.


468
00:52:47.939 --> 00:52:55.499
Inderjit Dhillon: Remember that this will again give an M by N matrix, but it has 9 K by construction


469
00:52:55.839 --> 00:52:59.898
Inderjit Dhillon: as long as K is less than r, okay, so I should say that.


470
00:53:05.209 --> 00:53:06.219
Inderjit Dhillon: okay.


471
00:53:07.129 --> 00:53:13.219
Inderjit Dhillon: And here's the very important property. Okay, suppose I fix K,


472
00:53:14.189 --> 00:53:22.929
Inderjit Dhillon: so you can actually take, for example, K, equal to one or K equal to 2. Okay, rank of


473
00:53:23.679 --> 00:53:24.869
Inderjit Dhillon: a K.


474
00:53:25.339 --> 00:53:26.499
Inderjit Dhillon: Sk.


475
00:53:28.249 --> 00:53:31.058
Inderjit Dhillon: And the Svd. Has the property.


476
00:53:32.749 --> 00:53:35.668
Inderjit Dhillon: And remember, Ak is the sorry I I.


477
00:53:36.449 --> 00:53:38.239
Inderjit Dhillon: Ak, is this matrix.


478
00:53:41.029 --> 00:53:43.838
Inderjit Dhillon: So Ak is clearly of rank. K,


479
00:53:44.439 --> 00:53:46.909
Inderjit Dhillon: it's an M. By N, matrix.


480
00:53:48.389 --> 00:53:53.279
Inderjit Dhillon: Okay. But it's the special matrix, which is Uk sigma K vk, transpose.


481
00:53:53.669 --> 00:54:00.608
Inderjit Dhillon: And the theorem is that among all rank K approximations


482
00:54:06.169 --> 00:54:14.839
Inderjit Dhillon: of a a k is the best.


483
00:54:15.789 --> 00:54:17.679
Inderjit Dhillon: And what does it mean to be the best?


484
00:54:18.009 --> 00:54:24.949
Inderjit Dhillon: It means that if I look at the matrix A, remember, my given matrix is A,


485
00:54:25.739 --> 00:54:35.799
Inderjit Dhillon: if I look at all matrices B, which are rank K, so B is off rank.


486
00:54:36.919 --> 00:54:37.939
Inderjit Dhillon: Okay?


487
00:54:38.579 --> 00:54:45.768
Inderjit Dhillon: And if I try to take the minimizer among all rank K matrices.


488
00:54:46.309 --> 00:54:49.539
Inderjit Dhillon: So Argmen, this is equal to Ak.


489
00:54:50.399 --> 00:54:53.149
Inderjit Dhillon: and this is true in the 2 norm.


490
00:54:54.309 --> 00:54:58.608
Inderjit Dhillon: And similarly, it is true, in the flopineous mode.


491
00:55:13.569 --> 00:55:17.338
Inderjit Dhillon: Okay? And it just, it is true, for all care.


492
00:55:25.129 --> 00:55:31.159
Inderjit Dhillon: Okay, so which means that. And remember that Ak is equal to UK.


493
00:55:31.479 --> 00:55:34.209
Inderjit Dhillon: Sigma K, we get lots of


494
00:55:43.819 --> 00:55:51.829
Inderjit Dhillon: okay with. So with that, I kind of conclude talking about the singular value decomposition. But it's really a


495
00:55:52.559 --> 00:55:57.849
Inderjit Dhillon: an amazing decomposition among all matrix decompositions that exist


496
00:55:58.793 --> 00:56:02.329
Inderjit Dhillon: any questions. So far, I'm now going to move on to regression.


497
00:56:03.219 --> 00:56:03.769
Hormoz Shahrzad: Yes.


498
00:56:03.769 --> 00:56:06.049
Inderjit Dhillon: Seeing how the Svd. Can be.


499
00:56:06.939 --> 00:56:09.449
Inderjit Dhillon: You know how the Svd. Helps us over there.


500
00:56:09.789 --> 00:56:11.679
Hormoz Shahrzad: I have a question, if I may.


501
00:56:13.009 --> 00:56:22.969
Hormoz Shahrzad: So so those sigma values like the singular values like sigma 1, 2, sigma and.


502
00:56:23.959 --> 00:56:47.999
Hormoz Shahrzad: Okay? So when we do the the K rank so the you mentioned that these are like kind of sorted. So the signal is the is the greatest one and the biggest, and then all the way to Sigma. And so when the the K. Rank thing, so are the let's say we do.


503
00:56:48.339 --> 00:56:49.749
Hormoz Shahrzad: K. Equals 2.


504
00:56:49.939 --> 00:56:50.599
Inderjit Dhillon: Yes.


505
00:56:50.599 --> 00:56:56.718
Hormoz Shahrzad: Sigma one, and Sigma 2 are the the exact value of the the full decomposition.


506
00:56:57.099 --> 00:57:01.899
Inderjit Dhillon: Exactly so. They are exactly this Sigma one


507
00:57:02.139 --> 00:57:05.948
Inderjit Dhillon: and the Sigma tool. So basically, it's saying that I can


508
00:57:07.889 --> 00:57:10.828
Inderjit Dhillon: take this decomposition, which is the full Svd.


509
00:57:11.519 --> 00:57:14.089
Inderjit Dhillon: and I can basically just take any


510
00:57:15.329 --> 00:57:20.389
Inderjit Dhillon: K left. Singular vectors, any K, the top K


511
00:57:21.169 --> 00:57:25.129
Inderjit Dhillon: single values and the top K write singular vectors.


512
00:57:26.179 --> 00:57:29.219
Inderjit Dhillon: and if I multiply them out, UK.


513
00:57:29.339 --> 00:57:39.318
Inderjit Dhillon: Sigma K. Vk, transpose, then, among all possible ranky approximations of a this is the closest way.


514
00:57:40.569 --> 00:57:42.409
Hormoz Shahrzad: That's that's pretty powerful.


515
00:57:42.689 --> 00:57:45.028
Inderjit Dhillon: Yes, it's it's a beautiful property.


516
00:57:45.209 --> 00:57:48.679
Inderjit Dhillon: and somewhat surprising, if you know, if you


517
00:57:49.844 --> 00:57:55.839
Inderjit Dhillon: if you think about it, really, it's kind of like a if. I just look at this particular problem


518
00:57:55.959 --> 00:58:00.099
Inderjit Dhillon: over here, and I think about trying to solve it.


519
00:58:00.269 --> 00:58:02.139
Inderjit Dhillon: This particular problem.


520
00:58:02.389 --> 00:58:05.729
Inderjit Dhillon: It's actually not clear that there is a closed bank solution.


521
00:58:06.819 --> 00:58:12.978
Inderjit Dhillon: This problem is not really even convex, because rank is not a kind of convex function, but it's just


522
00:58:13.569 --> 00:58:16.859
Inderjit Dhillon: kind of the wonders of single valley decomposition


523
00:58:17.109 --> 00:58:20.878
Inderjit Dhillon: that have this nested property. It's basically a nested property.


524
00:58:22.029 --> 00:58:24.689
Hormoz Shahrzad: Yeah, yeah, it's just like we can


525
00:58:25.199 --> 00:58:28.858
Hormoz Shahrzad: some sort of partition. Let me fix the way you want, and then.


526
00:58:29.859 --> 00:58:31.588
Hormoz Shahrzad: Yeah, that's.


527
00:58:31.589 --> 00:58:32.189
Inderjit Dhillon: Yeah.


528
00:58:32.609 --> 00:58:33.469
Hormoz Shahrzad: They're beautiful.


529
00:58:33.819 --> 00:58:37.566
Inderjit Dhillon: Yeah, yeah, yeah. So I mean, I can't stop gushing about the


530
00:58:38.169 --> 00:58:54.658
Inderjit Dhillon: Svd, but it's really kind of a remarkable decomposition, and actually helps us in like, I said, it helps us in interpretation of many things. It helps us in also, computationally. So let's talk a little bit about. Let's go back to our regression problem.


531
00:58:58.889 --> 00:59:03.849
Inderjit Dhillon: And let's give an example of one way. It can help us in interpreting regression.


532
00:59:04.229 --> 00:59:07.649
Inderjit Dhillon: And then next lecture. I'll talk a little bit about


533
00:59:08.629 --> 00:59:20.699
Inderjit Dhillon: regularization, which actually is a very important component of linear regression, right? And we'll see how it again helps us in interpreting things like ridge regression. Okay.


534
00:59:20.949 --> 00:59:33.549
Inderjit Dhillon: So remember, regression is the problem. Let me go back now to where you know you have this training data. XI, Yi, and based on that, I can form the vector y.


535
00:59:34.189 --> 00:59:49.429
Inderjit Dhillon: I have the matrix X transpose, which is x is one x, 1, 1 x, 2, 1.


536
00:59:49.739 --> 00:59:52.658
Inderjit Dhillon: We were assuming that we have capital N


537
00:59:53.589 --> 00:59:59.769
Inderjit Dhillon: training data. So remember, we have. Xi, YI.


538
01:00:00.839 --> 01:00:08.958
Inderjit Dhillon: I. Is between one and capital. N. Xi belongs to R.


539
01:00:09.769 --> 01:00:14.508
Inderjit Dhillon: The NYI belongs to all.


540
01:00:15.389 --> 01:00:17.468
Inderjit Dhillon: So we can form the matrix X,


541
01:00:17.869 --> 01:00:27.089
Inderjit Dhillon: and the regression problem is that we want to find coefficients W or a linear model, so that we


542
01:00:27.479 --> 01:00:34.498
Inderjit Dhillon: find the best fit among all. W's to the training table.


543
01:00:35.039 --> 01:00:35.949
Inderjit Dhillon: Okay?


544
01:00:36.729 --> 01:00:41.929
Inderjit Dhillon: And in the very 1st lecture on regression, we saw that you know


545
01:00:42.299 --> 01:00:45.678
Inderjit Dhillon: this least squares objective has a solution.


546
01:00:50.819 --> 01:00:58.329
Inderjit Dhillon: which is that, and we derived it using 2 different ways, one geometric and one looking at the gradients.


547
01:00:58.829 --> 01:01:02.088
Inderjit Dhillon: XX transpose times. W. Star.


548
01:01:02.219 --> 01:01:05.689
Inderjit Dhillon: that's the optimal. That's the minimizer


549
01:01:05.839 --> 01:01:10.798
Inderjit Dhillon: of this is equal to X times, y.


550
01:01:12.618 --> 01:01:20.498
Inderjit Dhillon: so W. Star is the solution. And if W. Star, if the matrix Xx transpose is


551
01:01:21.979 --> 01:01:32.919
Inderjit Dhillon: single non singular. Then I have XX transpose inverse times. XY, okay.


552
01:01:34.299 --> 01:01:37.769
Inderjit Dhillon: And now, so remember that you know I have.


553
01:01:38.209 --> 01:01:41.439
Inderjit Dhillon: Xi YI.


554
01:01:42.869 --> 01:01:51.659
Inderjit Dhillon: Right from one to N, and if I now think about what


555
01:01:53.409 --> 01:01:56.049
Inderjit Dhillon: what is X transpose? W. Star.


556
01:02:00.499 --> 01:02:07.089
Inderjit Dhillon: it is a prediction, right? It's the prediction of.


557
01:02:08.099 --> 01:02:10.069
Inderjit Dhillon: So that's equal to Y hat.


558
01:02:11.609 --> 01:02:18.659
Inderjit Dhillon: Right? Because W. Star is now my coefficients, and


559
01:02:19.519 --> 01:02:23.339
Inderjit Dhillon: X transpose W. Star is the prediction.


560
01:02:24.849 --> 01:02:30.109
Inderjit Dhillon: so I can think of it as an approximation to yi, which is the actual value.


561
01:02:30.259 --> 01:02:38.259
Inderjit Dhillon: But let's see what happens right if I now I got that W. Star is this quantity.


562
01:02:39.199 --> 01:02:42.909
Inderjit Dhillon: and so I have, y, hat


563
01:02:43.339 --> 01:02:50.039
Inderjit Dhillon: is X, transpose XX transpose inverse times. XY,


564
01:02:53.579 --> 01:03:01.659
Inderjit Dhillon: okay, so this is the prediction on training data


565
01:03:05.629 --> 01:03:09.349
Inderjit Dhillon: training data set.


566
01:03:13.029 --> 01:03:18.019
Inderjit Dhillon: Okay, so this is the prediction on the training data set.


567
01:03:19.559 --> 01:03:24.698
Inderjit Dhillon: Let's see what this actually means. Right? And one very, we'll see that the Svd.


568
01:03:24.899 --> 01:03:31.248
Inderjit Dhillon: we'll actually give you a really good way of interpreting what the solution of regression is


569
01:03:31.549 --> 01:03:34.859
Inderjit Dhillon: this quiz regression? Okay, so let


570
01:03:37.389 --> 01:03:43.228
Inderjit Dhillon: X transpose equal to U. Sigma V. Transpose, I'm going to drop the hats.


571
01:03:43.639 --> 01:03:53.249
Inderjit Dhillon: Okay, be the reduced Svd of X transpose.


572
01:03:54.959 --> 01:04:00.768
Inderjit Dhillon: Okay, what does that mean? Let me look at X. So if I look over here.


573
01:04:02.189 --> 01:04:11.639
Inderjit Dhillon: Here is my solution to the least squares problem. It's XX transpose inverse times. X. Transpose X. So let's use the Svd


574
01:04:12.629 --> 01:04:21.489
Inderjit Dhillon: 2, expand these quantities to look more detail, more deeper into these quantities. Right? So I have. Xx. Transpose.


575
01:04:22.719 --> 01:04:26.869
Inderjit Dhillon: then I have XX. Transpose inverse.


576
01:04:28.289 --> 01:04:31.449
Inderjit Dhillon: and then I have X transpose.


577
01:04:31.799 --> 01:04:34.928
Inderjit Dhillon: XX transpose inverse times. X,


578
01:04:35.379 --> 01:04:38.268
Inderjit Dhillon: okay, those are the different matrices that come up from here.


579
01:04:38.679 --> 01:04:50.269
Inderjit Dhillon: And let's use the Svd to analyze this. Okay, so what is Xx transpose is equal to V.


580
01:04:50.639 --> 01:04:53.058
Inderjit Dhillon: Sigma. Remember, Sigma is square.


581
01:04:55.019 --> 01:05:01.359
Inderjit Dhillon: U transpose times U. Sigma V transpose.


582
01:05:02.139 --> 01:05:05.809
Inderjit Dhillon: Okay, remember that I'm actually looking at. So just one subtlety that


583
01:05:05.949 --> 01:05:10.568
Inderjit Dhillon: I'm assuming that U. Sigma B transpose is the Svd of X transpose.


584
01:05:11.329 --> 01:05:17.748
Inderjit Dhillon: Okay, just to kind of keep the notation the same as before. Okay. But I think I used X transpose as the


585
01:05:18.879 --> 01:05:21.768
Inderjit Dhillon: you know, coefficients matrix over here.


586
01:05:21.959 --> 01:05:25.269
Inderjit Dhillon: So I'm taking the Svd on this matrix. Okay.


587
01:05:26.349 --> 01:05:33.749
Inderjit Dhillon: so now, U transpose U is the identity matrix. Remember, U is a column orthogonal matrix.


588
01:05:34.009 --> 01:05:38.229
Inderjit Dhillon: And so I have. V. Sigma square. V. Transpose.


589
01:05:41.559 --> 01:05:45.249
Inderjit Dhillon: And now I have. Xx transpose inverse


590
01:05:45.669 --> 01:05:51.808
Inderjit Dhillon: is v. Sigma square. V transpose from here.


591
01:05:52.759 --> 01:05:55.678
Inderjit Dhillon: Right? So I need to take the inverse of this matrix.


592
01:05:57.388 --> 01:06:01.138
Inderjit Dhillon: Okay? And remember that the product, the inverse of a product


593
01:06:01.449 --> 01:06:13.089
Inderjit Dhillon: is the product of the inverses multiplied in opposite order. So it becomes, V transpose inverse Sigma square


594
01:06:13.919 --> 01:06:16.578
Inderjit Dhillon: inverse. V, inverse.


595
01:06:17.549 --> 01:06:20.748
Inderjit Dhillon: Okay, I'm in the case where.


596
01:06:21.789 --> 01:06:24.728
Inderjit Dhillon: remember, V is a square matrix.


597
01:06:25.569 --> 01:06:30.888
Inderjit Dhillon: So this is equal to V sigma, minus 2.


598
01:06:31.109 --> 01:06:32.539
Inderjit Dhillon: We transpose.


599
01:06:33.768 --> 01:06:40.509
Inderjit Dhillon: Okay, because I have that V inverse is equal to V transpose.


600
01:06:40.729 --> 01:06:43.178
Inderjit Dhillon: Okay, right. V, inverse V equal to, I


601
01:06:43.279 --> 01:06:47.639
Inderjit Dhillon: v, transpose equal to I, so v inverse is equal to V transpose.


602
01:06:48.819 --> 01:06:54.919
Inderjit Dhillon: So I actually have a nice formula for inverse of Xx transpose.


603
01:06:56.529 --> 01:06:59.598
Inderjit Dhillon: Okay, now, if I plug that in right.


604
01:07:00.159 --> 01:07:02.359
Inderjit Dhillon: I can plug it over here.


605
01:07:02.979 --> 01:07:09.958
Inderjit Dhillon: and then I have X on either side. X on this side X transpose on this side. So let's put it all together.


606
01:07:10.739 --> 01:07:13.569
Inderjit Dhillon: Right? So I have.


607
01:07:14.259 --> 01:07:18.018
Inderjit Dhillon: X transpose is U. Sigma. V. Transpose.


608
01:07:19.689 --> 01:07:23.058
Inderjit Dhillon: then I have. Xx transpose inverse


609
01:07:23.849 --> 01:07:28.319
Inderjit Dhillon: is v. Sigma, minus 2 v. Transpose


610
01:07:30.459 --> 01:07:33.078
Inderjit Dhillon: from above same thing over here.


611
01:07:33.979 --> 01:07:37.858
Inderjit Dhillon: and then I have v. Sigma, U transpose.


612
01:07:38.759 --> 01:07:43.869
Inderjit Dhillon: Okay, so looks a little tedious, right? But look at what happens.


613
01:07:44.319 --> 01:07:46.679
Inderjit Dhillon: V transpose V is the identity


614
01:07:47.859 --> 01:07:50.379
Inderjit Dhillon: V transpose. V is the identity.


615
01:07:51.279 --> 01:07:58.499
Inderjit Dhillon: So I get U sigma sigma, minus 2 sigma U transpose.


616
01:08:00.509 --> 01:08:08.038
Inderjit Dhillon: Okay, this is the identity, because Sigma is a diagonal matrix. So Sigma


617
01:08:08.179 --> 01:08:11.859
Inderjit Dhillon: times Sigma minus 2 times. Sigma is identity.


618
01:08:12.429 --> 01:08:15.699
Inderjit Dhillon: And look at that, it becomes UU transpose


619
01:08:20.069 --> 01:08:22.319
Inderjit Dhillon: is UU transpose the identity.


620
01:08:23.529 --> 01:08:24.449
Inderjit Dhillon: No.


621
01:08:24.449 --> 01:08:24.974
Hormoz Shahrzad: Necessarily.


622
01:08:25.969 --> 01:08:35.079
Inderjit Dhillon: Because exactly, not necessarily, because, you know, in our case X is a rectangular matrix.


623
01:08:35.319 --> 01:08:42.589
Inderjit Dhillon: and when we looked at the thin or the reduced. Svd, U is also a rectangular matrix. Right? So in particular.


624
01:08:42.829 --> 01:08:52.629
Inderjit Dhillon: oh, remember that you know what is the dimension of XX is R.


625
01:08:52.929 --> 01:09:00.039
Inderjit Dhillon: D plus one by N, okay?


626
01:09:02.679 --> 01:09:16.268
Inderjit Dhillon: And then you is, you know, u 1, you, too, through UD plus one.


627
01:09:19.109 --> 01:09:24.039
Inderjit Dhillon: Okay, so U transpose U is the identity.


628
01:09:24.779 --> 01:09:28.759
Inderjit Dhillon: But UU transpose is not the identity.


629
01:09:29.919 --> 01:09:40.929
Inderjit Dhillon: And if you think about what UU transpose is right, UU transpose is u 1 u, 2


630
01:09:41.129 --> 01:09:51.718
Inderjit Dhillon: to UD. Plus one times u, 1 transpose u, 2 transpose UD plus one. Transpose


631
01:09:53.099 --> 01:09:59.458
Inderjit Dhillon: it's actually equal to u, 1 u, one transpose plus u, 2 u, 2 transpose


632
01:10:00.759 --> 01:10:04.608
Inderjit Dhillon: plus ud plus one ud plus one transport.


633
01:10:06.929 --> 01:10:07.829
Inderjit Dhillon: Okay?


634
01:10:08.069 --> 01:10:12.129
Inderjit Dhillon: And so you can look at this. And you, you realize


635
01:10:12.439 --> 01:10:21.509
Inderjit Dhillon: that UU transpose is essentially the orthogonal projector


636
01:10:24.629 --> 01:10:32.878
Inderjit Dhillon: onto the green space off expensive.


637
01:10:34.299 --> 01:10:36.888
Inderjit Dhillon: So that's all. What linear regression is doing.


638
01:10:38.209 --> 01:10:49.709
Inderjit Dhillon: Okay, what linear regression is doing is basically taking this vector y, and then doing this transformation.


639
01:10:50.859 --> 01:10:53.809
Inderjit Dhillon: UU transpose y, so basically taking.


640
01:10:55.299 --> 01:10:58.379
Inderjit Dhillon: So if I have, you know, the range space of


641
01:10:59.759 --> 01:11:02.278
Inderjit Dhillon: this is the range space of X transpose.


642
01:11:07.309 --> 01:11:13.778
Inderjit Dhillon: Okay, he's taking this vector Y which may not live in this range space.


643
01:11:13.879 --> 01:11:26.499
Inderjit Dhillon: which means that the training data, when we do linear regression on the training data. We won't get a 0 training set a 0 error on the training set. It's basically kind of dropping a perpendicular onto this.


644
01:11:27.049 --> 01:11:31.409
Inderjit Dhillon: And this is UU transpose X,


645
01:11:32.589 --> 01:11:35.008
Inderjit Dhillon: okay. And you can again see that this is


646
01:11:35.959 --> 01:11:41.769
Inderjit Dhillon: u, 1 u, one transpose plus u, 2 u, 2 transpose


647
01:11:42.649 --> 01:11:48.349
Inderjit Dhillon: plus ud plus one ud plus one transpose times. Y


648
01:11:49.139 --> 01:11:52.068
Inderjit Dhillon: right. And I can write this as


649
01:11:52.489 --> 01:11:57.398
Inderjit Dhillon: u, 1 u, one transpose y plus u, 2


650
01:11:57.739 --> 01:12:04.059
Inderjit Dhillon: u. 2, transpose y, and remember, all these uis are perpendicular to each other.


651
01:12:06.399 --> 01:12:12.669
Inderjit Dhillon: So it's basically like projecting onto each of the vectors right.


652
01:12:12.819 --> 01:12:16.948
Inderjit Dhillon: and which is equivalent to projecting onto the


653
01:12:17.089 --> 01:12:21.149
Inderjit Dhillon: column space off or range space of X transpose.


654
01:12:24.739 --> 01:12:30.318
Inderjit Dhillon: Okay, so we've seen now. And that's actually how we did the the.


655
01:12:30.629 --> 01:12:40.759
Inderjit Dhillon: if you remember, that's how we exactly did the geometric derivation of least squares regression. And now.


656
01:12:40.959 --> 01:12:43.998
Inderjit Dhillon: algebraically, we've kind of also verified.


657
01:12:44.199 --> 01:12:51.578
Inderjit Dhillon: So sometimes, like, you know, beautiful part of doing work in this area is


658
01:12:52.169 --> 01:12:54.839
Inderjit Dhillon: that you might actually have some formulaic.


659
01:12:55.369 --> 01:13:01.859
Inderjit Dhillon: but many times you might be able to not always might be able to give like a geometric


660
01:13:02.009 --> 01:13:07.049
Inderjit Dhillon: intuition or interpretation behind what you're doing, and least quiz


661
01:13:07.319 --> 01:13:11.958
Inderjit Dhillon: regression is one case where it kind of comes very beautifully together.


662
01:13:13.249 --> 01:13:20.158
Inderjit Dhillon: Okay, so that concludes this lecture any questions.


663
01:13:26.539 --> 01:13:30.387
Inderjit Dhillon: So next time what we will do is we will look at


664
01:13:32.539 --> 01:13:38.878
Inderjit Dhillon: The ways in which the current formulation we have of linear regression might be inadequate.


665
01:13:39.349 --> 01:13:48.099
Inderjit Dhillon: and we will look at a couple of ways of regularizing so that our predictions actually become


666
01:13:48.219 --> 01:13:54.749
Inderjit Dhillon: better. They don't overfit, and 2 popular ways are Ridge regression and


667
01:13:54.929 --> 01:14:03.139
Inderjit Dhillon: something called the lasso. So we look at these couple of different ways of


668
01:14:03.569 --> 01:14:07.718
Inderjit Dhillon: doing regularization, and that will bring us


669
01:14:08.481 --> 01:14:11.139
Inderjit Dhillon: close to the end of looking at regression.


670
01:14:11.449 --> 01:14:17.378
Inderjit Dhillon: After that I will look at, you know, a popular kind of collaborative filtering problem.


671
01:14:17.509 --> 01:14:19.629
Inderjit Dhillon: We'll also give you a homework.


672
01:14:19.959 --> 01:14:29.239
Inderjit Dhillon: so I think you can expect your 1st homework to come to you on. I think next Monday we'll assign the 1st homework


673
01:14:29.749 --> 01:14:38.479
Inderjit Dhillon: any questions, if not. Let's conclude, and I will see you all on Wednesday.


674
01:14:39.029 --> 01:14:40.338
Inderjit Dhillon: Okay, thank you.


675
01:14:40.599 --> 01:14:41.239
Hormoz Shahrzad: You.


676
01:14:41.609 --> 01:14:42.549
Chloe Chen: Thank you.



---- END OF LECTURE -------- START OF LECTURE 5 ----
WEBVTT

1
00:00:00.770 --> 00:00:04.979
Inderjit Dhillon: Okay, so we'll continue. I think today will be the


2
00:00:05.200 --> 00:00:08.019
Inderjit Dhillon: last lecture that we will have on regression.


3
00:00:08.580 --> 00:00:09.620
Inderjit Dhillon: So


4
00:00:11.450 --> 00:00:15.830
Inderjit Dhillon: So last time, what we did was, we


5
00:00:17.010 --> 00:00:29.360
Inderjit Dhillon: continue kind of with the review of linear algebra, and really concentrated on, you know, the Svd. So Svd is a very important decomposition. It pretty much tells you all there is to know


6
00:00:29.690 --> 00:00:37.870
Inderjit Dhillon: about a particular matrix. It exists for any matrix, and it is this decomposition U. Sigma V transpose.


7
00:00:38.030 --> 00:00:45.070
Inderjit Dhillon: And we talked about the full Svd. And also something called either the thin or the reduced Svd.


8
00:00:45.270 --> 00:00:52.119
Inderjit Dhillon: But also, you know, you can actually truncate the Spd at any K. So you can.


9
00:00:52.300 --> 00:00:57.819
Inderjit Dhillon: You know if the full Svd is U. Sigma V transpose as given over here


10
00:00:58.020 --> 00:01:04.720
Inderjit Dhillon: right the thin. The the thin svd sometimes called this u hat.


11
00:01:04.830 --> 00:01:15.619
Inderjit Dhillon: which takes where m is greater than equal to n, and you take the 1st n singular vectors u hat sigma hat v hat transpose, but you can actually truncate it at any k.


12
00:01:15.760 --> 00:01:34.740
Inderjit Dhillon: so you can just take u 1 u 2, for example, or u, 1 u, 2 U. 3, and then take the corresponding singular values. They are, of course, ordered in decreasing order, and then also the corresponding V's, and that is sometimes called the K truncated Svd, and one of the


13
00:01:35.450 --> 00:01:47.060
Inderjit Dhillon: really nice properties of the Svd. Is that actually, that gives the best rank, a approximation. To the given matrix. Right? So, for example, here.


14
00:01:47.170 --> 00:02:00.360
Inderjit Dhillon: you know, if you have a given matrix, A, and you want to find out the best rankier approximation of this matrix. Then you can basically just take, you know, Uk, sigma K. Vk, transpose


15
00:02:00.510 --> 00:02:05.910
Inderjit Dhillon: where Uk is n, by M. By K.


16
00:02:06.652 --> 00:02:08.830
Inderjit Dhillon: Maybe I should actually write it over here.


17
00:02:09.380 --> 00:02:12.040
Inderjit Dhillon: Sigma UUK.


18
00:02:12.190 --> 00:02:15.950
Inderjit Dhillon: Belongs to our M. By K.


19
00:02:16.640 --> 00:02:21.199
Inderjit Dhillon: Sigma K. Belongs to our K. By K.


20
00:02:21.820 --> 00:02:26.669
Inderjit Dhillon: And Vk belongs to or invite you.


21
00:02:27.634 --> 00:02:32.679
Inderjit Dhillon: Okay. And that gives you the the the best. Us.


22
00:02:32.870 --> 00:02:38.470
Inderjit Dhillon: Singular value. Best truncated best rank or ranky approximation. Okay.


23
00:02:39.020 --> 00:02:56.490
Inderjit Dhillon: then we went back to least squares regression, and we saw that least squares regression. If you analyze it using the Svd, it's actually just the orthogonal projector onto the range space of the design matrix, which is X transpose right, which is made of the training samples.


24
00:02:57.111 --> 00:03:02.240
Inderjit Dhillon: So today we will kind of talk a little bit more about regression.


25
00:03:02.360 --> 00:03:06.333
Inderjit Dhillon: Some of the issues that there are with


26
00:03:06.970 --> 00:03:14.080
Inderjit Dhillon: just doing regression as we've talked about it. For example, e squares regression and talk about


27
00:03:14.220 --> 00:03:25.040
Inderjit Dhillon: something that is really needed to make the regression better, which is regularization in regression.


28
00:03:27.970 --> 00:03:33.309
Inderjit Dhillon: And there we look at a particular way to regularize, which makes the prediction better.


29
00:03:33.740 --> 00:03:37.730
Inderjit Dhillon: and we will see that oh.


30
00:03:37.840 --> 00:03:42.289
Inderjit Dhillon: we can actually analyze, this method called Ridge regression


31
00:03:42.560 --> 00:03:48.430
Inderjit Dhillon: quite well by looking at the singular value decomposition.


32
00:03:49.070 --> 00:03:59.930
Inderjit Dhillon: Okay, so reminder that regression is, you know, I have training data XIYI


33
00:04:00.490 --> 00:04:05.860
Inderjit Dhillon: Xi is d dimensional d features


34
00:04:06.290 --> 00:04:13.779
Inderjit Dhillon: the y's. Since we are talking about regression, these are real numbers, and linear regression


35
00:04:19.290 --> 00:04:23.420
Inderjit Dhillon: is basically taking a linear combination which is


36
00:04:23.810 --> 00:04:35.390
Inderjit Dhillon: W. Naught plus w. 1 x one, plus W. 2 x 2 plus WT.


37
00:04:40.830 --> 00:04:53.270
Inderjit Dhillon: Actually, alright, it's the prediction for new unknown. Excel.


38
00:04:53.480 --> 00:04:54.360
Inderjit Dhillon: Okay.


39
00:04:54.650 --> 00:05:00.030
Inderjit Dhillon: Now, even though I've called it linear regression. Sometimes you can kind of do


40
00:05:02.570 --> 00:05:11.439
Inderjit Dhillon: nonlinear regression using this scheme. So let's talk about a particular example which I will use to motivate the need for regularization.


41
00:05:11.830 --> 00:05:18.780
Inderjit Dhillon: So let's think about polynomial interpret interpolation or polynomial fitting.


42
00:05:24.570 --> 00:05:27.769
Inderjit Dhillon: Okay? And we'll talk about, you know, the simplest case which is


43
00:05:28.090 --> 00:05:32.519
Inderjit Dhillon: univariate. Polynomial fitting. Okay, what do I mean by that? Okay?


44
00:05:32.640 --> 00:05:37.449
Inderjit Dhillon: So in this case, what I will say is that you know, we actually have


45
00:05:37.830 --> 00:05:40.770
Inderjit Dhillon: just one single value of X,


46
00:05:41.570 --> 00:05:45.990
Inderjit Dhillon: okay, so my D is equal to one.


47
00:05:46.820 --> 00:05:52.849
Inderjit Dhillon: Okay, but what I will do is I will actually derive some nonlinear features from X,


48
00:05:53.280 --> 00:06:01.339
Inderjit Dhillon: so I will dig X, and I will map it onto something called fee of X.


49
00:06:02.110 --> 00:06:08.980
Inderjit Dhillon: Okay. So I'm going from, actually our d.


50
00:06:09.370 --> 00:06:12.040
Inderjit Dhillon: okay? And in this particular case it is


51
00:06:12.731 --> 00:06:16.430
Inderjit Dhillon: d equals one to R. Let's say, d prime.


52
00:06:17.150 --> 00:06:20.999
Inderjit Dhillon: So fee of X is of dimension.


53
00:06:24.740 --> 00:06:27.640
Inderjit Dhillon: The plan okay? And in particular.


54
00:06:27.960 --> 00:06:34.620
Inderjit Dhillon: the way we can think about polynomial fitting is to think about p. Of x as


55
00:06:36.420 --> 00:06:42.570
Inderjit Dhillon: the monomials, so XX square x cube


56
00:06:43.350 --> 00:06:45.290
Inderjit Dhillon: 2 x. To the power d.


57
00:06:48.100 --> 00:06:50.220
Inderjit Dhillon: or or extra the power app.


58
00:06:50.760 --> 00:06:53.030
Inderjit Dhillon: Let me not reuse.


59
00:06:57.260 --> 00:07:03.890
Inderjit Dhillon: Okay, I guess I've used d prime. So in this case, M is equal to d prime.


60
00:07:05.500 --> 00:07:08.549
Inderjit Dhillon: Okay, so if I want to do linear prediction.


61
00:07:09.820 --> 00:07:10.990
Inderjit Dhillon: So


62
00:07:14.330 --> 00:07:15.790
Inderjit Dhillon: polynomial


63
00:07:21.740 --> 00:07:28.089
Inderjit Dhillon: bit, add X, we'll see, is equivalent to


64
00:07:32.650 --> 00:07:36.600
Inderjit Dhillon: linear regression.


65
00:07:41.200 --> 00:07:43.809
Inderjit Dhillon: 4, 3 of us.


66
00:07:44.680 --> 00:07:48.879
Inderjit Dhillon: Okay, so let's think about what linear regression of E of X is


67
00:07:49.160 --> 00:07:52.330
Inderjit Dhillon: right. And let me call that as Z.


68
00:07:52.780 --> 00:07:56.029
Inderjit Dhillon: So if I say that this vector is Z,


69
00:07:56.580 --> 00:08:00.669
Inderjit Dhillon: okay, so remember, what is the near regression for a vector Z.


70
00:08:00.780 --> 00:08:15.430
Inderjit Dhillon: It is w naught plus w 1 z. One, those W. 2 Z, 2 plus w m.


71
00:08:15.610 --> 00:08:16.669
Inderjit Dhillon: ZM,


72
00:08:21.480 --> 00:08:27.330
Inderjit Dhillon: okay? And in this case, my Z, as I've said.


73
00:08:27.490 --> 00:08:37.759
Inderjit Dhillon: the components of it are the power. So the Ith value the ith component of this vector Z is actually X to the power.


74
00:08:38.270 --> 00:08:39.160
Inderjit Dhillon: I.


75
00:08:39.520 --> 00:08:42.790
Inderjit Dhillon: So I have w naught plus w. 1 x.


76
00:08:43.300 --> 00:08:50.119
Inderjit Dhillon: That's W. 2 x squared, plus Wm. XM.


77
00:08:50.430 --> 00:09:17.369
Inderjit Dhillon: And many of you might have seen this problem before. Right? So what you are trying to do is you're trying to fit a polynomial to a univate polynomial to a given data. So this is a technique that is many times, you know, you're you're using linear regression. But you're actually doing a nonlinear fit because the different powers of X are nonlinear in x. Of the features of X.


78
00:09:17.440 --> 00:09:24.540
Inderjit Dhillon: So this is not that uncommon, and you can. You can actually apply it even when X has dimension greater than one.


79
00:09:24.620 --> 00:09:27.760
Inderjit Dhillon: So you typically take, you know your given Rd.


80
00:09:28.010 --> 00:09:31.350
Inderjit Dhillon: and you can map it onto our


81
00:09:31.560 --> 00:09:35.060
Inderjit Dhillon: d prime. Typically, d, prime is


82
00:09:35.540 --> 00:09:51.459
Inderjit Dhillon: much, much greater than D. So, for example, here, M is much, much could be much greater than one right? And you basically have the way you we denote it is you take X and you do a map with a nonlinear map called P,


83
00:09:51.650 --> 00:09:53.550
Inderjit Dhillon: and then we'll do prediction.


84
00:09:57.810 --> 00:09:59.439
Inderjit Dhillon: using few effects.


85
00:10:02.250 --> 00:10:06.085
Inderjit Dhillon: Okay, so let me.


86
00:10:07.680 --> 00:10:12.080
Inderjit Dhillon: give you show you some examples. Okay of polynomial fitting.


87
00:10:13.220 --> 00:10:18.029
Inderjit Dhillon: And let me share my screen.


88
00:10:39.780 --> 00:10:46.629
Inderjit Dhillon: Okay, so you guys can see my screen. And so this is an example from this textbook by Chris Bishop.


89
00:10:47.220 --> 00:10:49.959
Inderjit Dhillon: Okay, hopefully, all of you can see it.


90
00:10:52.770 --> 00:10:59.480
Inderjit Dhillon: Okay. So you know, here's an example that you know, you might want to recognize digits at the classification problem.


91
00:10:59.670 --> 00:11:02.979
Inderjit Dhillon: But here is the problem that we are talking about. Right?


92
00:11:03.700 --> 00:11:07.629
Inderjit Dhillon: So here, think of, you know, a particular X,


93
00:11:08.700 --> 00:11:11.660
Inderjit Dhillon: so X is given over here


94
00:11:12.100 --> 00:11:17.129
Inderjit Dhillon: right? And this is the value that you want to fit the y-axis.


95
00:11:17.420 --> 00:11:19.949
Inderjit Dhillon: right? So you have basically X


96
00:11:20.300 --> 00:11:23.660
Inderjit Dhillon: comma, and then you have. You know, you have


97
00:11:23.910 --> 00:11:46.999
Inderjit Dhillon: the value. At x 1. You have a value at another point XX, 2. You have a value at another point x 3, and so on, right? But for a particular X. What you want to do is you want to fit a polynomial right? So Y in this case is a polynomial, and you can see that is exactly what we are talking about. And it is linear regression on Phi of X, where


98
00:11:47.501 --> 00:11:53.469
Inderjit Dhillon: the component, the Ith component of the vector Phi of X is X to the power. I


99
00:11:53.810 --> 00:11:55.509
Inderjit Dhillon: right? And over here


100
00:11:56.130 --> 00:12:07.869
Inderjit Dhillon: the in the green line is shown like the kind of fit that you would kind of think is natural for this data set right. Notice that this fit does not exactly pass through all these points.


101
00:12:08.130 --> 00:12:13.289
Inderjit Dhillon: Okay, but let's see what happens when you try to vary capital. M,


102
00:12:13.840 --> 00:12:21.839
Inderjit Dhillon: okay, so you, of course, you know, we can do least squares regression. This is just the sum of squares error function.


103
00:12:22.950 --> 00:12:26.650
Inderjit Dhillon: And now think about what a zeroth order polynomial is.


104
00:12:29.630 --> 00:12:34.560
Inderjit Dhillon: Well, a zeroth order polynomial says that I'm just fitting things by W. Naught.


105
00:12:36.090 --> 00:12:54.059
Inderjit Dhillon: Okay? And that's it. And you might say, Okay, what should the value of W. Not be? And you know, if you do the math it'll end up being the average of all the Y values. But it is just a this red line is the fit that you would get if you just take M. Equal to 0,


106
00:12:54.410 --> 00:13:04.719
Inderjit Dhillon: and you can right right away say that, hey? You know that's probably not a very good value of M to take right if my goal is to fit this set of points.


107
00:13:04.890 --> 00:13:08.500
Inderjit Dhillon: So then you can say, Okay, I can increase capital M,


108
00:13:09.140 --> 00:13:13.859
Inderjit Dhillon: and if I make capital M equal to one, then I get a straight line.


109
00:13:15.460 --> 00:13:27.409
Inderjit Dhillon: and this is the straight line you'll get if you fit it to this set of points. Now, how many points are there? 12345678910 points. Okay.


110
00:13:28.030 --> 00:13:29.360
Inderjit Dhillon: so you get that.


111
00:13:30.230 --> 00:13:34.730
Inderjit Dhillon: That again. Seems you know, it's better than a than a constant


112
00:13:35.228 --> 00:13:39.560
Inderjit Dhillon: but still you can visually see that it's not very good. So let's try to


113
00:13:39.660 --> 00:13:42.920
Inderjit Dhillon: increase M. And this time to M equal to 3.


114
00:13:43.890 --> 00:13:49.430
Inderjit Dhillon: And you can see that it's actually now kind of getting better. Right? So M. Equal to 3.


115
00:13:49.720 --> 00:13:54.359
Inderjit Dhillon: Is this value right now, from the theory of polynomials.


116
00:13:54.660 --> 00:13:59.279
Inderjit Dhillon: you know that a 3rd degree polynomial can have


117
00:13:59.410 --> 00:14:08.170
Inderjit Dhillon: kind of 3 roots, and you can see that you know there is one going to be one root or the value of the polynomial 0 here.


118
00:14:08.290 --> 00:14:11.050
Inderjit Dhillon: one here and then one over here.


119
00:14:11.730 --> 00:14:16.390
Inderjit Dhillon: Okay, so the polynomial will kind of oscillate right? Because there are these roots.


120
00:14:17.430 --> 00:14:23.570
Inderjit Dhillon: And then you can say, Okay, I can. Obviously the fit got better as I went from M equal to 0


121
00:14:26.220 --> 00:14:30.290
Inderjit Dhillon: to M, equal to one to M, equal to 3. Let's try to make it even better.


122
00:14:31.220 --> 00:14:34.849
Inderjit Dhillon: Let's try something well, much higher. 9th order, Polynom.


123
00:14:38.890 --> 00:14:40.589
Inderjit Dhillon: Oh, and look at this fit.


124
00:14:41.050 --> 00:14:45.439
Inderjit Dhillon: Now, this red line is the fit.


125
00:14:47.200 --> 00:14:51.699
Inderjit Dhillon: Now anybody wants to kind of say whether this is a good fit or not.


126
00:14:55.409 --> 00:14:56.509
Hormoz Shahrzad: That's terrible!


127
00:14:58.770 --> 00:15:00.889
Inderjit Dhillon: So remember, our goal is prediction.


128
00:15:02.130 --> 00:15:08.759
Inderjit Dhillon: Okay, if it was into so one of the things that you will notice. So the reason, I think, Hormone, you're saying it is terrible is that


129
00:15:08.940 --> 00:15:12.660
Inderjit Dhillon: you know it actually just oscillates very wildly right?


130
00:15:14.080 --> 00:15:19.660
Inderjit Dhillon: And but know that at all the points that were given in the training set.


131
00:15:21.120 --> 00:15:23.549
Inderjit Dhillon: it actually passes exactly through them.


132
00:15:25.360 --> 00:15:36.459
Inderjit Dhillon: Okay, but remember, our goal is not polynomial interpolation, which means to fit the polynomial exactly at that point some of you might have taken like a numerical analysis course.


133
00:15:36.530 --> 00:16:04.320
Inderjit Dhillon: and there you might actually have used polynomial interpolation. And clearly, if there are 10 points like, for example, if there are 2 points, you can fit them exactly by a line. If there are 3 points, you can fit them with a polynomial of degree. 2. And in general there's a theorem that says that if there are m plus one points, you can fit it with a polynomial of degree. M. That means you'll make 0 error.


134
00:16:04.830 --> 00:16:11.370
Inderjit Dhillon: But remember, that's not our goal here. Our goal is to be able to predict at a new point. Let's suppose an X over here.


135
00:16:11.690 --> 00:16:14.270
Inderjit Dhillon: It does fine over here, right?


136
00:16:15.060 --> 00:16:18.510
Inderjit Dhillon: Okay. And even if you do polynomial interpolation. By the way, this is not.


137
00:16:18.630 --> 00:16:24.890
Inderjit Dhillon: this is not good good. So people do other things like piecewise polynomials.


138
00:16:25.580 --> 00:16:36.380
Inderjit Dhillon: So what is happening over here? Okay, what is happening here? And this is the 1st time you're seeing this. And this is a big problem in machine learning where you are trying to overfit.


139
00:16:36.870 --> 00:16:40.539
Inderjit Dhillon: You're trying to fit too well to the training data.


140
00:16:41.100 --> 00:16:45.769
Inderjit Dhillon: But remember, our goal is what's actually called generalization.


141
00:16:46.080 --> 00:16:48.839
Inderjit Dhillon: Yes, you want to fit to the training data.


142
00:16:49.310 --> 00:16:53.920
Inderjit Dhillon: you know. And we want the training data to be representative of the final task


143
00:16:54.210 --> 00:17:04.770
Inderjit Dhillon: right? But the goal is to predict at some new point. Okay? So if you look at the so typically what you can do is, you can divide your data. So all you have, like a training set


144
00:17:05.220 --> 00:17:06.689
Inderjit Dhillon: and a test set


145
00:17:07.410 --> 00:17:15.889
Inderjit Dhillon: right, the training sets. What happens is in this place as you increase the degree of the polynomial, you can see that it decreases.


146
00:17:16.690 --> 00:17:29.299
Inderjit Dhillon: Okay, sorry. The training set is in is in blue, and you can see it as decreases. And, like, I said, a 9th degree polynomial will fit these 10 points exactly. So the training error actually decreases to 0.


147
00:17:30.230 --> 00:17:40.829
Inderjit Dhillon: Okay, but the test error, which is a new point when I try to predict on this the trailing error kind of declines. But then it shoots up.


148
00:17:41.290 --> 00:17:52.169
Inderjit Dhillon: and that's this phenomenon called overfitting. That means you've overfit too much to the training data. Your predictions will actually not be good for a new point or the test data.


149
00:17:52.870 --> 00:17:59.399
Inderjit Dhillon: Okay and like, why is it happening over here? If you look at the polynomial coefficients.


150
00:17:59.560 --> 00:18:11.100
Inderjit Dhillon: you'll see what might be happening. Right? So with M equal to 0. I had a coefficient of 0 point 1 9 right? And I'm guessing that that is the mean of all the Y values over here


151
00:18:13.230 --> 00:18:17.779
Inderjit Dhillon: with M equal to one. You see that you have coefficients of the order of one


152
00:18:18.140 --> 00:18:24.089
Inderjit Dhillon: M equal to 3. The coefficients are actually kind of slightly higher right? It's 25 and 17.


153
00:18:24.870 --> 00:18:31.900
Inderjit Dhillon: But look at what's happening with M equal to M equal to 9.


154
00:18:33.440 --> 00:18:36.679
Inderjit Dhillon: The coefficients are actually becoming really big.


155
00:18:38.080 --> 00:18:57.939
Inderjit Dhillon: and the reason they are becoming really big is what's happening is that some values become very large and some values become very small. So that's why you'll see that. You know, you have very large positive values, very large negative values. And they somehow cancel to give you 0 error at the training data on the training data.


156
00:18:58.390 --> 00:19:03.699
Inderjit Dhillon: Okay? So one symptom of overfitting is that my


157
00:19:04.500 --> 00:19:08.790
Inderjit Dhillon: coefficients, right, are actually getting very large.


158
00:19:09.480 --> 00:19:14.670
Inderjit Dhillon: You can think of that as that. Your your predictor


159
00:19:14.970 --> 00:19:20.340
Inderjit Dhillon: is actually getting a little bit more complex right where you know what?


160
00:19:20.740 --> 00:19:27.159
Inderjit Dhillon: And that's 1 in in this case it manifests by these very, very large values. Okay?


161
00:19:28.220 --> 00:19:31.330
Inderjit Dhillon: So what do we do then?


162
00:19:33.350 --> 00:19:35.610
Inderjit Dhillon: What should we try to do? Right? Well.


163
00:19:36.140 --> 00:19:42.070
Inderjit Dhillon: remember that so far, you know, we talked about the error on the training data.


164
00:19:43.300 --> 00:19:50.909
Inderjit Dhillon: This was the error on the training data, and we are looking at least squares regression. So we take the squared error and we are trying to decrease it.


165
00:19:51.460 --> 00:19:54.650
Inderjit Dhillon: But we've seen that just decreasing. It


166
00:19:54.940 --> 00:19:58.759
Inderjit Dhillon: isn't the best idea, because you can overfit.


167
00:20:00.310 --> 00:20:07.840
Inderjit Dhillon: Okay, the model is becoming too complicated, and it's trying to overfit simple model is better


168
00:20:08.290 --> 00:20:13.700
Inderjit Dhillon: in this case, what happens is that this manifests in the coefficients becoming too large.


169
00:20:14.740 --> 00:20:24.814
Inderjit Dhillon: So then, what you can then do is you can say, Okay, I will not. My goal is not to just fit the training data. Right? I'm actually going to


170
00:20:25.990 --> 00:20:28.059
Inderjit Dhillon: do something called regularization.


171
00:20:28.950 --> 00:20:35.600
Inderjit Dhillon: which means that I'm actually going to penalize my coefficient 2 large values of my coefficients.


172
00:20:36.350 --> 00:20:46.130
Inderjit Dhillon: So this is called this particular regularization. Where this is the 2 norm square is the bridge regularization.


173
00:20:46.300 --> 00:20:51.420
Inderjit Dhillon: And lambda is a parameter. That kind of trades off between the fit


174
00:20:51.560 --> 00:20:55.450
Inderjit Dhillon: to the training data and the complexity of the model.


175
00:20:55.940 --> 00:21:06.649
Inderjit Dhillon: Okay. So for example, if lambda equal to 0, we'll be back to our previous case, where we are just fitting to the training data. And we might get too complicated a model we might overfit


176
00:21:07.490 --> 00:21:13.060
Inderjit Dhillon: on the other extreme. If we take Lambda close to, let's say very large.


177
00:21:13.480 --> 00:21:14.810
Inderjit Dhillon: So in the limit.


178
00:21:15.050 --> 00:21:31.250
Inderjit Dhillon: we take it to infinity. Right? Our W will be, you know, in some sense constant or even 0 right? Because if I take this to infinity, what I will get is that you know, W should be actually all zeros, and that's not a good predictor.


179
00:21:31.500 --> 00:21:35.322
Inderjit Dhillon: So typically you want lambda to be


180
00:21:37.220 --> 00:21:41.440
Inderjit Dhillon: to be good and ideally. What you want is that


181
00:21:42.920 --> 00:21:48.359
Inderjit Dhillon: you know, over here, you know, we had this M. If you kind of look at


182
00:21:48.520 --> 00:22:00.150
Inderjit Dhillon: a value of lambda over here. Typically, that's what will happen that when you have 2 larger model it'll be too large a lambda. It'll be a very simple model.


183
00:22:00.370 --> 00:22:05.659
Inderjit Dhillon: Your training as well as your error, will be high test error


184
00:22:05.850 --> 00:22:18.320
Inderjit Dhillon: as you then start decreasing. Lambda. Your test error will keep on declining, but then, as you make your lambda close to 0, you will actually overfit, and your test error will start increasing.


185
00:22:22.010 --> 00:22:26.350
Inderjit Dhillon: Okay, so let me now stop sharing the slides and move back to.


186
00:22:27.190 --> 00:22:28.450
Inderjit Dhillon: Oh.


187
00:22:30.060 --> 00:22:35.498
Inderjit Dhillon: I see. So I sorry. I'm just looking at the at the chat, and I think, prachi, you kind of


188
00:22:37.082 --> 00:22:42.279
Inderjit Dhillon: foresaw what I was going to say, and yes, this is struggling from overfitting.


189
00:22:43.190 --> 00:22:45.149
Inderjit Dhillon: Okay, so let me go back.


190
00:22:45.660 --> 00:22:46.970
Inderjit Dhillon: Do my


191
00:23:03.140 --> 00:23:03.960
Inderjit Dhillon: version.


192
00:23:06.610 --> 00:23:11.270
Inderjit Dhillon: Okay? So now, what we will do is we will do Ridge refresh.


193
00:23:11.570 --> 00:23:13.150
Inderjit Dhillon: Okay, so let me.


194
00:23:13.630 --> 00:23:19.909
Inderjit Dhillon: I'm going to contrast it with just the linear regression that we did. So. Let me talk a little bit about Ridge regression over here.


195
00:23:21.220 --> 00:23:24.459
Inderjit Dhillon: So let me write it over here. Rich regression.


196
00:23:30.840 --> 00:23:35.900
Inderjit Dhillon: Okay? So the goal in red regression is to minimize


197
00:23:36.900 --> 00:23:42.190
Inderjit Dhillon: a combination of fit and regularizer. Okay, so remember the fit.


198
00:23:42.570 --> 00:23:45.149
Inderjit Dhillon: Since we are still talking about regularization.


199
00:23:45.490 --> 00:23:48.410
Inderjit Dhillon: we sorry we're still talking about regression.


200
00:23:48.880 --> 00:23:56.490
Inderjit Dhillon: And these squares regression. The fittest measure would like this. And then I have regularization, which is lambda


201
00:23:57.320 --> 00:24:03.909
Inderjit Dhillon: W. 2 norm square square. So it's the sum of the squares of the coefficients.


202
00:24:04.060 --> 00:24:11.239
Inderjit Dhillon: And just to contrast things, draw a line over here.


203
00:24:11.600 --> 00:24:14.380
Inderjit Dhillon: right? Our regular, least squares regression


204
00:24:18.170 --> 00:24:20.770
Inderjit Dhillon: which we have examined so far.


205
00:24:23.670 --> 00:24:32.349
Inderjit Dhillon: Well, just minimum of XX transpose W minus y to null square.


206
00:24:33.200 --> 00:24:38.189
Inderjit Dhillon: Okay? And remember that in this case, what was our solution?


207
00:24:40.360 --> 00:24:47.280
Inderjit Dhillon: The solution was, you know, the normal equations. You can take the gradients, or you can do the geometric analysis.


208
00:24:47.390 --> 00:24:52.160
Inderjit Dhillon: which is Xx transpose. W. Star is equal to X times y.


209
00:24:52.960 --> 00:25:04.079
Inderjit Dhillon: Right? So it's equal to W star, which are the coefficients being equal to XX transpose inverse times.


210
00:25:05.200 --> 00:25:06.380
Inderjit Dhillon: XY.


211
00:25:08.700 --> 00:25:09.640
Inderjit Dhillon: Okay.


212
00:25:09.800 --> 00:25:11.780
Inderjit Dhillon: Well, what is the solution over here?


213
00:25:18.800 --> 00:25:26.800
Inderjit Dhillon: Okay, well, I'll leave it as an exercise. You can again mimic what I did for least squares regression. And it's not not really complicated at all


214
00:25:27.020 --> 00:25:30.939
Inderjit Dhillon: right. And what you get is, if you take the gradient


215
00:25:31.280 --> 00:25:35.479
Inderjit Dhillon: and you set it equal to 0, you'll see that the resulting


216
00:25:36.480 --> 00:25:44.560
Inderjit Dhillon: equation that you will get is basically very similar, except lambda makes an occurrence.


217
00:25:45.230 --> 00:25:47.760
Inderjit Dhillon: and lambda makes an occurrence in this way.


218
00:25:48.430 --> 00:25:54.039
Inderjit Dhillon: Xx transpose plus lambda. IW. Star is equal to XY.


219
00:25:56.150 --> 00:26:02.269
Inderjit Dhillon: Okay, so you can see that, you know there is seemingly not much difference.


220
00:26:02.840 --> 00:26:06.340
Inderjit Dhillon: There's Xx transpose here. There's Xx transpose here.


221
00:26:07.040 --> 00:26:13.550
Inderjit Dhillon: But here there's a lambda I, and as a result, my optimal coefficients.


222
00:26:15.140 --> 00:26:23.169
Inderjit Dhillon: RXX transpose plus lambda. I inverse times. XY.


223
00:26:26.770 --> 00:26:32.779
Inderjit Dhillon: And if you think about it right, what's happening over here is that earlier, my


224
00:26:33.190 --> 00:26:39.099
Inderjit Dhillon: this inverse value. Remember, as I was going from M equal to 0 1 to 9,


225
00:26:39.790 --> 00:26:42.780
Inderjit Dhillon: you know, these values were actually becoming very big.


226
00:26:43.800 --> 00:26:48.440
Inderjit Dhillon: So W. Star was getting very large.


227
00:26:48.590 --> 00:26:52.189
Inderjit Dhillon: And what ends up happening is that when you do lambda


228
00:26:52.880 --> 00:26:58.720
Inderjit Dhillon: right, it dampens things and you're taking the inverse of not Xx transpose.


229
00:27:01.080 --> 00:27:06.830
Inderjit Dhillon: But Xx transpose plus lambda. I any questions.


230
00:27:17.560 --> 00:27:26.722
Inderjit Dhillon: By the way, if they if there is a question that comes up on the chat because many times I don't pay attention to the chat while I'm lecturing somebody can. Just


231
00:27:27.250 --> 00:27:29.640
Inderjit Dhillon: you know. Just say, Hey, there's a question on the chat.


232
00:27:31.320 --> 00:27:38.649
Inderjit Dhillon: Okay, there's no question right now. So let's let's look at now how these compare. Okay? So now.


233
00:27:38.880 --> 00:27:48.290
Inderjit Dhillon: the in least squares regression. We saw last time that prediction on training data.


234
00:27:52.560 --> 00:27:59.400
Inderjit Dhillon: Okay? So remember, the prediction on training data is always going to be X transpose W. Star.


235
00:28:00.670 --> 00:28:06.649
Inderjit Dhillon: okay? And that's just going to be X transpose times W. Star is given over here.


236
00:28:06.870 --> 00:28:12.880
Inderjit Dhillon: So it's XX transpose inverse XY.


237
00:28:16.410 --> 00:28:23.279
Inderjit Dhillon: Similarly, over here, the prediction, long training data


238
00:28:27.740 --> 00:28:32.320
Inderjit Dhillon: is, X transpose. Sorry


239
00:28:35.540 --> 00:28:37.960
Inderjit Dhillon: X transpose W. Star.


240
00:28:38.840 --> 00:28:45.960
Inderjit Dhillon: and now I have x transpose, and my W. Star is XX transpose plus lambda. I


241
00:28:46.820 --> 00:28:48.590
Inderjit Dhillon: inverse times. XY.


242
00:28:52.900 --> 00:28:54.860
Inderjit Dhillon: So very similar right.


243
00:28:55.020 --> 00:29:03.880
Inderjit Dhillon: But now let's see how the singular value decomposition can actually help us in analyzing this.


244
00:29:04.280 --> 00:29:10.090
Inderjit Dhillon: Okay, not talking about competition, but just to analyze what is going on.


245
00:29:10.550 --> 00:29:15.990
Inderjit Dhillon: So remember, we did this last time also. So we did. X transpose.


246
00:29:18.220 --> 00:29:24.869
Inderjit Dhillon: So let's be, you know. Let's look at the Svd of X transpose.


247
00:29:26.200 --> 00:29:32.230
Inderjit Dhillon: Okay, then, if you do, XX transpose is equal to


248
00:29:37.430 --> 00:29:46.340
Inderjit Dhillon: we sigma u transpose u sigma retransports.


249
00:29:46.750 --> 00:29:50.570
Inderjit Dhillon: and this is equal to V Sigma squared. V. Transpose.


250
00:29:51.120 --> 00:30:06.909
Inderjit Dhillon: And if I look at Xx, transpose inverse, that's going to be we Sigma minus 2 return space.


251
00:30:07.860 --> 00:30:11.539
Inderjit Dhillon: Okay? And then, when I want to look at the


252
00:30:11.750 --> 00:30:23.859
Inderjit Dhillon: prediction which is X transpose W. Star, which is X Xx. Transpose inverse X transpose W. Star


253
00:30:24.360 --> 00:30:26.320
Inderjit Dhillon: that is going to be.


254
00:30:28.320 --> 00:30:29.689
Inderjit Dhillon: What is that? Gonna be


255
00:30:34.120 --> 00:30:40.039
Inderjit Dhillon: sorry. I think I may have. Let's see, sometimes I get hooked up between the transposes.


256
00:30:40.660 --> 00:30:46.980
Inderjit Dhillon: So it is X transpose and X over here.


257
00:30:47.150 --> 00:30:51.539
Inderjit Dhillon: Okay, so it's U. Sigma V transpose.


258
00:30:52.670 --> 00:30:54.860
Inderjit Dhillon: And then from here


259
00:30:56.340 --> 00:31:02.319
Inderjit Dhillon: I have v sigma, minus 2 V transpose. Remember, I'm actually just repeating things from last time.


260
00:31:03.490 --> 00:31:11.440
Inderjit Dhillon: Okay? And then I have v sigma u transverse right? And what happens is


261
00:31:11.550 --> 00:31:15.970
Inderjit Dhillon: but this gives me identity. This gives me identity.


262
00:31:16.140 --> 00:31:21.879
Inderjit Dhillon: So I get u sigma sigma minus 2 Sigma U transpose.


263
00:31:22.110 --> 00:31:24.659
Inderjit Dhillon: And that's equal to UU. Transpace.


264
00:31:25.920 --> 00:31:30.162
Inderjit Dhillon: Okay with you. You transpose times


265
00:31:31.210 --> 00:31:33.829
Inderjit Dhillon: So again, I made a typo over here.


266
00:31:34.470 --> 00:31:35.940
Inderjit Dhillon: This was


267
00:31:40.390 --> 00:31:41.700
Inderjit Dhillon: while I think.


268
00:31:46.270 --> 00:31:48.300
Inderjit Dhillon: okay, so it's taking your


269
00:31:49.170 --> 00:31:54.059
Inderjit Dhillon: the given data, the given training data, you form A vector and then you project it


270
00:31:54.269 --> 00:31:58.889
Inderjit Dhillon: onto the space, the rain space of


271
00:31:59.019 --> 00:32:02.189
Inderjit Dhillon: X transpose. Okay, so this is.


272
00:32:02.539 --> 00:32:12.260
Inderjit Dhillon: I can also write this as summation of Ui ui transpose. Y, okay, right? Okay, 1, 2,


273
00:32:12.470 --> 00:32:20.580
Inderjit Dhillon: t plus one where these dimensions are okay. And this is orthogonal


274
00:32:23.959 --> 00:32:35.800
Inderjit Dhillon: production, 1, 2, range space, 1 5 0.


275
00:32:37.810 --> 00:32:41.010
Inderjit Dhillon: Let's look at what happens in the case of reintegration.


276
00:32:42.900 --> 00:32:48.130
Inderjit Dhillon: Okay, so let's do something similar. Right? So again.


277
00:32:48.760 --> 00:32:54.680
Inderjit Dhillon: my X transpose is equal to U. Sigma V transpose


278
00:32:55.270 --> 00:32:58.860
Inderjit Dhillon: same thing as before. Again, I have


279
00:33:00.730 --> 00:33:06.790
Inderjit Dhillon: now remember that in this time I have, I have Xx transpose equal to V.


280
00:33:11.010 --> 00:33:20.180
Inderjit Dhillon: Sigma Square, v. Transpose, and what is Xx transpose plus lambda I


281
00:33:20.960 --> 00:33:24.169
Inderjit Dhillon: so earlier I had Xx transpose inverse.


282
00:33:24.690 --> 00:33:25.630
Inderjit Dhillon: Okay?


283
00:33:26.240 --> 00:33:32.170
Inderjit Dhillon: But now I actually have X transpose Xx transpose plus lambda items.


284
00:33:35.270 --> 00:33:36.190
Inderjit Dhillon: Okay?


285
00:33:36.650 --> 00:33:48.060
Inderjit Dhillon: So well, I can actually write this as v times, sigma squared plus lambda. I we transpose.


286
00:33:48.750 --> 00:33:52.379
Inderjit Dhillon: And that's because V is n, by n, it is square.


287
00:33:52.620 --> 00:33:56.130
Inderjit Dhillon: right? And that gives me the identity.


288
00:33:59.320 --> 00:34:01.530
Inderjit Dhillon: So I can write lambda insert.


289
00:34:03.600 --> 00:34:04.470
Inderjit Dhillon: Okay?


290
00:34:04.660 --> 00:34:11.940
Inderjit Dhillon: Then I look at X transpose W. Star, and I get X transpose


291
00:34:13.760 --> 00:34:21.670
Inderjit Dhillon: XX transpose plus lambda. I inverse times. XY,


292
00:34:22.130 --> 00:34:26.820
Inderjit Dhillon: so very similar right, except instead of Xx. Transpose inverse. I have


293
00:34:27.120 --> 00:34:30.049
Inderjit Dhillon: XX transpose plus lambda. I inverse.


294
00:34:30.200 --> 00:34:33.730
Inderjit Dhillon: and then I can just substitute from here right? So


295
00:34:37.370 --> 00:34:44.120
Inderjit Dhillon: XX transpose plus lambda I inverse is equal to B.


296
00:34:44.810 --> 00:34:51.080
Inderjit Dhillon: Sigma squared plus lambda. IV. Transpose inverse of this.


297
00:34:51.380 --> 00:34:59.110
Inderjit Dhillon: and that's just equal to v sigma squared plus lambda. I inverse of the plant space.


298
00:35:01.110 --> 00:35:02.010
Inderjit Dhillon: Okay?


299
00:35:02.360 --> 00:35:08.290
Inderjit Dhillon: Because V transpose is the inverse of okay.


300
00:35:08.630 --> 00:35:14.809
Inderjit Dhillon: And then my prediction is X transpose W. Star.


301
00:35:15.590 --> 00:35:20.630
Inderjit Dhillon: Right. So I have U. Sigma v transpose


302
00:35:21.490 --> 00:35:27.179
Inderjit Dhillon: v sigma squared plus lambda. I inverse. V transpose


303
00:35:28.220 --> 00:35:47.279
Inderjit Dhillon: 3 sigma bear with me, because finally, when we come to the end we will actually kind of see a quite a beautiful explanation of what Ridge regularization is doing. But you know, we have to kind of go through the through the algebra. Okay? So again, this is equal to identity.


304
00:35:47.580 --> 00:35:52.300
Inderjit Dhillon: This is equal to identity, and what you get is view.


305
00:35:53.630 --> 00:36:00.280
Inderjit Dhillon: Sigma sigma square plus lambda. I inverse.


306
00:36:00.730 --> 00:36:04.369
Inderjit Dhillon: And sigma times, uniform space.


307
00:36:06.750 --> 00:36:10.629
Inderjit Dhillon: Okay, so and earlier, remember that when lambda was equal to 0,


308
00:36:10.970 --> 00:36:13.240
Inderjit Dhillon: then these would have all canceled out.


309
00:36:15.570 --> 00:36:18.979
Inderjit Dhillon: Okay. But this time lambda is not equal to 0.


310
00:36:19.330 --> 00:36:21.889
Inderjit Dhillon: Right? So things actually don't cancel it.


311
00:36:22.870 --> 00:36:29.029
Inderjit Dhillon: So what is this matrix, this matrix? What is this?


312
00:36:32.200 --> 00:36:35.809
Inderjit Dhillon: If you think about it? It's remember it's a diagonal matrix.


313
00:36:37.400 --> 00:36:46.589
Inderjit Dhillon: Sigma has, you know, the little sigmas on the diagonal. So here is the 1st element of this matrix. It's Sigma, one square


314
00:36:47.160 --> 00:36:52.800
Inderjit Dhillon: that comes from this Sigma and the Sigma.


315
00:36:53.240 --> 00:36:56.770
Inderjit Dhillon: And then, since it's the inverse of a diagonal matrix.


316
00:36:57.380 --> 00:37:03.529
Inderjit Dhillon: the elements of the the diagonal elements of this matrix are basically just the inverses of the corresponding diagonals.


317
00:37:03.960 --> 00:37:08.979
Inderjit Dhillon: So it becomes Sigma, one square divided by Sigma, one square plus lambda.


318
00:37:09.930 --> 00:37:13.880
Inderjit Dhillon: The 2 2 element is Sigma, one sigma, 2 square.


319
00:37:14.280 --> 00:37:19.760
Inderjit Dhillon: divided by Sigma, 2 square plus lambda, and all the way.


320
00:37:24.030 --> 00:37:27.150
Inderjit Dhillon: okay, and it's Sigma D square.


321
00:37:27.580 --> 00:37:30.289
Inderjit Dhillon: divided by Sigma D is 12 plus 9.


322
00:37:32.780 --> 00:37:38.820
Inderjit Dhillon: It's a diagonal matrix zeros on the off diagonals. Okay?


323
00:37:39.330 --> 00:37:47.740
Inderjit Dhillon: And if I want to get a similar representation over here as here right.


324
00:37:47.940 --> 00:37:53.250
Inderjit Dhillon: If I take this, then what I get is that this is equal to


325
00:37:54.000 --> 00:37:57.610
Inderjit Dhillon: summation of I equals one through d plus one


326
00:38:03.220 --> 00:38:13.389
Inderjit Dhillon: sigma i square divided by sigma y squared plus lambda times ui ui transfers.


327
00:38:13.950 --> 00:38:19.680
Inderjit Dhillon: and I guess, instead of d still go d then this will still d plus one.


328
00:38:23.720 --> 00:38:28.830
Inderjit Dhillon: Now, what can you say about this this coefficient?


329
00:38:29.220 --> 00:38:31.700
Inderjit Dhillon: Remember when lambda equal to 0,


330
00:38:33.350 --> 00:38:36.050
Inderjit Dhillon: it's equal to one when lambda equal to 0?


331
00:38:37.930 --> 00:38:42.519
Inderjit Dhillon: Okay, but in general it is less than equal to one.


332
00:38:45.300 --> 00:38:47.790
Inderjit Dhillon: it's equal to one. When lambda equal to 0.


333
00:38:48.350 --> 00:38:53.719
Inderjit Dhillon: As lambda becomes bigger, the denominator keeps on becoming bigger.


334
00:38:54.020 --> 00:38:57.289
Inderjit Dhillon: and it actually starts going towards 0.


335
00:38:57.690 --> 00:39:00.780
Inderjit Dhillon: So so let's let's let's look at that again. Okay.


336
00:39:00.890 --> 00:39:04.910
Inderjit Dhillon: so if my Sigma I squared, which is this, ith singular value


337
00:39:05.070 --> 00:39:07.590
Inderjit Dhillon: is much much bigger than Lamb.


338
00:39:08.240 --> 00:39:14.030
Inderjit Dhillon: Then what happens to this number, Sigma? I squared, divided by Sigma. I squared plus lambda.


339
00:39:14.690 --> 00:39:18.119
Inderjit Dhillon: Okay, if Sigma I squared is much much bigger than Lambda.


340
00:39:18.300 --> 00:39:23.649
Inderjit Dhillon: This will be nearly equal to Sigma. I squared, divided by Sigma I square.


341
00:39:24.870 --> 00:39:27.879
Inderjit Dhillon: and so it'll be equal, is nearly equal to one


342
00:39:29.520 --> 00:39:31.750
Inderjit Dhillon: in particular if lambda equal to 0.


343
00:39:32.340 --> 00:39:33.160
Inderjit Dhillon: Right?


344
00:39:33.300 --> 00:39:39.640
Inderjit Dhillon: But now let's see what will happen if Sigma I square is much smaller than lambda.


345
00:39:40.910 --> 00:39:45.699
Inderjit Dhillon: then I have Sigma, I square divided by Sigma. I squared plus lambda.


346
00:39:46.120 --> 00:39:49.250
Inderjit Dhillon: and this will be approximately equal to.


347
00:39:52.500 --> 00:39:56.860
Inderjit Dhillon: So remember, Sigma, I square that'll stay in the numerator


348
00:39:57.990 --> 00:40:05.029
Inderjit Dhillon: and Sigma, I squared plus lambda. When Sigma I squared is negligible compared to lambda is going to be lambda.


349
00:40:06.290 --> 00:40:10.059
Inderjit Dhillon: and that is going to be nearly equal to 0.


350
00:40:13.010 --> 00:40:16.719
Inderjit Dhillon: Okay, so what range regression does?


351
00:40:22.870 --> 00:40:28.350
Inderjit Dhillon: And we are able to only do this analysis by looking carefully at the singular value. Decomposition.


352
00:40:28.780 --> 00:40:32.569
Inderjit Dhillon: What read regression does is it shrinks


353
00:40:35.850 --> 00:40:40.870
Inderjit Dhillon: the small singular values.


354
00:40:44.580 --> 00:40:45.539
Inderjit Dhillon: 2. 0.


355
00:40:52.810 --> 00:40:59.579
Inderjit Dhillon: Okay, any questions so far, I think you should. Many people should have questions. Right?


356
00:41:04.580 --> 00:41:08.400
Inderjit Dhillon: Okay? So just to repeat the difference.


357
00:41:09.720 --> 00:41:16.060
Inderjit Dhillon: What ends up happening over here is, you have this matrix X transpose.


358
00:41:16.550 --> 00:41:17.710
Inderjit Dhillon: It has


359
00:41:21.010 --> 00:41:25.630
Inderjit Dhillon: d plus one singular values, and they can vary in magnitude.


360
00:41:27.260 --> 00:41:32.900
Inderjit Dhillon: The 1st singular value can be much larger than the d plus one singular value in particular.


361
00:41:33.410 --> 00:41:39.330
Inderjit Dhillon: if X transpose, is not full rank. Suppose the features are identical right


362
00:41:39.600 --> 00:41:42.660
Inderjit Dhillon: then sigma d plus one can actually be equal to 0.


363
00:41:44.020 --> 00:41:48.289
Inderjit Dhillon: But what linear regression does is kind of it


364
00:41:48.620 --> 00:41:51.859
Inderjit Dhillon: actually eliminates Sigma square sigma over here.


365
00:41:53.430 --> 00:41:59.470
Inderjit Dhillon: Sigma is not over here, so it treats each singular value to have the same importance.


366
00:42:00.730 --> 00:42:06.160
Inderjit Dhillon: And it basically just does a projection on the entire space.


367
00:42:07.430 --> 00:42:10.199
Inderjit Dhillon: But what can happen is that


368
00:42:11.650 --> 00:42:14.349
Inderjit Dhillon: what can end up happening is that.


369
00:42:17.380 --> 00:42:24.630
Inderjit Dhillon: oh, sorry that some of these singular values could actually be very small.


370
00:42:25.470 --> 00:42:29.890
Inderjit Dhillon: So when you take the inverse or minus sigma minus 2.


371
00:42:30.250 --> 00:42:32.389
Inderjit Dhillon: This could actually be very big.


372
00:42:36.380 --> 00:42:39.989
Inderjit Dhillon: Okay? So the coefficients can actually blow up in the prediction.


373
00:42:40.900 --> 00:42:44.699
Inderjit Dhillon: And what read regression does is that it?


374
00:42:46.320 --> 00:42:48.459
Inderjit Dhillon: The large singular values?


375
00:42:48.620 --> 00:42:51.689
Inderjit Dhillon: When Sigma I square is greater, greater than lambda.


376
00:42:52.280 --> 00:42:59.409
Inderjit Dhillon: then it basically does the same thing as regression as least squares regression. It basically keeps them at one.


377
00:42:59.410 --> 00:43:03.090
Nilesh Gupta: Hey, Indarjit? There's a question on the chat by Rishab.


378
00:43:03.850 --> 00:43:06.539
Inderjit Dhillon: Yeah, go ahead, Rishab. You can feel free to ask me your question.


379
00:43:07.200 --> 00:43:18.280
Rishab Maheshwari: I just wanted to clarify on the right side when we were going over the equations for Ridge regularization. What happened to the Y term. Did that get canceled out somewhere.


380
00:43:19.080 --> 00:43:24.510
Inderjit Dhillon: Oh, thank you for bringing that to my attention. I'll just add that back in.


381
00:43:25.710 --> 00:43:26.270
Rishab Maheshwari: Nope.


382
00:43:35.720 --> 00:43:36.620
Inderjit Dhillon: Is that good?


383
00:43:37.420 --> 00:43:38.580
Rishab Maheshwari: Yup. Thank you.


384
00:43:39.240 --> 00:43:39.910
Inderjit Dhillon: No.


385
00:43:41.750 --> 00:43:43.230
Nilesh Gupta: This is another, one, by.


386
00:43:43.230 --> 00:43:58.079
Inderjit Dhillon: There's another one. Yes. Can we say small? So me, Mohammed, can we say small 0 values or noise in the data and regularization leads to not overfitting to the noise. Yeah, I think. That's kind of a valid way of looking at it.


387
00:43:59.320 --> 00:44:01.349
Inderjit Dhillon: Does does that answer your question? Mir.


388
00:44:06.720 --> 00:44:07.500
Inderjit Dhillon: yeah.


389
00:44:07.820 --> 00:44:09.950
Inderjit Dhillon: And sometimes remember that we are.


390
00:44:10.080 --> 00:44:13.999
Inderjit Dhillon: Oh, you know, when you're running on the computer.


391
00:44:15.100 --> 00:44:21.670
Inderjit Dhillon: you can't represent a real number, exactly right? So you represent numbers as floating point numbers.


392
00:44:21.960 --> 00:44:25.029
Inderjit Dhillon: Whenever you do an operation. There's some error in it.


393
00:44:25.900 --> 00:44:29.789
Inderjit Dhillon: So many times when you get these small values, you cannot actually trust them.


394
00:44:30.320 --> 00:44:35.429
Inderjit Dhillon: and what bridge aggression is doing is in some sense kind of eliminating them. But in a smooth way.


395
00:44:35.800 --> 00:44:39.409
Inderjit Dhillon: So notice that, you know here's another thing we could have done right.


396
00:44:39.950 --> 00:44:47.900
Inderjit Dhillon: What if we had done the following that, you know, for so so this is shrinkage.


397
00:44:50.590 --> 00:44:53.490
Inderjit Dhillon: What if you had done some hard shrinkage.


398
00:44:57.000 --> 00:44:59.170
Inderjit Dhillon: Let me clarify what I mean.


399
00:44:59.300 --> 00:45:01.610
Inderjit Dhillon: What if I had said that?


400
00:45:02.040 --> 00:45:07.870
Inderjit Dhillon: You know I'll take the Sigma I square as is.


401
00:45:08.980 --> 00:45:14.069
Inderjit Dhillon: for I equal to one through. Let's say K,


402
00:45:15.040 --> 00:45:19.399
Inderjit Dhillon: and then I will take Sigma. I square and replace it by 0,


403
00:45:20.280 --> 00:45:25.559
Inderjit Dhillon: for I equal to K plus one through d plus one.


404
00:45:28.720 --> 00:45:30.560
Inderjit Dhillon: I could have done that also. Right?


405
00:45:32.450 --> 00:45:36.610
Inderjit Dhillon: What I've done above is like it's a smooth change.


406
00:45:36.960 --> 00:45:42.839
Inderjit Dhillon: But I could have done this and that. All that is saying is that I'm actually now using the


407
00:45:43.480 --> 00:45:45.039
Inderjit Dhillon: get truncated.


408
00:45:46.970 --> 00:45:47.990
Inderjit Dhillon: Yes, really.


409
00:45:51.100 --> 00:45:53.170
Inderjit Dhillon: Because if you look at it carefully.


410
00:45:53.350 --> 00:45:56.570
Inderjit Dhillon: then it'll just be the that you're taking the


411
00:45:56.730 --> 00:46:01.730
Inderjit Dhillon: case singular values as is, and you are truncating the rest of them to 0.


412
00:46:03.260 --> 00:46:07.009
Inderjit Dhillon: Okay? And that actually is also kind of a valid thing


413
00:46:07.470 --> 00:46:15.270
Inderjit Dhillon: that you can try to do right. I mean, you can. I think Ridge regularization is kind of more accepted in the in the language of


414
00:46:16.532 --> 00:46:19.820
Inderjit Dhillon: modern deep learning. It's sometimes called weight decay.


415
00:46:19.990 --> 00:46:29.500
Inderjit Dhillon: Okay? But you could potentially also set the small singular values explicitly to 0, whereas here they are kind of being gradually set to 0.


416
00:46:31.590 --> 00:46:36.870
Inderjit Dhillon: And remember, and, for example, over here it could happen that all the singular values are large.


417
00:46:38.140 --> 00:46:42.580
Inderjit Dhillon: in which case, you know it won't make that much of a difference.


418
00:46:44.770 --> 00:46:48.895
Inderjit Dhillon: Okay, so I I think this is, you know, very


419
00:46:51.320 --> 00:46:55.080
Inderjit Dhillon: Most people when they look at read regression. Right? They see this.


420
00:47:02.130 --> 00:47:07.849
Inderjit Dhillon: Okay. And it's kind of a good motivation. And you can say that you know you don't want your model to be too complex.


421
00:47:08.180 --> 00:47:10.459
Inderjit Dhillon: You don't want your W to be small.


422
00:47:10.900 --> 00:47:14.109
Inderjit Dhillon: but then, when you kind of look at the solution, you know.


423
00:47:14.110 --> 00:47:16.349
Nilesh Gupta: There's another question by Ritesh.


424
00:47:16.350 --> 00:47:18.770
Inderjit Dhillon: Yeah, let me finish.


425
00:47:18.960 --> 00:47:23.359
Inderjit Dhillon: So you basically, you know, get this.


426
00:47:25.820 --> 00:47:31.379
Inderjit Dhillon: The solution is this, and what it says is to obtain


427
00:47:32.860 --> 00:47:35.110
Inderjit Dhillon: to solve for this minimum. W,


428
00:47:36.030 --> 00:47:41.619
Inderjit Dhillon: that's equivalent to saying that you're going to take the small singular values and start ignoring.


429
00:47:42.370 --> 00:47:46.310
Inderjit Dhillon: Okay? What was the question? Why did they by somebody


430
00:47:47.650 --> 00:47:53.180
Inderjit Dhillon: routine. It's 0 property. Oh, yes, ritesh.


431
00:47:53.930 --> 00:47:57.979
Inderjit Dhillon: you cannot speak, but that's the definition of Svd, right?


432
00:48:00.780 --> 00:48:05.260
Inderjit Dhillon: So if I go back to my previous lecture.


433
00:48:10.360 --> 00:48:12.520
Inderjit Dhillon: When we talked about the Svd.


434
00:48:15.810 --> 00:48:17.839
Inderjit Dhillon: We arrange them in this way right?


435
00:48:23.830 --> 00:48:29.589
Inderjit Dhillon: That, Sigma one is greater than equal to Sigma, 2 is greater than equal to Sigma, and so they are increased in


436
00:48:29.720 --> 00:48:31.000
Inderjit Dhillon: decreasing order.


437
00:48:32.170 --> 00:48:34.849
Inderjit Dhillon: So does that answer your question, Ritesh.


438
00:48:38.500 --> 00:48:42.140
Inderjit Dhillon: I guess you said you can't speak, but I'm hoping you can say, yes.


439
00:48:45.920 --> 00:48:48.570
Inderjit Dhillon: yeah. Just knowing single value is back in.


440
00:48:48.760 --> 00:48:55.229
Inderjit Dhillon: Well, okay. So I'm not sure what you are asking. Ritesh, but


441
00:48:56.780 --> 00:49:03.620
Inderjit Dhillon: range regression does not do does not just remove the small singular values and set them to 0.


442
00:49:04.880 --> 00:49:10.639
Inderjit Dhillon: Range regression does exactly what I gave over here.


443
00:49:13.020 --> 00:49:15.990
Inderjit Dhillon: It's essentially taking the ith singular value.


444
00:49:16.280 --> 00:49:21.080
Inderjit Dhillon: and replacing it by sigma squared, divided by sigma squared plus lambda.


445
00:49:22.810 --> 00:49:26.460
Inderjit Dhillon: and that has the property, that when Sigma Square


446
00:49:27.120 --> 00:49:32.340
Inderjit Dhillon: is very large, much bigger than the the regularization coefficient lambda.


447
00:49:32.450 --> 00:49:39.230
Inderjit Dhillon: Then the Sigma square will be essentially you know. What you're doing is that coefficient is one.


448
00:49:40.300 --> 00:49:42.949
Inderjit Dhillon: and when Sigma Square is very small.


449
00:49:43.300 --> 00:49:49.099
Inderjit Dhillon: then it is sigma squared, divided by lambda, which will tend to 0. But it's not exactly 0.


450
00:49:49.680 --> 00:49:55.949
Inderjit Dhillon: An alternate way could be to do this or shrinkage


451
00:49:56.430 --> 00:49:59.250
Inderjit Dhillon: where you set the small singular values to 0.


452
00:50:00.640 --> 00:50:01.540
Inderjit Dhillon: Okay?


453
00:50:02.010 --> 00:50:04.839
Inderjit Dhillon: And you keep the large signal values as is.


454
00:50:05.600 --> 00:50:11.979
Inderjit Dhillon: that's also a a way to do things. But it's not the same as Ridge Regularization Ridge regularization.


455
00:50:12.900 --> 00:50:18.650
Inderjit Dhillon: And if you now think about doing this on the computer right to do hard regularization.


456
00:50:18.910 --> 00:50:21.000
Inderjit Dhillon: you actually need to know the Svd


457
00:50:23.770 --> 00:50:29.430
Inderjit Dhillon: to do Ridge regularization, you don't need the Svd. All I'm doing is I'm showing an analysis.


458
00:50:30.010 --> 00:50:32.729
Inderjit Dhillon: So, for example, if you want to solve this problem.


459
00:50:33.530 --> 00:50:36.299
Inderjit Dhillon: then you don't need to find the Svd effects.


460
00:50:37.130 --> 00:50:45.779
Inderjit Dhillon: you can actually do it using, you know, pulesky decomposition. QR. Decomposition, and so on. Or you can actually use an iterative algorithm


461
00:50:45.960 --> 00:50:47.269
Inderjit Dhillon: to do the solution.


462
00:50:49.280 --> 00:50:58.969
Inderjit Dhillon: Okay? And I think you'll see in the next. In the first, st in the 1st homework that we'll assign on Monday. When you look at this collaborative filtering problem, you know, which is


463
00:50:59.980 --> 00:51:11.549
Inderjit Dhillon: like, let's suppose you are given like a whole bunch of movies and ratings by users of movies, then and you then want to predict, the rating


464
00:51:11.660 --> 00:51:15.720
Inderjit Dhillon: of a movie by a given user. Then


465
00:51:16.410 --> 00:51:20.309
Inderjit Dhillon: you will see that you know the algorithm that you will use in your homework


466
00:51:20.600 --> 00:51:24.980
Inderjit Dhillon: will not need you to compute the Svd. Because the Svd finally is


467
00:51:25.110 --> 00:51:27.889
Inderjit Dhillon: quite an expensive competition to do


468
00:51:28.320 --> 00:51:34.080
Inderjit Dhillon: so here. What it's doing is it's really on helping us to understand what is going on.


469
00:51:34.390 --> 00:51:38.499
Inderjit Dhillon: But you typically will not solve this problem by computing the Svd.


470
00:51:38.620 --> 00:51:41.530
Inderjit Dhillon: so that's why doing this hard shrinkage


471
00:51:42.160 --> 00:51:45.410
Inderjit Dhillon: is something that is much more expensive, whereas


472
00:51:45.550 --> 00:51:54.940
Inderjit Dhillon: Ridge regularization is much more. You know, it can be done without the Svd. But it can be interpreted as doing some changes to the singular guidance.


473
00:51:56.320 --> 00:52:01.090
Inderjit Dhillon: Okay, does that clarify things? Yeah. So so that's actually a pretty important point.


474
00:52:01.470 --> 00:52:04.279
Inderjit Dhillon: So the singular value decomposition is


475
00:52:04.480 --> 00:52:06.690
Inderjit Dhillon: is a very good tool, but


476
00:52:06.950 --> 00:52:09.979
Inderjit Dhillon: it's actually quite expensive. And when we talk about


477
00:52:11.310 --> 00:52:16.399
Inderjit Dhillon: very large data sets. And the analysis of that, typically you do not. You don't compute it.


478
00:52:16.850 --> 00:52:18.540
Inderjit Dhillon: You will not be able to configure.


479
00:52:22.070 --> 00:52:35.210
Inderjit Dhillon: Okay, so let's look at another view of range regularization. Okay? So Ridge regression.


480
00:52:40.730 --> 00:52:55.370
Inderjit Dhillon: So so far, we've said, you know, minimize X transpose W minus y square plus lambda times, 2 normal


481
00:52:56.050 --> 00:52:56.970
Inderjit Dhillon: squid.


482
00:52:57.260 --> 00:53:02.539
Inderjit Dhillon: Okay, this problem is actually a little equivalent.


483
00:53:08.910 --> 00:53:11.280
Inderjit Dhillon: Do the following


484
00:53:15.010 --> 00:53:18.920
Inderjit Dhillon: problem, okay, so do the following.


485
00:53:22.030 --> 00:53:28.000
Inderjit Dhillon: okay, so take the same objective as


486
00:53:31.400 --> 00:53:37.269
Inderjit Dhillon: least squares regression. But then, you say I don't want


487
00:53:39.030 --> 00:53:46.660
Inderjit Dhillon: my coefficients to grow too much, but I'm actually going to put a constraint that my w Twos


488
00:53:46.770 --> 00:53:50.980
Inderjit Dhillon: Square should be less than equal to C or something.


489
00:53:54.350 --> 00:54:00.349
Inderjit Dhillon: So over here the ridge, the the trade off between regularization


490
00:54:00.790 --> 00:54:03.600
Inderjit Dhillon: and fit to the data is lambda.


491
00:54:03.940 --> 00:54:05.780
Inderjit Dhillon: And over here it is. C,


492
00:54:05.990 --> 00:54:10.280
Inderjit Dhillon: okay. So the the following is a constrained optimization problem.


493
00:54:32.930 --> 00:54:47.140
Inderjit Dhillon: Okay, so this problem, which is typically, how did regularization is posed is equivalent to this constrained optimization problem.


494
00:54:53.130 --> 00:54:57.340
Inderjit Dhillon: And the reason I'm mentioning the last part is that


495
00:54:57.840 --> 00:55:01.149
Inderjit Dhillon: we can geometrically kind of try to understand what is happening


496
00:55:01.280 --> 00:55:06.899
Inderjit Dhillon: right? And it'll actually end help, you understand, a different kind of regularization, which is


497
00:55:07.100 --> 00:55:11.620
Inderjit Dhillon: when we don't regularize by the sum of squares. But we regularize by


498
00:55:11.860 --> 00:55:23.540
Inderjit Dhillon: sum of absolute values. So instead of L 2 norm square, we can actually regularize by the l 1 norm, which leads to what is known as the lasso which you might have heard about.


499
00:55:24.580 --> 00:55:32.730
Inderjit Dhillon: Okay, so geometric interpretation


500
00:55:37.330 --> 00:55:43.590
Inderjit Dhillon: of this above constrained optimization problem.


501
00:55:55.680 --> 00:56:02.790
Inderjit Dhillon: Okay, so what you can show is that if I look at this function.


502
00:56:07.670 --> 00:56:13.460
Inderjit Dhillon: or least squares function, then it actually looks like the following.


503
00:56:15.050 --> 00:56:19.630
Inderjit Dhillon: okay, it's if I look at the contour map or level sets.


504
00:56:22.930 --> 00:56:25.270
Inderjit Dhillon: and I'll explain what level sets are.


505
00:56:26.270 --> 00:56:27.160
Inderjit Dhillon: Okay.


506
00:56:27.470 --> 00:56:29.453
Inderjit Dhillon: So if I look at


507
00:56:30.380 --> 00:56:35.120
Inderjit Dhillon: okay, so level sets are where fw, is a constant.


508
00:56:36.810 --> 00:56:37.670
Inderjit Dhillon: Okay?


509
00:56:39.230 --> 00:56:44.880
Inderjit Dhillon: So the contour map is actually something like looks like this.


510
00:56:45.850 --> 00:56:51.360
Inderjit Dhillon: These are ellipses, and I'm not showing that over here, but it can be shown.


511
00:56:54.640 --> 00:57:03.560
Inderjit Dhillon: and this each of the contour maps corresponds to fw, equal to c 1 or some particular c. 1.


512
00:57:04.060 --> 00:57:05.000
Inderjit Dhillon: Okay.


513
00:57:05.160 --> 00:57:10.489
Inderjit Dhillon: And this one would correspond to fw, equal to c, 2


514
00:57:10.770 --> 00:57:13.260
Inderjit Dhillon: and C 2 is less than c 1.


515
00:57:15.190 --> 00:57:18.760
Inderjit Dhillon: Okay, and this value over here


516
00:57:19.880 --> 00:57:23.839
Inderjit Dhillon: is going to be fw, equal to 0. So this is the minimizer.


517
00:57:24.490 --> 00:57:27.309
Inderjit Dhillon: This is the D square solution.


518
00:57:41.160 --> 00:57:45.720
Inderjit Dhillon: And now, if I look at my constraint set right. So let me.


519
00:57:46.450 --> 00:57:55.100
Inderjit Dhillon: This is my constraint set right. And if I look at that, okay.


520
00:57:57.500 --> 00:58:05.119
Inderjit Dhillon: if I fix a particular CI guess I should have used different notation. Let's see.


521
00:58:06.440 --> 00:58:09.160
Inderjit Dhillon: maybe I'll change this C to.


522
00:58:16.280 --> 00:58:23.160
Inderjit Dhillon: okay, okay, let me just say, R for radius for some.


523
00:58:25.040 --> 00:58:31.810
Inderjit Dhillon: Okay, so this is typically going to be like, R is the radius.


524
00:58:31.920 --> 00:58:36.050
Inderjit Dhillon: And what I'm saying is that my X norm of X squared


525
00:58:37.470 --> 00:58:39.560
Inderjit Dhillon: is less than equal to R.


526
00:58:39.950 --> 00:58:43.620
Inderjit Dhillon: And what I'm saying is, my constraint set is this.


527
00:58:44.700 --> 00:58:48.970
Inderjit Dhillon: I want only sorry I wrote Wx, but it's W.


528
00:58:49.960 --> 00:58:54.209
Inderjit Dhillon: My, the only W's that I want are actually within this.


529
00:58:56.080 --> 00:59:02.620
Inderjit Dhillon: So the solution of Ridge regularization or the the constrained version of Ridge regularization is


530
00:59:02.910 --> 00:59:13.089
Inderjit Dhillon: that you basically look at the level sets. And the 1st time the level set meets the constraint set.


531
00:59:14.500 --> 00:59:18.490
Inderjit Dhillon: Okay, that is the rich regularization solution.


532
00:59:27.700 --> 00:59:32.230
Inderjit Dhillon: Okay, so this is another interpretation of bridge regularization. Right? You're minimizing


533
00:59:32.480 --> 00:59:35.269
Inderjit Dhillon: F of W. It will be minimized over here


534
00:59:36.670 --> 00:59:43.580
Inderjit Dhillon: if you're just looking at F of W. Which is the least squares problem that gives you the least square solution.


535
00:59:44.740 --> 00:59:51.440
Inderjit Dhillon: Okay, but that's not what you want. Because this W. Magnitude might actually be very far. It might be very far from the origin.


536
00:59:52.730 --> 00:59:55.489
Inderjit Dhillon: As a result, these W values could be very large.


537
00:59:58.310 --> 01:00:09.479
Inderjit Dhillon: You want these W values to be small. So you have this constraint that my W. Values cannot have some squared. To be bigger than R. They must be, is less than equal to


538
01:00:10.180 --> 01:00:20.350
Inderjit Dhillon: so geometrically the minimum value of this constraint in optimization problem is when the level sets 1st hit this


539
01:00:21.070 --> 01:00:24.959
Inderjit Dhillon: or radius or sphere.


540
01:00:26.470 --> 01:00:30.289
Inderjit Dhillon: Okay, so that is an interpretation of Ridge regularization.


541
01:00:32.280 --> 01:00:33.989
Inderjit Dhillon: Any questions about that.


542
01:00:37.170 --> 01:00:41.090
Hormoz Shahrzad: So is there a relationship between R. And Lambda.


543
01:00:41.300 --> 01:00:42.040
Inderjit Dhillon: What's that?


544
01:00:42.380 --> 01:00:45.989
Hormoz Shahrzad: So relation relationship between R. And Lambda.


545
01:00:45.990 --> 01:00:50.690
Inderjit Dhillon: Yeah. So basically, for every lambda there exists now. But there's no straightforward relationship.


546
01:00:52.600 --> 01:00:53.770
Hormoz Shahrzad: Thank you. I see.


547
01:00:54.020 --> 01:01:07.680
Inderjit Dhillon: Yeah. But basically, you know, you can either work with ours or you can work with lambdas right and solving like a constraint. Optimization problem is generally numerically a little bit more difficult. So people, whenever they solve it, they really look at


548
01:01:07.820 --> 01:01:09.689
Inderjit Dhillon: this version of the plug.


549
01:01:11.350 --> 01:01:18.300
Inderjit Dhillon: Okay? And that that typically, you know, we'll use either an iterative, you know, depending on the size of the problem. Either they'll use a very


550
01:01:18.813 --> 01:01:24.660
Inderjit Dhillon: an iterative algorithm. It's very large, or they can use the cholesky decomposition, or even the Sv.


551
01:01:26.760 --> 01:01:27.590
Inderjit Dhillon: Okay.


552
01:01:28.080 --> 01:01:31.689
Inderjit Dhillon: But the important point to note is that this can be represented as


553
01:01:31.810 --> 01:01:35.819
Inderjit Dhillon: a concern, and this is equivalent to a constrained optimization form.


554
01:01:38.690 --> 01:01:42.399
Inderjit Dhillon: And one of the reasons I'm I'm sorry. Sorry. Go ahead.


555
01:01:42.790 --> 01:01:44.729
Hormoz Shahrzad: No, I just said, Thank you. Got it.


556
01:01:44.730 --> 01:01:52.240
Inderjit Dhillon: Sure. Yeah. And the and one of the reasons I'm talking about this is that there's another kind of regularization that is actually very popular


557
01:01:53.680 --> 01:01:55.540
Inderjit Dhillon: or maybe used to be popular.


558
01:01:56.750 --> 01:01:59.490
Inderjit Dhillon: Is gold lasso.


559
01:02:00.530 --> 01:02:06.940
Inderjit Dhillon: Okay? And lasso has the following objective, same objective as


560
01:02:07.430 --> 01:02:10.499
Inderjit Dhillon: these squares regression in terms of the data fit.


561
01:02:17.400 --> 01:02:22.400
Inderjit Dhillon: So X transpose W minus y 2 non square.


562
01:02:23.360 --> 01:02:30.230
Inderjit Dhillon: But now the regularization is changed to be the one norm of the


563
01:02:36.740 --> 01:02:40.940
Inderjit Dhillon: so seemingly like a pretty innocuous change, right?


564
01:02:41.450 --> 01:02:43.898
Inderjit Dhillon: Instead of some square


565
01:02:45.580 --> 01:02:50.809
Inderjit Dhillon: of some of the squares of W, we are penalizing the sum of the absolute values.


566
01:02:51.520 --> 01:02:59.500
Inderjit Dhillon: Okay? So it's basically one norm of W instead.


567
01:03:01.170 --> 01:03:06.840
Inderjit Dhillon: Oh, when the name


568
01:03:11.000 --> 01:03:18.190
Inderjit Dhillon: people use this. And one of the reasons actually, people use, this is the following, that La soul


569
01:03:18.400 --> 01:03:20.500
Inderjit Dhillon: many times leads to


570
01:03:23.770 --> 01:03:25.320
Inderjit Dhillon: a solution


571
01:03:29.740 --> 01:03:37.330
Inderjit Dhillon: went sparse. W, okay, and what do I mean by sparse W.


572
01:03:37.900 --> 01:03:44.860
Inderjit Dhillon: That is coefficient vector, with many Zeros.


573
01:03:48.580 --> 01:03:54.870
Inderjit Dhillon: And you can think of that. As you know, it's so, basically a more parsimonious model.


574
01:03:55.460 --> 01:03:57.300
Inderjit Dhillon: Or see more news.


575
01:04:00.680 --> 01:04:05.600
Inderjit Dhillon: Right? So remember that I'm taking a linear combination of the D features


576
01:04:06.890 --> 01:04:15.090
Inderjit Dhillon: right d. Could be very large in modern problems. D could be as large as a million.


577
01:04:16.500 --> 01:04:21.119
Inderjit Dhillon: So what that means is that you now have a coefficient of 1 million features.


578
01:04:22.340 --> 01:04:25.830
Inderjit Dhillon: Okay? But somebody can ask, okay, can you give me a solution


579
01:04:26.590 --> 01:04:30.140
Inderjit Dhillon: which has a still, a good solution, a good fit?


580
01:04:30.440 --> 01:04:35.659
Inderjit Dhillon: Can you give me a solution which has lesser coefficients, it's a more interpretable model.


581
01:04:36.770 --> 01:04:39.049
Inderjit Dhillon: So what happens is that the lasso


582
01:04:39.900 --> 01:04:43.429
Inderjit Dhillon: results in a more parsimonious model?


583
01:04:44.670 --> 01:04:51.310
Inderjit Dhillon: And why is that? Okay? And now we can see that if I look at lasso.


584
01:04:52.750 --> 01:05:00.610
Inderjit Dhillon: okay, I can think of it as being equivalent. Actually, let me say that it's equivalent


585
01:05:08.280 --> 01:05:09.370
Inderjit Dhillon: who?


586
01:05:12.978 --> 01:05:15.770
Inderjit Dhillon: The following constraint, optimization problem.


587
01:05:16.430 --> 01:05:33.490
Inderjit Dhillon: the same thing that we did before. I'm following that optimization problem which is minimize W


588
01:05:33.960 --> 01:05:42.399
Inderjit Dhillon: X transpose W. Minus y squared such that what is the constraint? Now?


589
01:05:43.780 --> 01:05:46.970
Inderjit Dhillon: The norm of W.


590
01:05:47.080 --> 01:05:49.170
Inderjit Dhillon: But the one norm of W


591
01:05:49.430 --> 01:05:51.680
Inderjit Dhillon: is less than equal to R.


592
01:05:54.570 --> 01:05:59.040
Inderjit Dhillon: Now, if I look at the, if I draw it geometrically.


593
01:05:59.810 --> 01:06:02.599
Inderjit Dhillon: what is the norm of W. Less than equal to R.


594
01:06:03.980 --> 01:06:09.180
Inderjit Dhillon: I wonder if I did that before? Right. So, for example, if I go back to the Linear Algebra review


595
01:06:09.860 --> 01:06:12.239
Inderjit Dhillon: when we talked about different norms.


596
01:06:14.780 --> 01:06:17.030
Inderjit Dhillon: Oh, yeah, look, I did that here, right


597
01:06:20.469 --> 01:06:23.840
Inderjit Dhillon: norm of W is less than equal to one.


598
01:06:24.219 --> 01:06:32.479
Inderjit Dhillon: Is this set right? Because this has boundaries 1 0 0. This is corner points as 1 0 0


599
01:06:32.750 --> 01:06:34.539
Inderjit Dhillon: 0 0 1, and so on.


600
01:06:35.349 --> 01:06:38.209
Inderjit Dhillon: And in between these are straight lines.


601
01:06:39.460 --> 01:06:41.479
Inderjit Dhillon: Where are the 2 norm?


602
01:06:43.710 --> 01:06:51.410
Inderjit Dhillon: The constraint set is a sphere, but this actually has corners.


603
01:06:52.000 --> 01:06:57.119
Inderjit Dhillon: this boundary has corners, and that's why it'll actually give star solution.


604
01:06:58.710 --> 01:07:01.250
Inderjit Dhillon: Okay, so this constraint set will be


605
01:07:06.440 --> 01:07:11.410
Inderjit Dhillon: sorry. My drawing is not the best, but it's going to be


606
01:07:12.570 --> 01:07:16.040
Inderjit Dhillon: norm of w, 1 is less than equal to.


607
01:07:18.299 --> 01:07:20.999
Inderjit Dhillon: Okay, I'm given my data. My.


608
01:07:21.730 --> 01:07:25.959
Inderjit Dhillon: you know. Suppose the same problem. I'll basically get something like this.


609
01:07:26.719 --> 01:07:31.799
Inderjit Dhillon: my F of W, right? So if I draw the level sets of that.


610
01:07:32.200 --> 01:07:34.199
Inderjit Dhillon: it's gonna be something like this.


611
01:07:35.720 --> 01:07:39.590
Inderjit Dhillon: and then the solution will be the 1st time it actually hits.


612
01:07:42.420 --> 01:07:48.409
Inderjit Dhillon: And because this has corners, you can see that the solution is here.


613
01:07:49.360 --> 01:07:51.410
Inderjit Dhillon: This is the last. So solution


614
01:07:55.780 --> 01:07:59.630
Inderjit Dhillon: is usually sports.


615
01:08:00.490 --> 01:08:09.149
Inderjit Dhillon: Now remember, I'm drawing this into 2 dimensions. But you know this is happening in d dimensions when there are d features. So d is a million.


616
01:08:09.310 --> 01:08:16.009
Inderjit Dhillon: your like D dimensional space. So it's not going to be like, only one variable is negative or is not negative.


617
01:08:16.140 --> 01:08:20.889
Inderjit Dhillon: But there may be many variables. Okay? And remember, there is the regularization coefficient. Right?


618
01:08:21.760 --> 01:08:23.730
Inderjit Dhillon: So as you


619
01:08:26.800 --> 01:08:35.440
Inderjit Dhillon: increase lambda, right? That basically means that you are decreasing. R,


620
01:08:36.210 --> 01:08:44.370
Inderjit Dhillon: right? And you can, you will basically oh, gets bar solutions.


621
01:08:47.140 --> 01:08:49.279
Inderjit Dhillon: Okay, any questions about that.


622
01:09:00.170 --> 01:09:02.699
Inderjit Dhillon: Sometimes it's also called the control map.


623
01:09:04.780 --> 01:09:08.410
Inderjit Dhillon: You've probably seen these kinds of maps when you do hiking, and so on. Right?


624
01:09:12.280 --> 01:09:16.519
Inderjit Dhillon: This is a convex function. So you know, it's like this.


625
01:09:16.810 --> 01:09:19.790
Inderjit Dhillon: And every level set is ellipse.


626
01:09:21.830 --> 01:09:23.809
Inderjit Dhillon: Okay? Any questions for me.


627
01:09:27.290 --> 01:09:28.860
Inderjit Dhillon: Okay, so let's recap.


628
01:09:29.300 --> 01:09:33.919
Inderjit Dhillon: We started this course off with talking about regression.


629
01:09:34.590 --> 01:09:42.829
Inderjit Dhillon: We looked at least squares regression, the sum of the squares. I think right. In the beginning there was a question like, Why are we looking at some of squares?


630
01:09:43.020 --> 01:09:47.920
Inderjit Dhillon: And you've seen that we've developed kind of a you know, some really nice intuition.


631
01:09:48.190 --> 01:09:52.170
Inderjit Dhillon: And one of the big reasons why we use sum of squares is because it's


632
01:09:52.760 --> 01:09:56.540
Inderjit Dhillon: in some sense good to analyze. It's easy to analyze right?


633
01:09:56.860 --> 01:09:57.735
Inderjit Dhillon: So


634
01:09:59.260 --> 01:10:05.239
Inderjit Dhillon: Then we realized that, hey, you know, we need some linear algebra for this. I did some linear algebra review.


635
01:10:05.390 --> 01:10:10.819
Inderjit Dhillon: Then we came back and said, today. What we saw is that, hey? Least squares regression by itself


636
01:10:11.310 --> 01:10:14.750
Inderjit Dhillon: is not a good predictive method, because it can overfit.


637
01:10:15.210 --> 01:10:21.359
Inderjit Dhillon: And that principle is very true of machine learning in general. So this is a particular example where


638
01:10:21.470 --> 01:10:29.039
Inderjit Dhillon: you don't want too complex a model that fits your training data exactly, because that leads to overfitting


639
01:10:29.490 --> 01:10:33.910
Inderjit Dhillon: in the context of regression. We saw that we can prevent overfitting


640
01:10:34.190 --> 01:10:39.300
Inderjit Dhillon: by penalizing the coefficients that we have the coefficients. W.


641
01:10:40.000 --> 01:10:43.449
Inderjit Dhillon: Bridge regularization is one way to do it.


642
01:10:43.910 --> 01:10:49.479
Inderjit Dhillon: and you can think of, you know, that has the interpretation. When we look at the Svd.


643
01:10:49.630 --> 01:10:57.140
Inderjit Dhillon: That you know you're kind of shrinking. The small singular values towards 0 and the large singular values towards one.


644
01:10:57.350 --> 01:11:00.230
Inderjit Dhillon: But it's not doing like a hard operation on them.


645
01:11:00.470 --> 01:11:04.739
Inderjit Dhillon: And then we saw that there's another regularization method called


646
01:11:05.120 --> 01:11:07.900
Inderjit Dhillon: the soul. Okay? And I kind of


647
01:11:08.830 --> 01:11:18.899
Inderjit Dhillon: gave you some intuition as to why that leads to a more parsimonious or a sparsal model where the coefficients have many zeros. As a result.


648
01:11:19.060 --> 01:11:24.489
Inderjit Dhillon: the predictor is only using a few of the defeaters.


649
01:11:25.740 --> 01:11:32.199
Inderjit Dhillon: Okay, so with that, we are kind of coming to the end of how much regression we will do in this course.


650
01:11:32.480 --> 01:11:34.015
Inderjit Dhillon: I will.


651
01:11:36.127 --> 01:11:48.820
Inderjit Dhillon: On Monday I will give a lecture on collaborative filtering, which is, you know, recommendations by the, you know, like a very important problem these days, right?


652
01:11:48.980 --> 01:11:51.450
Inderjit Dhillon: Which is collaborative filtering


653
01:11:51.650 --> 01:12:10.099
Inderjit Dhillon: recommendation algorithms. And so on right. So we will look at one particular way of doing collaborative filtering or recommendation, and that, you know, back when the Netflix price happened, was actually one of the leading methods. I'll talk about that method and see that that naturally, basically is a


654
01:12:10.260 --> 01:12:11.490
Inderjit Dhillon: regression problem.


655
01:12:12.110 --> 01:12:15.249
Inderjit Dhillon: We'll I'll do that lecture on Monday.


656
01:12:15.400 --> 01:12:22.210
Inderjit Dhillon: Give you your 1st homework on Monday. That will in involve programming


657
01:12:22.470 --> 01:12:25.389
Inderjit Dhillon: right? And then once I finish the


658
01:12:26.022 --> 01:12:34.519
Inderjit Dhillon: collaborative filtering lecture, then from next from next week onwards. Wednesday. I'll start talking about classification.


659
01:12:34.930 --> 01:12:41.330
Inderjit Dhillon: Okay, where the variable to be predicted is not real, but is categoric.


660
01:12:41.460 --> 01:12:45.000
Inderjit Dhillon: Predict whether, for example, an email is spam or not.


661
01:12:45.520 --> 01:12:49.180
Inderjit Dhillon: Okay. So that's what is coming up in the course.


662
01:12:50.900 --> 01:12:55.710
Inderjit Dhillon: Okay. So with that, we are at the end of this lecture, any questions


663
01:12:56.840 --> 01:12:59.200
Inderjit Dhillon: on the material that we've seen so far.


664
01:12:59.640 --> 01:13:06.649
Hormoz Shahrzad: Excuse me, you mentioned that lasso is not popular anymore. Is there any reason for that specific.


665
01:13:06.650 --> 01:13:07.750
Inderjit Dhillon: Go.


666
01:13:08.570 --> 01:13:14.865
Inderjit Dhillon: Oh, when I say lasso is not popular, I mean, basically, everything has been taken over by deep learning. So


667
01:13:16.760 --> 01:13:17.520
Hormoz Shahrzad: I see.


668
01:13:17.680 --> 01:13:21.140
Inderjit Dhillon: So, and they.


669
01:13:21.651 --> 01:13:23.699
Hormoz Shahrzad: Are more naturally for.


670
01:13:23.700 --> 01:13:29.890
Inderjit Dhillon: Yeah, we weight decay is like, you know, just naturally comes up in deep learning.


671
01:13:30.400 --> 01:13:41.415
Inderjit Dhillon: And then if you try to get spar solutions. You have to deal with the fact that it's not a differentiable function, and so on. So typically, I haven't really seen much use of


672
01:13:42.070 --> 01:13:44.769
Inderjit Dhillon: less. So kind of regression in deep learning.


673
01:13:46.050 --> 01:13:47.270
Hormoz Shahrzad: Thank you. Got it.


674
01:13:47.420 --> 01:13:51.939
Inderjit Dhillon: But it's still, it's still very important. It's good to understand. And maybe there are some cases where


675
01:13:52.090 --> 01:13:55.399
Inderjit Dhillon: we kind of you know, may want to look at


676
01:13:55.860 --> 01:14:00.830
Inderjit Dhillon: l. 1 regularization, even in, you know, deep learning applications.


677
01:14:01.300 --> 01:14:05.410
Hormoz Shahrzad: Yeah, I think interpretability is still a big issue. Right?


678
01:14:05.930 --> 01:14:09.873
Inderjit Dhillon: Well, that's a huge issue with I mean, if it's a big big issue with


679
01:14:10.240 --> 01:14:15.159
Inderjit Dhillon: range regression, it's an even bigger issue with deep learning, but I think generally


680
01:14:15.720 --> 01:14:18.050
Inderjit Dhillon: generally, what we have seen is


681
01:14:18.350 --> 01:14:25.330
Inderjit Dhillon: that the quality of the approximation people care more about than interpretability. So in particular.


682
01:14:25.510 --> 01:14:31.999
Inderjit Dhillon: people generally might care more about. You know the goodness of a recommendation algorithm than being able to interpret it.


683
01:14:33.770 --> 01:14:35.140
Inderjit Dhillon: Yeah, makes sense. Thank you.


684
01:14:35.500 --> 01:14:40.230
Inderjit Dhillon: That that is just that's just true. Okay, I see some comments.


685
01:14:41.280 --> 01:14:48.070
Inderjit Dhillon: Got it. Thank you. Thank you. Okay, good. So that ends this lecture, and I will see you on Monday.


686
01:14:48.460 --> 01:14:48.971
Inderjit Dhillon: bye. Have a good weekend.


---- END OF LECTURE -------- START OF LECTURE 6 ----
WEBVTT

1
00:00:01.198 --> 00:00:08.248
Inderjit Dhillon: So today we will actually, in some sense, it'll be the last lecture on regression. But I will actually


2
00:00:08.768 --> 00:00:17.318
Inderjit Dhillon: talk a little bit about a seemingly different problem of collaborative filtering recommendation or missing value estimation.


3
00:00:17.478 --> 00:00:29.687
Inderjit Dhillon: And we'll see how regression naturally comes in. And then we will actually give you a homework today on the Netflix problem. So Netflix.


4
00:00:29.838 --> 00:00:35.957
Inderjit Dhillon: I think about nearly probably been about 20 years ago. They came up with the competition


5
00:00:36.188 --> 00:00:47.587
Inderjit Dhillon: where they gave, you know, training data, the release training data, or, you know, subset of ratings that they had.


6
00:00:48.122 --> 00:00:56.267
Inderjit Dhillon: I think it was about 20,000 movies, maybe half a million users or anonymized users.


7
00:00:56.458 --> 00:01:00.077
Inderjit Dhillon: And they said that, you know, on the


8
00:01:00.298 --> 00:01:06.668
Inderjit Dhillon: on a held out data set, they were achieving about point 9 5 root mean square error.


9
00:01:06.858 --> 00:01:10.568
Inderjit Dhillon: And they posed a challenge and said that if anybody can


10
00:01:10.718 --> 00:01:18.778
Inderjit Dhillon: improve the performance by 10%, so about point 8 5 root mean square error. Then they would actually give that team


11
00:01:18.928 --> 00:01:20.398
Inderjit Dhillon: a million dollars.


12
00:01:20.718 --> 00:01:29.237
Inderjit Dhillon: So it was the Netflix 1 million dollar challenge problem. And it actually spurred on a lot of interesting work in


13
00:01:29.378 --> 00:01:36.968
Inderjit Dhillon: machine learning, in particular, in recommender systems. And we'll see one aspect of it where


14
00:01:37.398 --> 00:01:57.188
Inderjit Dhillon: regression naturally comes and plays a role. So I'll talk a little bit about that. Talk about the problem, and then we will assign that as a homework, and this homework will be available to you on canvas, and it'll be due next week on Friday.


15
00:01:58.148 --> 00:02:00.988
Inderjit Dhillon: So before I start any questions.


16
00:02:05.308 --> 00:02:06.268
Inderjit Dhillon: okay.


17
00:02:06.998 --> 00:02:16.708
Inderjit Dhillon: so let me kind of review a little bit about. You know what we spoke about last time we talked about regression


18
00:02:17.088 --> 00:02:20.937
Inderjit Dhillon: and in particular regularization and regression.


19
00:02:21.118 --> 00:02:27.738
Inderjit Dhillon: we took in a particular example about univariate polynomial fitting, and we


20
00:02:28.328 --> 00:02:40.367
Inderjit Dhillon: I showed you some slides from the Bishop book, which we will also upload, where if you are used to


21
00:02:40.718 --> 00:03:02.817
Inderjit Dhillon: higher degree of a polynomial to fit you end up fitting the training data exactly. But you actually end up doing a poor job on the if a new test point came up right? So that's the phenomenon of overfitting. And then we saw that one of the ways in which it was manifest is that many coefficients were very large.


22
00:03:02.918 --> 00:03:09.458
Inderjit Dhillon: And so we tried to. We basically added up ended up adding


23
00:03:10.828 --> 00:03:15.648
Inderjit Dhillon: penalty on the size of the coefficient, coefficient terms.


24
00:03:15.758 --> 00:03:28.187
Inderjit Dhillon: And when you add the penalty as sum of squares, which is the norm of W Square. Then we get Ridge regression. And we looked at Ridge regression, and we saw it. Actually.


25
00:03:28.308 --> 00:03:33.977
Inderjit Dhillon: solution is actually very similar to a classical, least squares problem, except that you have lambda. I


26
00:03:34.128 --> 00:03:35.328
Inderjit Dhillon: over here.


27
00:03:36.068 --> 00:03:48.557
Inderjit Dhillon: But then we also analyzed it, using the Svd, so not just a computational tool. And we compared it to the least squares regression problem, which is essentially an orthogonal projection


28
00:03:48.678 --> 00:03:57.477
Inderjit Dhillon: onto a particular range space, and we saw over here that this was, in some sense, you know, like a


29
00:03:57.668 --> 00:04:06.148
Inderjit Dhillon: a case where the small singular values kind of are regarded as noise, and they would be set to 0 in a smooth way.


30
00:04:06.308 --> 00:04:35.348
Inderjit Dhillon: using ridge regression, and the large singular values would essentially be preserved and would be equal to one. So that kind of helps you a little bit better in understanding what regularized regression does. And so you can think of it as a little bit of this phenomenon called shrinkage, which is, you take the large singular values you kind of keep them and the smaller singular values you kind of shrink them towards 0.


31
00:04:35.598 --> 00:04:50.378
Inderjit Dhillon: Okay, we gave us geometric interpretation, also of Ridge regularization that, you know. If you were doing just least squares regression, you would just be. You know, these points over here, which are the global minima.


32
00:04:50.628 --> 00:04:54.957
Inderjit Dhillon: right? And so that would just be the solution. And these are the level sets


33
00:04:55.198 --> 00:05:00.148
Inderjit Dhillon: of the quadratic functional. That's in the least squares problem.


34
00:05:00.694 --> 00:05:16.917
Inderjit Dhillon: and you can think of Ridge regression as saying that I'm going to make sure that my coefficients are, have l. 2 norm squared less than equal to certain value, and that's given by this sphere. And so the


35
00:05:17.228 --> 00:05:26.718
Inderjit Dhillon: when you minimize F of W subject to the constraint that your variables lie in this region. Then you get this solution.


36
00:05:26.848 --> 00:05:39.298
Inderjit Dhillon: Okay? And then we didn't do too much. We just introduced the lasso, which is, instead of the squared penalty. We have absolute values.


37
00:05:39.428 --> 00:05:56.688
Inderjit Dhillon: you know, and so that can be thought of. As you know, all my coefficients where the l 1 norm is less than a particular value is captured by this surface, and you can see that this surface has corners.


38
00:05:56.998 --> 00:06:19.297
Inderjit Dhillon: And as a result, you know, when you minimize F of W. Subject to your coefficients living in this space, you kind of end up getting sparser solutions. Note that this, you know. Obviously, I'm drawing in 2D. But you know your computation is actually taking place, or your model is lives in a d dimensional space.


39
00:06:20.398 --> 00:06:33.067
Inderjit Dhillon: Okay? So we will kind of use a little bit of that material. And today we will look at the problem of a special problem in missing value estimation.


40
00:06:40.358 --> 00:06:41.218
Inderjit Dhillon: Okay?


41
00:06:41.558 --> 00:06:44.818
Inderjit Dhillon: And in particular, this problem comes up in


42
00:06:45.228 --> 00:06:50.108
Inderjit Dhillon: the problem that we are going to talk about comes up in recommender systems.


43
00:06:53.868 --> 00:07:01.438
Inderjit Dhillon: Okay? And again, the particular problem that we are going to talk about is also known as Oh.


44
00:07:02.118 --> 00:07:04.058
Inderjit Dhillon: matrix completion.


45
00:07:04.788 --> 00:07:14.928
Inderjit Dhillon: And we'll see why it is called that or matrix factorization for recommender systems.


46
00:07:17.418 --> 00:07:18.368
Inderjit Dhillon: Okay?


47
00:07:18.488 --> 00:07:24.468
Inderjit Dhillon: So I told you a little bit about the next Netflix challenge. Okay, so suppose you have


48
00:07:25.538 --> 00:07:27.487
Inderjit Dhillon: a bunch of users.


49
00:07:28.968 --> 00:07:35.107
Inderjit Dhillon: Okay? So let's assume that there are M. Users. And you know, obviously, M could be in the


50
00:07:35.618 --> 00:07:41.468
Inderjit Dhillon: hundreds of millions, even billions. Now, right? And you have


51
00:07:42.318 --> 00:07:50.458
Inderjit Dhillon: movies, or they actually could be, you know, some items, for example, let's say on amazon.com.


52
00:07:52.908 --> 00:07:55.718
Inderjit Dhillon: Okay? And you have m users.


53
00:07:56.958 --> 00:07:59.607
Inderjit Dhillon: And suppose you have N movies


54
00:08:00.158 --> 00:08:09.987
Inderjit Dhillon: right? And typically users will rate a subset of their movies right? So suppose there's a particular user. And he might say that


55
00:08:10.268 --> 00:08:20.357
Inderjit Dhillon: they did not like this movie at all. So they gave it rating for one. But then they liked this movie a lot, which is, they rated as 5.


56
00:08:20.958 --> 00:08:25.648
Inderjit Dhillon: Okay, maybe I won't draw the circles.


57
00:08:28.328 --> 00:08:35.908
Inderjit Dhillon: Okay, so 1 5. But you know, a user typically doesn't rate. You know


58
00:08:37.078 --> 00:08:44.607
Inderjit Dhillon: all the movies right? They can't even watch all the movies clearly, right? And they may not also rate all the movies that they watched.


59
00:08:45.218 --> 00:08:49.927
Inderjit Dhillon: Okay, you can maybe get an implicit feel for that. But there'll be many user movies


60
00:08:50.078 --> 00:08:57.907
Inderjit Dhillon: where we don't know their rating. Okay, so maybe another user really liked this movie that this other user hated


61
00:08:58.533 --> 00:09:01.157
Inderjit Dhillon: they also like this movie.


62
00:09:01.798 --> 00:09:12.157
Inderjit Dhillon: And maybe this user is a little bit like the 1st user where they like, did not like this movie. And like this movie.


63
00:09:12.448 --> 00:09:16.448
Inderjit Dhillon: And so on. Okay, so there are many ratings. Okay, over here.


64
00:09:19.168 --> 00:09:23.478
Inderjit Dhillon: And the question is, suppose there is a


65
00:09:24.228 --> 00:09:30.387
Inderjit Dhillon: particular user movie pair. What is its rating? So yes, it's easy. If


66
00:09:30.638 --> 00:09:33.938
Inderjit Dhillon: this user has already rated this particular movie.


67
00:09:34.048 --> 00:09:39.207
Inderjit Dhillon: but otherwise you have to try to infer. If these missing members


68
00:09:43.178 --> 00:09:49.817
Inderjit Dhillon: and the goal of you know recommender system might be to increase engagement on their platform. Say what they want to recommend


69
00:09:49.928 --> 00:09:59.018
Inderjit Dhillon: movies that you know you might actually like. So they want to recommend movies where the prediction, let's say, is above 4 or 4.5.


70
00:10:01.898 --> 00:10:06.707
Inderjit Dhillon: Okay, so we have a ratings matrix.


71
00:10:09.548 --> 00:10:11.288
Inderjit Dhillon: Sorry. Wait.


72
00:10:15.508 --> 00:10:17.908
Inderjit Dhillon: ratings, matrix.


73
00:10:19.128 --> 00:10:21.888
Inderjit Dhillon: And let me denote that by R,


74
00:10:22.958 --> 00:10:27.237
Inderjit Dhillon: okay? And so I have this matrix. Now, which is R,


75
00:10:28.898 --> 00:10:31.617
Inderjit Dhillon: notice that it's actually going to be very sparse.


76
00:10:33.108 --> 00:10:38.318
Inderjit Dhillon: Okay, if you think about all possible user movie pairs. It's m times, n.


77
00:10:39.258 --> 00:10:48.007
Inderjit Dhillon: but a pretty typical user might only rate, you know, tens of movies or hundreds of movies, or maybe even 1,000, if they really like rating right?


78
00:10:48.328 --> 00:10:52.128
Inderjit Dhillon: But the number of movies might be, you know, in the millions.


79
00:10:52.678 --> 00:11:03.187
Inderjit Dhillon: So this matrix is going to be actually extremely sparse. Okay, with many. And when I say, Miss Sparse, that means there are many, many missing values.


80
00:11:07.828 --> 00:11:13.287
Inderjit Dhillon: Again, 99% or more of this matrix may be missing values.


81
00:11:13.568 --> 00:11:15.348
Inderjit Dhillon: So the goal over here.


82
00:11:15.598 --> 00:11:17.197
Inderjit Dhillon: Okay, the goal.


83
00:11:17.838 --> 00:11:23.648
Inderjit Dhillon: Okay? And now you can start seeing parallels with the regression problem that we've talked about so far.


84
00:11:23.898 --> 00:11:26.588
Inderjit Dhillon: Okay, is that we are given ratings.


85
00:11:30.698 --> 00:11:34.088
Inderjit Dhillon: Okay, we want to predict the missing values.


86
00:11:41.648 --> 00:11:44.548
Inderjit Dhillon: And these would be the actual recommendations.


87
00:11:44.998 --> 00:11:51.987
Inderjit Dhillon: So that's the problem that we will be looking at. Okay, predict any kind of arbitrary missing value


88
00:11:52.788 --> 00:12:05.318
Inderjit Dhillon: in real life situation. What you really want is you want to predict the top K, right? Maybe the top 10 or top 20 movies that a new that a particular user might like that they have not seen.


89
00:12:05.618 --> 00:12:11.967
Inderjit Dhillon: Okay? So that problem is similar, but actually slightly different, because you don't care whether


90
00:12:12.818 --> 00:12:21.928
Inderjit Dhillon: a user is going to rate a movie 1.5 versus 1.7 right? Because they don't like those movies, so you would never really surface them. Okay.


91
00:12:23.468 --> 00:12:31.432
Inderjit Dhillon: But now let's look at it. So any thoughts about how you you know, this is like a classical recommendation problem, any thoughts on how you might


92
00:12:31.878 --> 00:12:36.978
Inderjit Dhillon: end up solving this problem like, how do you guys, you know, try to figure out


93
00:12:37.208 --> 00:12:39.178
Inderjit Dhillon: what movies to watch on Netflix.


94
00:12:40.878 --> 00:12:44.058
Inderjit Dhillon: I guess Netflix already gives a recommendation. Right? So.


95
00:12:44.578 --> 00:12:46.837
Inderjit Dhillon: But how would you think about solving this problem?


96
00:12:50.588 --> 00:12:53.508
Inderjit Dhillon: Suppose you are only given this particular matrix.


97
00:12:57.648 --> 00:13:00.668
Inderjit Dhillon: Oh, there's something on the chat. Let me show. Sorry.


98
00:13:04.318 --> 00:13:08.388
Inderjit Dhillon: Okay, so Rishup said. Find users who are similar to others


99
00:13:08.538 --> 00:13:13.748
Inderjit Dhillon: and then fill in the ratings based on the users. Other users, average ratings.


100
00:13:13.858 --> 00:13:31.797
Inderjit Dhillon: So great. Right? Actually, you know, some of the initial 1st work on collaborative filtering was based on this Knn kind of approach, K. Being Knn being K nearest neighbors. Right? So you find out neighbors right who are similar to you.


101
00:13:31.948 --> 00:13:36.818
Inderjit Dhillon: and then you use their ratings to? Oh.


102
00:13:37.348 --> 00:13:57.627
Inderjit Dhillon: infer these ratings. So that is, you know, a fine method. And today we will look at, you know, if you. If you, if we later on look at it, we'll see that it has certain similarities. But it's actually a little bit more of a global method. Okay, so let me just put what Bishop kind of suggested right? So initial


103
00:13:57.938 --> 00:14:01.568
Inderjit Dhillon: methods. And these were actually some of the methods that were used


104
00:14:01.728 --> 00:14:12.568
Inderjit Dhillon: in the, I want to say late 19 nineties, right? When Amazon, for example, was coming up. And if you look, there are certain papers on recommendation systems


105
00:14:12.718 --> 00:14:15.598
Inderjit Dhillon: over, you know how to recommend


106
00:14:16.237 --> 00:14:22.207
Inderjit Dhillon: items to users who, you know, may have already bought some things. Okay?


107
00:14:22.358 --> 00:14:34.808
Inderjit Dhillon: So the idea is, you look at neighbors of a user.


108
00:14:35.078 --> 00:14:44.008
Inderjit Dhillon: And when I say neighbors, I mean people who have similar ratings.


109
00:14:48.978 --> 00:14:57.118
Hormoz Shahrzad: I guess, even for today, when you buy something from Amazon, it shows you a list of you know, people that buy this also buy those.


110
00:14:57.518 --> 00:14:59.998
Hormoz Shahrzad: But and actually, those have.


111
00:15:00.208 --> 00:15:05.007
Inderjit Dhillon: You know, the the kinds of methods that I'm talking about are actually used over there.


112
00:15:06.978 --> 00:15:10.417
Inderjit Dhillon: does that? Does that? Was that a question, Hormuz, or you were just.


113
00:15:10.418 --> 00:15:11.897
Hormoz Shahrzad: No, no, I'm just.


114
00:15:12.698 --> 00:15:15.808
Hormoz Shahrzad: I'm just going with that, you know. Method that.


115
00:15:16.028 --> 00:15:16.638
Inderjit Dhillon: No.


116
00:15:16.638 --> 00:15:22.327
Hormoz Shahrzad: Don't mention, you know, and you mentioned that on Amazon was using it, and said.


117
00:15:22.328 --> 00:15:22.878
Inderjit Dhillon: Oh, yeah.


118
00:15:22.878 --> 00:15:26.487
Hormoz Shahrzad: Still see that they're using something similar.


119
00:15:26.488 --> 00:15:29.369
Inderjit Dhillon: Oh, yeah, yeah. And it's actually a big driver of


120
00:15:30.218 --> 00:15:34.777
Inderjit Dhillon: of their products, right? Because it basically comes up to how people discover.


121
00:15:35.228 --> 00:15:42.738
Inderjit Dhillon: Maybe things which are new to them. They might not have seen before, or things which you know have high ratings or people have liked.


122
00:15:42.938 --> 00:15:43.638
Inderjit Dhillon: Okay?


123
00:15:43.888 --> 00:15:56.978
Inderjit Dhillon: So. Yes, definitely. I did not mean to say that. You know they were using them, and they don't use them. But I just mean to say that they were. Actually, you know, one of the 1st companies that needed those recommendations. Right? Be at a large scale.


124
00:15:58.708 --> 00:16:06.308
Inderjit Dhillon: Okay? So what we will study is, you know, I talked about matrix completion or matrix factorization.


125
00:16:07.408 --> 00:16:12.677
Inderjit Dhillon: And this is the problem. By the way, you are getting in your homework, right? So we are giving you a data set.


126
00:16:12.898 --> 00:16:17.898
Inderjit Dhillon: and you will basically implement this procedure.


127
00:16:20.698 --> 00:16:23.517
Inderjit Dhillon: And you'll see that there are some subtleties that will come up


128
00:16:23.788 --> 00:16:25.427
Inderjit Dhillon: when you try to implement it.


129
00:16:26.308 --> 00:16:33.628
Inderjit Dhillon: Okay, so here is the matrix, or.


130
00:16:35.438 --> 00:16:41.698
Inderjit Dhillon: okay, suppose it has. You know, I throw.


131
00:16:50.428 --> 00:16:52.687
Inderjit Dhillon: And this is the jet column.


132
00:16:55.788 --> 00:17:01.758
Inderjit Dhillon: Remember, this matrix is users by movies.


133
00:17:03.048 --> 00:17:09.788
Inderjit Dhillon: M of users and N movies. Okay, this is the matrix r, and suppose


134
00:17:09.958 --> 00:17:14.327
Inderjit Dhillon: you know, if I look at the jets movie, suppose it has been rated by a few users.


135
00:17:16.088 --> 00:17:20.158
Inderjit Dhillon: And suppose I look at the Iet user and they have rated some movies.


136
00:17:21.238 --> 00:17:25.117
Inderjit Dhillon: And what I would like to find out is, what is this value?


137
00:17:26.068 --> 00:17:36.198
Inderjit Dhillon: Okay? And that is, Rij, okay? And Rij, might, the Ih user might not have rated the jet mode.


138
00:17:37.548 --> 00:17:47.357
Inderjit Dhillon: So the model that we use the model. Actually, that was quite successful in this competition, right? The winning team actually used that as one of the components was the following right


139
00:17:47.518 --> 00:17:53.568
Inderjit Dhillon: that I'm going to represent it. So the idea is kind of similar to the singular value decomposition.


140
00:17:54.898 --> 00:17:57.248
Inderjit Dhillon: But there'll be differences that I'll talk about.


141
00:17:58.908 --> 00:18:04.668
Inderjit Dhillon: Then I'm going to say that I'm going to factor this as you M transpose.


142
00:18:11.818 --> 00:18:24.938
Inderjit Dhillon: Okay? And this matrix U is M Mike K, okay, bye. And okay.


143
00:18:25.278 --> 00:18:32.348
Inderjit Dhillon: And the model is the following right? And K is typically going to be much smaller than


144
00:18:33.978 --> 00:18:38.228
Inderjit Dhillon: Mn, so much smaller than men of Mns.


145
00:18:38.908 --> 00:18:43.387
Inderjit Dhillon: okay. And the model is that if I if I I just said that there is a


146
00:18:43.828 --> 00:18:50.298
Inderjit Dhillon: approximate sign over here so approximate equality. So what you're doing is you're saying that if I look at


147
00:18:50.638 --> 00:18:54.698
Inderjit Dhillon: this overall ratings matrix, I can actually factor it.


148
00:18:54.868 --> 00:18:58.517
Inderjit Dhillon: and that there are latent factors. There are key latent factors


149
00:18:59.698 --> 00:19:04.898
Inderjit Dhillon: that I can factor it as a product of the user matrix and a movie matrix.


150
00:19:05.948 --> 00:19:15.827
Inderjit Dhillon: And in particular, a rating Rij is actually going to be given by the product of the


151
00:19:17.028 --> 00:19:32.628
Inderjit Dhillon: Ith row of U. So let's call it Ui transpose, and the Jf. Column, which is Mj. Transports.


152
00:19:36.828 --> 00:19:40.618
Inderjit Dhillon: So my model is the following, that Rij.


153
00:19:40.998 --> 00:19:46.673
Inderjit Dhillon: yes, again. It's a simple model, right? I'm not saying that this is exactly what different


154
00:19:47.723 --> 00:19:51.818
Inderjit Dhillon: places use currently. But it's it's


155
00:19:52.648 --> 00:19:58.807
Inderjit Dhillon: but it was actually quite. It's quite successful. So I don't want to say that it is very, very simplistic. Okay, so it is.


156
00:19:59.428 --> 00:20:09.688
Inderjit Dhillon: Ui transpose times. Mj, okay, and you should convince yourself that U of M transpose


157
00:20:09.888 --> 00:20:12.787
Inderjit Dhillon: is a bank K matrix.


158
00:20:17.618 --> 00:20:20.697
Inderjit Dhillon: Okay, so that's the model. That's that R


159
00:20:21.198 --> 00:20:25.398
Inderjit Dhillon: is approximately equal to transfers.


160
00:20:28.298 --> 00:20:34.710
Inderjit Dhillon: Okay? And let me just kind of. I've already written it. But you know, this was


161
00:20:35.548 --> 00:20:49.248
Inderjit Dhillon: I enter J. Column of M. Okay, so row I of you is ui transpose?


162
00:20:50.088 --> 00:20:52.788
Inderjit Dhillon: Okay? So you can think of it as


163
00:20:53.268 --> 00:20:56.747
Inderjit Dhillon: it is, a low, dimensional representation


164
00:20:57.698 --> 00:21:06.998
Inderjit Dhillon: of the I introduced so low, dimensional, and what is it? K.


165
00:21:07.488 --> 00:21:14.048
Inderjit Dhillon: K. Dimensional representation of user.


166
00:21:14.158 --> 00:21:15.048
Inderjit Dhillon: I.


167
00:21:16.548 --> 00:21:20.527
Inderjit Dhillon: Okay, similarly, if I look at this is M transpose


168
00:21:21.038 --> 00:21:32.068
Inderjit Dhillon: M transpose. So I can say, that row J of matrix M is Mj, transpose.


169
00:21:33.488 --> 00:21:36.657
Inderjit Dhillon: Okay. And this is the low dimensional.


170
00:21:42.938 --> 00:21:46.728
Hormoz Shahrzad: Excuse me, sit there 3, for the same color.


171
00:21:47.388 --> 00:21:48.278
Inderjit Dhillon: What's that?


172
00:21:48.858 --> 00:21:51.867
Hormoz Shahrzad: The J. Draw of the J. Column of the M.


173
00:21:51.868 --> 00:21:57.617
Inderjit Dhillon: No, no. So remember that I so just keep in mind the notation I'm using. This is M. Transpose.


174
00:21:58.728 --> 00:21:59.148
Hormoz Shahrzad: Got it.


175
00:21:59.148 --> 00:22:01.678
Inderjit Dhillon: And over here, I said, row J. Of M.


176
00:22:03.838 --> 00:22:05.487
Inderjit Dhillon: Is that clear? Harmus?


177
00:22:05.508 --> 00:22:06.537
Hormoz Shahrzad: Yeah, yes. Thank you.


178
00:22:06.538 --> 00:22:08.079
Inderjit Dhillon: Yeah. Yeah. So I'm just.


179
00:22:08.618 --> 00:22:17.807
Inderjit Dhillon: yeah. But it is exactly this. You know, this particular column of M transpose right? So row J, of M is the same as column J of M. Transpose.


180
00:22:18.438 --> 00:22:19.398
Inderjit Dhillon: Okay.


181
00:22:20.208 --> 00:22:28.158
Inderjit Dhillon: And in particular, like I said, if I look at columns, I call


182
00:22:36.538 --> 00:22:47.637
Inderjit Dhillon: columns of you columns of M. So if I look at columns of U


183
00:22:48.088 --> 00:22:52.017
Inderjit Dhillon: right? These you can think of as the different latent factors.


184
00:22:57.918 --> 00:22:58.808
Inderjit Dhillon: Right?


185
00:22:59.188 --> 00:23:07.807
Inderjit Dhillon: And that is feet. You can think of these as some features, some extracted features.


186
00:23:08.158 --> 00:23:09.877
Inderjit Dhillon: Okay, 4 users.


187
00:23:13.168 --> 00:23:17.868
Inderjit Dhillon: Okay? And these are latent factors.


188
00:23:22.348 --> 00:23:33.427
Inderjit Dhillon: features 4 movies, you know. And if I actually remember. You know there are.


189
00:23:33.558 --> 00:23:43.278
Inderjit Dhillon: because, you know, it's not just for Netflix, but I believe it was also used for dating websites like.


190
00:23:43.418 --> 00:23:47.137
Inderjit Dhillon: I think eharmony even used to advertise


191
00:23:47.688 --> 00:24:11.008
Inderjit Dhillon: that they had, like a 30 dimensional representation that they could build. They did not say that in exactly these terms, in their commercial. But they would say that they basically extract out 30 factors for making sure that the people they matched up with were compatible with each other. Okay. So another example


192
00:24:11.038 --> 00:24:20.147
Inderjit Dhillon: where you have, you know, in some sense, matchmaking, you're trying to match users and movies or pairs of people together.


193
00:24:25.728 --> 00:24:30.017
Inderjit Dhillon: Okay? So now, the question, of course, is that this is the model.


194
00:24:30.658 --> 00:24:34.067
Inderjit Dhillon: How do you actually find the uis and the Mjs.


195
00:24:44.348 --> 00:24:48.338
Inderjit Dhillon: okay, remember that I have that. RIJ.


196
00:24:48.818 --> 00:24:53.968
Inderjit Dhillon: I want to be approximately equal to Ui transpose. Mj.


197
00:24:54.808 --> 00:24:58.107
Inderjit Dhillon: so any thoughts on how I would find use and M's.


198
00:25:04.948 --> 00:25:06.708
Inderjit Dhillon: so I'm given the hours.


199
00:25:08.828 --> 00:25:10.978
Inderjit Dhillon: But remember, this matrix is sparse.


200
00:25:12.938 --> 00:25:16.357
Inderjit Dhillon: These matrices are actually going to be dense, right?


201
00:25:17.588 --> 00:25:19.128
Inderjit Dhillon: And given this.


202
00:25:20.468 --> 00:25:25.837
Inderjit Dhillon: That's all the information I have. There may be lots and lots of users. There's lots and lots of movies


203
00:25:26.118 --> 00:25:28.238
Inderjit Dhillon: from that. I want to get this.


204
00:25:29.438 --> 00:25:33.398
Hormoz Shahrzad: It sounds like it partially. A CD, right? Okay.


205
00:25:33.398 --> 00:25:36.677
Inderjit Dhillon: Right? So you know, this is a low rank matrix.


206
00:25:37.058 --> 00:25:39.828
Inderjit Dhillon: We talked about the key truncated Svd.


207
00:25:40.318 --> 00:25:45.628
Inderjit Dhillon: so it sounds actually related to an Svd. Right? And one of the things that the Svd does


208
00:25:45.778 --> 00:25:56.768
Inderjit Dhillon: is it uses, you know, the squared loss right? Or the square, you know, least squares the loss that comes up in least squares regression, right? So we could.


209
00:25:57.258 --> 00:26:00.627
Inderjit Dhillon: And that's a natural thing to do is that we look at


210
00:26:00.968 --> 00:26:05.047
Inderjit Dhillon: a loss function which is the quadratic loss squared loss.


211
00:26:06.368 --> 00:26:08.407
Inderjit Dhillon: So when I say that ri


212
00:26:08.688 --> 00:26:13.478
Inderjit Dhillon: j is nearly ui transpose Mj, then I can say that.


213
00:26:14.618 --> 00:26:17.958
Inderjit Dhillon: you know. Let me look at this, and somehow.


214
00:26:18.748 --> 00:26:22.227
Inderjit Dhillon: you know, I can summish some overall. Ij.


215
00:26:23.518 --> 00:26:28.408
Inderjit Dhillon: and then I can try to minimize this


216
00:26:28.818 --> 00:26:34.158
Inderjit Dhillon: over all the use. Now, how many use are there? u, 1 u, 2


217
00:26:34.628 --> 00:26:43.678
Inderjit Dhillon: to your M. There are M. Users, and then I have movies m, 1 m. 2.


218
00:26:53.848 --> 00:26:56.857
Inderjit Dhillon: Oh, so is this solution. Is it just given by?


219
00:26:57.698 --> 00:27:05.328
Inderjit Dhillon: Because I can write this as in matrix notation, no need for the sum


220
00:27:06.438 --> 00:27:14.918
Inderjit Dhillon: right? We learned about norms. Right? So I I can write this as R minus transpose


221
00:27:18.008 --> 00:27:19.847
Inderjit Dhillon: Flobinius Norm Square.


222
00:27:20.528 --> 00:27:21.637
Inderjit Dhillon: Is that correct?


223
00:27:23.898 --> 00:27:24.578
Hormoz Shahrzad: Yes.


224
00:27:27.438 --> 00:27:31.958
Inderjit Dhillon: Also do I just then can I just use the Svd then to solve it?


225
00:27:32.568 --> 00:27:41.188
Inderjit Dhillon: Because, remember, I want to minimize over you and M.


226
00:27:43.288 --> 00:27:50.748
Hormoz Shahrzad: Yeah. But what do we put for the missing values? Do we just put 0 in there.


227
00:27:50.748 --> 00:27:56.297
Inderjit Dhillon: Aha, right? So that's the the critical difference or importance.


228
00:27:56.518 --> 00:28:02.258
Inderjit Dhillon: Okay, that this objective. Thank you. Hormos. Yeah, that's exactly the point


229
00:28:02.588 --> 00:28:06.397
Inderjit Dhillon: that this IJ. Is not over all M. And N.


230
00:28:08.508 --> 00:28:17.018
Inderjit Dhillon: If I knew all rij from I equal to one to M. Or J. Equal to one to M. Which is that all users have rated all the movies and so on.


231
00:28:17.138 --> 00:28:19.357
Inderjit Dhillon: right? Which is, of course, unrealistic.


232
00:28:19.578 --> 00:28:23.788
Inderjit Dhillon: Then this problem would be solved by the Svd. But that's not the given problem.


233
00:28:24.498 --> 00:28:33.068
Inderjit Dhillon: The problem is that you only know certain indices. So let me say that K is


234
00:28:34.038 --> 00:28:39.867
Inderjit Dhillon: the set of all known ratings.


235
00:28:43.378 --> 00:28:51.878
Inderjit Dhillon: Okay, so K, is all the pairs, IJ, such that all Id is known.


236
00:28:53.448 --> 00:28:56.777
Inderjit Dhillon: And so I'm trying to fit the training data.


237
00:28:57.258 --> 00:29:05.168
Inderjit Dhillon: the model to the training data. So this is over all pairs that are known.


238
00:29:06.028 --> 00:29:08.288
Inderjit Dhillon: So this is what is this race?


239
00:29:08.938 --> 00:29:12.987
Inderjit Dhillon: The important difference, right, which makes this problem very different


240
00:29:13.188 --> 00:29:19.637
Inderjit Dhillon: than the traditional singular value decomposition form. So this is our objective.


241
00:29:22.848 --> 00:29:23.718
Inderjit Dhillon: Okay?


242
00:29:24.718 --> 00:29:28.638
Inderjit Dhillon: And I can write this in matrix form as follows.


243
00:29:29.648 --> 00:29:34.898
Inderjit Dhillon: that I can write this as W.


244
00:29:35.418 --> 00:29:37.958
Inderjit Dhillon: Element wise dot.


245
00:29:41.028 --> 00:29:47.178
Inderjit Dhillon: Where this is the this matrix WIJ.


246
00:29:47.288 --> 00:29:52.218
Inderjit Dhillon: This matrix W has entries WIJ. Which is one.


247
00:29:52.988 --> 00:30:01.208
Inderjit Dhillon: If IJ belongs to K, and remember, K is the known ratings.


248
00:30:06.008 --> 00:30:07.877
Inderjit Dhillon: or is 0.


249
00:30:12.548 --> 00:30:18.967
Inderjit Dhillon: and.is the element wise product.


250
00:30:25.088 --> 00:30:30.738
Inderjit Dhillon: Okay? And that also has the name called automart product.


251
00:30:36.268 --> 00:30:43.218
Inderjit Dhillon: Okay, so is the, is it clear so far what I've talked about any questions.


252
00:30:46.158 --> 00:30:49.338
Inderjit Dhillon: So this is our model transpose.


253
00:30:49.648 --> 00:30:52.087
Inderjit Dhillon: And I'm trying to fit it to the training data.


254
00:30:52.598 --> 00:30:56.888
Inderjit Dhillon: And the trading data is exactly this, Ij belong to K,


255
00:31:04.808 --> 00:31:10.528
Inderjit Dhillon: and then, you know, we've talked a little bit about regularization. Right?


256
00:31:10.838 --> 00:31:17.338
Inderjit Dhillon: So typically we do want to see regularization. And and you'll see as you


257
00:31:17.468 --> 00:31:23.578
Inderjit Dhillon: do the homework, that regularization is very important. Also, right? So the regularization we'll do is


258
00:31:23.778 --> 00:31:31.228
Inderjit Dhillon: Ridge regularization, and that is plus lambda summation of Ui


259
00:31:32.458 --> 00:31:36.168
Inderjit Dhillon: square. I is one through M.


260
00:31:37.128 --> 00:31:39.658
Inderjit Dhillon: Plus oh, sorry


261
00:31:43.238 --> 00:31:47.398
Inderjit Dhillon: plus lambda summation. I equals one through N.


262
00:31:49.888 --> 00:31:56.317
Inderjit Dhillon: Mj. Squared 2 months. Okay? And I can write that as lambda


263
00:31:57.658 --> 00:32:01.737
Inderjit Dhillon: use for minus non square plus lambda.


264
00:32:03.818 --> 00:32:08.937
Inderjit Dhillon: And okay, so this is adding.


265
00:32:09.488 --> 00:32:13.077
Hormoz Shahrzad: Sorry do we use same lambda for U. And M.


266
00:32:14.075 --> 00:32:19.330
Inderjit Dhillon: You could. You could try different lambdas, too, so that that


267
00:32:20.404 --> 00:32:23.457
Inderjit Dhillon: you know, you could do experiments with


268
00:32:24.247 --> 00:32:30.768
Inderjit Dhillon: I forget in the homework, whether we have same lambdas or different Lambdas in the homework. We've given you the same Lambdas.


269
00:32:31.787 --> 00:32:38.048
Inderjit Dhillon: I think, when people tried it, it did not make too much difference whether they use the same lambdas or not.


270
00:32:39.248 --> 00:32:42.448
Inderjit Dhillon: Okay, you could actually also end up using, you know,


271
00:32:44.378 --> 00:32:53.317
Inderjit Dhillon: different sets of lambdas, right? Users who are rated more movies versus users who had rated less movies. But right now we'll just assume


272
00:32:53.518 --> 00:32:55.737
Inderjit Dhillon: that the lambda is the same.


273
00:32:55.888 --> 00:32:57.328
Inderjit Dhillon: Does that answer your question?


274
00:32:57.558 --> 00:32:58.537
Hormoz Shahrzad: Yes, thank you.


275
00:32:58.888 --> 00:33:08.178
Inderjit Dhillon: Okay, okay. So now remember that that you know, if my problem right?


276
00:33:08.918 --> 00:33:18.817
Inderjit Dhillon: If okay, is the set of all. Ij.


277
00:33:26.038 --> 00:33:30.068
Inderjit Dhillon: okay, then the problem would be of the following form, summation of


278
00:33:30.428 --> 00:33:36.188
Inderjit Dhillon: I equals one through M. Summation of J equals one through N.


279
00:33:36.508 --> 00:33:43.218
Inderjit Dhillon: RIJ. Minus ui, transpose Mj square.


280
00:33:45.508 --> 00:33:46.578
Inderjit Dhillon: Okay?


281
00:33:48.578 --> 00:33:59.881
Inderjit Dhillon: And again, minimize overuse and M, and that would be equivalent 2, minimize


282
00:34:02.998 --> 00:34:05.958
Inderjit Dhillon: R, minus transpose.


283
00:34:08.208 --> 00:34:09.108
Inderjit Dhillon: Okay?


284
00:34:09.388 --> 00:34:11.118
Inderjit Dhillon: And the solution


285
00:34:16.058 --> 00:34:25.678
Inderjit Dhillon: would be given bye, the okay truncated.


286
00:34:28.758 --> 00:34:29.808
Inderjit Dhillon: That's good.


287
00:34:31.758 --> 00:34:39.648
Inderjit Dhillon: Okay. But as I have said that, and let me just write it to reinforce it right. But


288
00:34:40.378 --> 00:34:41.908
Inderjit Dhillon: in our case


289
00:34:45.338 --> 00:34:57.598
Inderjit Dhillon: we only no ratings that belong to a subset of all possible pairs. Right? So, ij.


290
00:34:58.508 --> 00:35:00.998
Inderjit Dhillon: that belongs to a given okay?


291
00:35:02.538 --> 00:35:07.898
Inderjit Dhillon: So again, to emphasize, we do not no.


292
00:35:09.258 --> 00:35:15.978
Inderjit Dhillon: or RIJ. For all I do from here.


293
00:35:18.508 --> 00:35:26.618
Inderjit Dhillon: Eyes one to M. And J. Is 1, 2.


294
00:35:28.608 --> 00:35:34.908
Inderjit Dhillon: Okay, so we cannot use the procedure to compute the Svd.


295
00:35:35.178 --> 00:35:44.248
Inderjit Dhillon: Now, that's another also thing that we haven't really studied is, how do you actually given a matrix? How do you actually compute it? Says, feeding. It's actually not trivial.


296
00:35:44.878 --> 00:35:50.078
Inderjit Dhillon: right? But we know that you open up python, or you open up numpy


297
00:35:50.258 --> 00:35:53.678
Inderjit Dhillon: right? You will find a procedure to find the Svd.


298
00:35:54.398 --> 00:35:56.208
Inderjit Dhillon: But we cannot use that procedure.


299
00:35:57.588 --> 00:36:01.078
Inderjit Dhillon: So any thoughts on how we might go about solving this problem.


300
00:36:06.298 --> 00:36:08.698
Inderjit Dhillon: we need to solve this problem over here.


301
00:36:20.858 --> 00:36:26.968
Inderjit Dhillon: I kind of given you a little bit of a hint. I've talked a little bit about regularization. I've mentioned regression to you.


302
00:36:27.608 --> 00:36:29.968
Inderjit Dhillon: So does this look like a regression problem.


303
00:36:32.178 --> 00:36:38.828
Hormoz Shahrzad: It is, I mean, should be solvable with some gradient based method.


304
00:36:39.108 --> 00:36:39.848
Hormoz Shahrzad: Hey?


305
00:36:40.268 --> 00:36:45.288
Inderjit Dhillon: Yeah, yeah, remember, what are the variables that we are solving for?


306
00:36:46.678 --> 00:36:49.268
Inderjit Dhillon: It's U and M,


307
00:36:57.508 --> 00:37:04.838
Inderjit Dhillon: okay, so there's something that we can do. We can use the procedure called alternating minimization.


308
00:37:16.028 --> 00:37:16.968
Inderjit Dhillon: Okay?


309
00:37:18.658 --> 00:37:25.748
Inderjit Dhillon: And what this procedure does is the following, right? So it 1st does fix.


310
00:37:26.568 --> 00:37:30.017
Inderjit Dhillon: So it starts with some. You. Maybe I should just say that.


311
00:37:31.708 --> 00:37:37.448
Inderjit Dhillon: Okay, start with some.


312
00:37:38.328 --> 00:37:40.668
Inderjit Dhillon: You as a guest.


313
00:37:43.878 --> 00:37:56.018
Inderjit Dhillon: Okay, now, given this U right fix you and compute. M


314
00:38:01.038 --> 00:38:05.778
Inderjit Dhillon: okay, now think a little bit about this right? Suppose I'm given a U


315
00:38:07.018 --> 00:38:11.258
Inderjit Dhillon: and I'm trying to do this problem right.


316
00:38:13.698 --> 00:38:19.598
Inderjit Dhillon: And what you can do is you can look at only one column of R.


317
00:38:22.258 --> 00:38:23.228
Inderjit Dhillon: Right?


318
00:38:23.618 --> 00:38:28.778
Inderjit Dhillon: Then you'll have that the I-th column or the let's say the Jth column of R


319
00:38:29.768 --> 00:38:32.398
Inderjit Dhillon: is equal to you. Know your fixed U


320
00:38:32.888 --> 00:38:36.048
Inderjit Dhillon: and the Jth column of M transpose


321
00:38:39.708 --> 00:38:40.888
Inderjit Dhillon: and square.


322
00:38:43.298 --> 00:38:46.117
Inderjit Dhillon: Okay? And of course, there's a regularization also.


323
00:38:46.318 --> 00:38:48.787
Inderjit Dhillon: But my question is, can you solve that problem?


324
00:38:49.858 --> 00:38:51.088
Inderjit Dhillon: Fix you.


325
00:38:51.978 --> 00:38:55.518
Hormoz Shahrzad: And that's a that's a normal aggression problem. Right?


326
00:38:55.518 --> 00:39:00.197
Inderjit Dhillon: So that's least squares regression. That's exactly what we've studied so far.


327
00:39:00.808 --> 00:39:01.648
Inderjit Dhillon: Right?


328
00:39:01.858 --> 00:39:06.818
Inderjit Dhillon: So we let me come to that. But the algorithm is actually going to be this right?


329
00:39:07.078 --> 00:39:21.148
Inderjit Dhillon: Start with some U fix U and compute m right, and then fix m and compute you.


330
00:39:21.848 --> 00:39:25.258
Inderjit Dhillon: Okay, and and repeat.


331
00:39:27.498 --> 00:39:29.018
Inderjit Dhillon: And I suppose?


332
00:39:30.318 --> 00:39:35.678
Inderjit Dhillon: Capital. K. Times, okay.


333
00:39:35.788 --> 00:39:38.468
Inderjit Dhillon: So you would start with some U naught.


334
00:39:40.768 --> 00:39:44.617
Inderjit Dhillon: You would get m 1 from it.


335
00:39:45.528 --> 00:39:52.498
Inderjit Dhillon: Then you fix m 1. You get u 1 from it. You fix u 1, you get m, 2 from it.


336
00:39:52.868 --> 00:39:54.818
Inderjit Dhillon: You get u 2, and so on.


337
00:39:57.958 --> 00:40:02.417
Inderjit Dhillon: Okay, and remember what MI would be


338
00:40:03.548 --> 00:40:09.128
Inderjit Dhillon: mi would be obtained by a procedure where I'm minimizing over M.


339
00:40:10.278 --> 00:40:18.188
Inderjit Dhillon: R. Minus you. Let's see, I minus one.


340
00:40:18.958 --> 00:40:19.758
Hormoz Shahrzad: Yes.


341
00:40:19.988 --> 00:40:25.837
Inderjit Dhillon: Times, M transpose familiar storm square.


342
00:40:26.398 --> 00:40:28.598
Inderjit Dhillon: Okay, plus lambda.


343
00:40:28.918 --> 00:40:31.257
Inderjit Dhillon: I have this regularization also.


344
00:40:33.178 --> 00:40:35.018
Inderjit Dhillon: Okay, so I'll be solving this problem.


345
00:40:36.638 --> 00:40:37.998
Inderjit Dhillon: And am I


346
00:40:44.688 --> 00:40:50.418
Inderjit Dhillon: am, I would be the minimizer or argument of this one.


347
00:40:52.828 --> 00:40:53.738
Inderjit Dhillon: Okay?


348
00:40:53.978 --> 00:41:02.498
Inderjit Dhillon: And similarly, Ui would be the arc men over you.


349
00:41:03.838 --> 00:41:14.358
Inderjit Dhillon: R minus UMI transpose for minus non square plus lambda.


350
00:41:16.348 --> 00:41:17.258
Inderjit Dhillon: You?


351
00:41:21.808 --> 00:41:30.701
Inderjit Dhillon: Okay? So let's let's and and let's look in detail at what happens. Okay, let's fix


352
00:41:34.238 --> 00:41:41.048
Inderjit Dhillon: Well, if I have R is nearly equal to transpose.


353
00:41:42.318 --> 00:41:51.977
Inderjit Dhillon: let me flip. This around R. Transpose is nearly equal to MU. Transpose, because let's see


354
00:41:56.098 --> 00:42:00.198
Inderjit Dhillon: because I think I'll just fix M 1st and then compute use.


355
00:42:00.468 --> 00:42:04.547
Inderjit Dhillon: Okay, I guess I guess I could do either one of them. Okay.


356
00:42:04.868 --> 00:42:08.148
Inderjit Dhillon: But right now suppose we fix M.


357
00:42:13.568 --> 00:42:18.527
Inderjit Dhillon: The other case would be very analogous. You would just be working with the transporters. Okay.


358
00:42:18.658 --> 00:42:21.008
Inderjit Dhillon: so let me just draw this out.


359
00:42:24.738 --> 00:42:29.988
Inderjit Dhillon: So this would be the matrix R transpose.


360
00:42:30.288 --> 00:42:36.708
Inderjit Dhillon: Okay, so all transpose is number of movies by the number of users.


361
00:42:37.278 --> 00:42:41.718
Inderjit Dhillon: And I want this to be equal to M,


362
00:42:48.308 --> 00:42:53.837
Inderjit Dhillon: which is n, by K, this is the matrix M, and


363
00:42:58.518 --> 00:43:03.498
Inderjit Dhillon: through transports, okay? And this is him.


364
00:43:13.228 --> 00:43:14.168
Inderjit Dhillon: Okay?


365
00:43:15.088 --> 00:43:25.988
Inderjit Dhillon: And let's look at this is, I had user factor.


366
00:43:31.278 --> 00:43:36.878
Inderjit Dhillon: Ui, okay. And what I'm saying is that


367
00:43:38.418 --> 00:43:41.418
Inderjit Dhillon: I want to look at the id column over here.


368
00:43:48.708 --> 00:43:56.307
Inderjit Dhillon: I have problems. Let me call that as Ri, so I have. RI


369
00:43:57.738 --> 00:44:02.168
Inderjit Dhillon: is nearly equal to M. Times. Ui, is that clear?


370
00:44:05.428 --> 00:44:09.258
Inderjit Dhillon: Okay, Ri, is the Ith column of R transports?


371
00:44:10.188 --> 00:44:12.667
Inderjit Dhillon: Okay? So what I can try to do is


372
00:44:14.088 --> 00:44:17.988
Inderjit Dhillon: RI, minus m times, ui


373
00:44:21.058 --> 00:44:22.158
Inderjit Dhillon: square.


374
00:44:26.988 --> 00:44:35.048
Inderjit Dhillon: Okay, plus if I add regularization. So remember now, this is like all coefficients. W,


375
00:44:36.328 --> 00:44:41.007
Inderjit Dhillon: this is like my matrix X, and this is like, why.


376
00:44:42.508 --> 00:44:49.827
Inderjit Dhillon: okay, y is replaced by Ri X is replaced by M,


377
00:44:50.638 --> 00:44:55.207
Inderjit Dhillon: and this is like our coefficient. W, okay, so I have this.


378
00:44:58.478 --> 00:45:04.978
Inderjit Dhillon: And I want to try to find out the minimum over. Ui.


379
00:45:08.978 --> 00:45:11.087
Inderjit Dhillon: so is this the problem we want to solve.


380
00:45:12.628 --> 00:45:15.517
Inderjit Dhillon: This looks exactly like a red regression problem.


381
00:45:32.518 --> 00:45:37.528
Inderjit Dhillon: And then I can basically solve it for nei right?


382
00:45:43.548 --> 00:45:45.218
Inderjit Dhillon: What is the mistake that I made?


383
00:45:50.898 --> 00:45:54.017
Inderjit Dhillon: So remember, Ri is the entire Ia column.


384
00:45:55.118 --> 00:45:57.178
Inderjit Dhillon: Do we know the entire Ith column.


385
00:45:58.578 --> 00:46:00.058
Hormoz Shahrzad: No, we.


386
00:46:01.278 --> 00:46:02.268
Inderjit Dhillon: We don't right.


387
00:46:02.268 --> 00:46:04.727
Hormoz Shahrzad: How's the w thing in there.


388
00:46:04.728 --> 00:46:06.908
Inderjit Dhillon: We only know certain elements.


389
00:46:09.418 --> 00:46:14.708
Inderjit Dhillon: Okay, so in particular, so


390
00:46:16.528 --> 00:46:27.688
Inderjit Dhillon: in particular, if the you know, we have what M users and n, movies. So if this particular user has only rated a few movies


391
00:46:28.368 --> 00:46:29.308
Inderjit Dhillon: right.


392
00:46:29.658 --> 00:46:30.948
Hormoz Shahrzad: Right, then.


393
00:46:31.518 --> 00:46:35.957
Inderjit Dhillon: You know, we don't know that entire column. So this is actually kind of incorrect.


394
00:46:45.408 --> 00:46:53.468
Inderjit Dhillon: Okay, so what we do need to know is we need to concentrate only on these values.


395
00:46:57.718 --> 00:47:00.188
Inderjit Dhillon: These are the Y values that I know.


396
00:47:01.008 --> 00:47:06.017
Inderjit Dhillon: Okay, so what we will say is that we'll look at the index set.


397
00:47:12.708 --> 00:47:25.378
Inderjit Dhillon: and I'll call it I no, not the best notation. But so I subscript, I represents indices


398
00:47:26.938 --> 00:47:36.168
Inderjit Dhillon: of movies rated by user voice.


399
00:47:38.698 --> 00:47:39.668
Inderjit Dhillon: Okay?


400
00:47:40.108 --> 00:47:42.148
Inderjit Dhillon: And so now let's look at what?


401
00:47:42.278 --> 00:47:53.027
Inderjit Dhillon: Oh, what the correct objective is. Right. So let me denote by RI subscript Ri.


402
00:47:53.578 --> 00:47:55.488
Inderjit Dhillon: As these values.


403
00:47:56.488 --> 00:48:00.508
Inderjit Dhillon: So these values over here.


404
00:48:00.808 --> 00:48:05.837
Inderjit Dhillon: This, this, this. So suppose Ii is just these 3 pairs.


405
00:48:06.148 --> 00:48:09.098
Inderjit Dhillon: then this would be a vector. Of length 3.


406
00:48:10.688 --> 00:48:11.648
Inderjit Dhillon: Okay.


407
00:48:11.828 --> 00:48:16.388
Inderjit Dhillon: And remember that this is now going to pick out exactly.


408
00:48:16.708 --> 00:48:17.918
Inderjit Dhillon: Oh,


409
00:48:21.188 --> 00:48:25.908
Inderjit Dhillon: Exactly! The corresponding rows off.


410
00:48:27.368 --> 00:48:28.628
Inderjit Dhillon: Oh.


411
00:48:33.798 --> 00:48:38.837
Inderjit Dhillon: some particular a subset of the matrix.


412
00:48:38.998 --> 00:48:43.118
Inderjit Dhillon: M, so it won't be the entire matrix. M,


413
00:48:44.138 --> 00:48:46.847
Inderjit Dhillon: right? It'll be a subset of the matrix app.


414
00:48:47.628 --> 00:48:53.508
Inderjit Dhillon: Okay, so let me call that as M. Of I I


415
00:48:54.508 --> 00:49:00.208
Inderjit Dhillon: okay. So there'll only be 3 rows of the corresponding matrix.


416
00:49:04.878 --> 00:49:07.988
Inderjit Dhillon: Okay, so, for example, if I have these threes.


417
00:49:08.748 --> 00:49:13.758
Inderjit Dhillon: this will be one. This will be 2, and this will be 3.


418
00:49:15.208 --> 00:49:19.858
Inderjit Dhillon: Okay, so if I put these over here


419
00:49:21.128 --> 00:49:25.478
Inderjit Dhillon: and assemble them into this matrix, this will be M, like.


420
00:49:26.728 --> 00:49:30.557
Inderjit Dhillon: okay. So similarly, if I assemble it over here.


421
00:49:33.188 --> 00:49:41.998
Inderjit Dhillon: Okay, then this is all, bye, and this will be okay.


422
00:49:42.948 --> 00:49:47.748
Inderjit Dhillon: Will be 3 rows if I is of size. 3.


423
00:49:48.118 --> 00:49:52.998
Inderjit Dhillon: And so I have, MI. Times you are.


424
00:49:59.168 --> 00:50:01.478
Inderjit Dhillon: And I still have that regularization.


425
00:50:05.708 --> 00:50:06.638
Inderjit Dhillon: Okay?


426
00:50:07.798 --> 00:50:11.067
Inderjit Dhillon: And you know, I can just put a half over here.


427
00:50:12.048 --> 00:50:13.797
Inderjit Dhillon: And that is my objective.


428
00:50:16.168 --> 00:50:20.918
Inderjit Dhillon: Okay? And let me call this objective as JI.


429
00:50:21.708 --> 00:50:23.857
Inderjit Dhillon: And I basically need to


430
00:50:26.878 --> 00:50:35.358
Inderjit Dhillon: minimize this over your, then any questions.


431
00:50:36.558 --> 00:50:42.837
Hormoz Shahrzad: Excuse me, how do we decide the dimensionality of the latent space, like the K. Here.


432
00:50:43.948 --> 00:50:52.687
Inderjit Dhillon: So that's something that you know we can actually just experiment with. But you can start off with one K, and you can just try different K's and see typically


433
00:50:53.378 --> 00:50:57.979
Inderjit Dhillon: in machine learning, what will happen is that you will be given like


434
00:50:58.838 --> 00:51:01.778
Inderjit Dhillon: training set. And then there is a held out test set.


435
00:51:03.618 --> 00:51:09.258
Inderjit Dhillon: Okay, so you can basically try different values of R and see how they perform on the test set.


436
00:51:09.638 --> 00:51:14.218
Inderjit Dhillon: And typically, you also divide it into training, set validation, set and test, set.


437
00:51:15.198 --> 00:51:15.758
Hormoz Shahrzad: Right.


438
00:51:16.438 --> 00:51:20.598
Inderjit Dhillon: And you basically then try to find out the K or the rank


439
00:51:20.748 --> 00:51:25.657
Inderjit Dhillon: that you know gives you the best performance. So clearly too low a rank


440
00:51:26.048 --> 00:51:30.473
Inderjit Dhillon: would be not good enough. It won't fit the data. It's kind of like that.


441
00:51:31.238 --> 00:51:41.778
Inderjit Dhillon: you know the curve that we showed right that if you have too low, then you'll be underfitting. You have too high, you'll be overfitting, and there'll be some sweet spot.


442
00:51:42.648 --> 00:51:48.187
Inderjit Dhillon: Okay? And you would. You would do the same for other other hyper parameters, such as Lambda, also.


443
00:51:49.458 --> 00:51:50.038
Hormoz Shahrzad: Right.


444
00:51:50.668 --> 00:51:54.748
Inderjit Dhillon: Okay, was there another question?


445
00:51:58.108 --> 00:51:59.018
Inderjit Dhillon: Okay?


446
00:52:00.268 --> 00:52:04.478
Inderjit Dhillon: And then, so this is my objective. And now I can. You know.


447
00:52:04.888 --> 00:52:19.748
Inderjit Dhillon: this is basically a written regression problem. Okay? And this is like a regression problem where? N, capital, n, that we talked about in regression is actually just 3.


448
00:52:20.998 --> 00:52:24.878
Inderjit Dhillon: Okay, so it'll actually give you some really interesting cases where.


449
00:52:25.038 --> 00:52:31.288
Inderjit Dhillon: you know, when we talked about regression, you know, there was a dimensionality, and there was a number of observations d and n.


450
00:52:31.568 --> 00:52:42.298
Inderjit Dhillon: right? Typically in regression problems, we assume that the number of measurements which is N is, you know, greater than d, or much greater than D.


451
00:52:42.408 --> 00:52:51.738
Inderjit Dhillon: And here you'll see that there will be some times when, especially in the Netflix data that you know. Some users only rated a couple of movies.


452
00:52:52.308 --> 00:53:03.307
Inderjit Dhillon: Some movies were only rated by a couple of users. So you actually get the other case also, where the number of measurements is much smaller than the dimensionality, right? Dimensionality is fixed at K,


453
00:53:03.728 --> 00:53:06.563
Inderjit Dhillon: okay, so you will actually run across these


454
00:53:07.138 --> 00:53:21.178
Inderjit Dhillon: issues when you kind of try to solve the problem right? And so, you know, I'll just, I'm actually just redriving it right? But you can, you know, solve this problem as a regression problem where we take the partial derivative


455
00:53:21.438 --> 00:53:27.427
Inderjit Dhillon: with respect to ui right? And what you will get is, you know,


456
00:53:29.118 --> 00:53:32.268
Inderjit Dhillon: this ri minus MI


457
00:53:34.078 --> 00:53:37.687
Inderjit Dhillon: ui, right? You'll basically just get


458
00:53:41.988 --> 00:53:48.238
Inderjit Dhillon: the residual, which is RIII minus MI


459
00:53:49.768 --> 00:53:57.167
Inderjit Dhillon: ui, right. The half will cancel with the 2 right, and then you'll get a minus MI,


460
00:53:57.958 --> 00:54:05.847
Inderjit Dhillon: okay, it's this is basically just the analogous to the normal equations.


461
00:54:06.088 --> 00:54:09.938
Inderjit Dhillon: Okay, so when I set this


462
00:54:14.358 --> 00:54:24.608
Inderjit Dhillon: to be equal to 0, that implies that you have M. Of, I transpose MFI.


463
00:54:28.758 --> 00:54:31.268
Inderjit Dhillon: Good. This should have been a transpose, I guess.


464
00:54:32.008 --> 00:54:47.998
Inderjit Dhillon: Okay, plus lambda I ui is equal to MI transpose times.


465
00:54:50.078 --> 00:54:51.018
Inderjit Dhillon: All right.


466
00:54:54.778 --> 00:55:00.578
Inderjit Dhillon: Okay. And one thing to note is that this will be a K by K matrix.


467
00:55:00.878 --> 00:55:13.708
Inderjit Dhillon: nothing to do with M or n, okay, this will be okay. Times, why I matrix.


468
00:55:14.528 --> 00:55:16.658
Inderjit Dhillon: And this will be our size.


469
00:55:19.888 --> 00:55:23.138
Inderjit Dhillon: Okay, and suppose, now, I have


470
00:55:24.448 --> 00:55:34.248
Inderjit Dhillon: that the size of I is equal to? Ni, okay, so


471
00:55:34.838 --> 00:55:38.808
Inderjit Dhillon: what is the amount of competition which was involved?


472
00:55:40.538 --> 00:55:46.037
Inderjit Dhillon: Okay, so let me actually try to do this quickly.


473
00:55:46.328 --> 00:55:55.728
Inderjit Dhillon: MII is, I, I, by K, which is Ni, my care.


474
00:55:56.168 --> 00:55:58.837
Inderjit Dhillon: So I have to 1st form this matrix.


475
00:56:03.148 --> 00:56:07.927
Inderjit Dhillon: Okay? And I'm actually doing this because you'll actually have this problem right? You'll actually


476
00:56:08.578 --> 00:56:13.077
Inderjit Dhillon: for your homework, you'll actually need to do these competitions.


477
00:56:13.308 --> 00:56:16.087
Inderjit Dhillon: So forming this matrix will require.


478
00:56:17.328 --> 00:56:21.928
Inderjit Dhillon: So it's, you know, basically multiplication of 2 rectangular matrices.


479
00:56:22.498 --> 00:56:29.268
Inderjit Dhillon: Okay? And this will be K squared times, Ni operations


480
00:56:32.678 --> 00:56:37.178
Inderjit Dhillon: solving the linear system. It's a k by K, linear system.


481
00:56:37.828 --> 00:56:40.338
Inderjit Dhillon: K, by K, linear system.


482
00:56:43.848 --> 00:56:49.967
Inderjit Dhillon: Okay, you won't. You'll probably use some routine


483
00:56:50.238 --> 00:56:53.888
Inderjit Dhillon: right? But it'll take order. Cube K cube operations.


484
00:56:59.138 --> 00:57:01.628
Inderjit Dhillon: Okay? And so the total complexity.


485
00:57:09.218 --> 00:57:11.158
Inderjit Dhillon: Okay, off


486
00:57:14.238 --> 00:57:25.448
Inderjit Dhillon: finding you from MI, okay, is going to be equal to


487
00:57:26.088 --> 00:57:35.888
Inderjit Dhillon: order K square summation of Ni over. I, okay.


488
00:57:39.408 --> 00:57:47.018
Inderjit Dhillon: And that is equal to oh, kind of number of ratings.


489
00:57:48.588 --> 00:57:53.128
Inderjit Dhillon: So summation of Ni is basically the total number of ratings in the matrix


490
00:57:53.458 --> 00:57:56.557
Inderjit Dhillon: times, X squared. K squared. Yeah, should not


491
00:57:58.758 --> 00:58:07.338
Inderjit Dhillon: times K squared plus. I have this part right? Mk, cubed and that needs to be solved. M mk, times.


492
00:58:09.618 --> 00:58:10.828
Inderjit Dhillon: Kq plans.


493
00:58:13.198 --> 00:58:21.617
Inderjit Dhillon: Okay, so this is the method of alternating these squares alternating minimization.


494
00:58:24.468 --> 00:58:31.288
Inderjit Dhillon: But the minimization problem is these squares are that every step okay?


495
00:58:31.568 --> 00:58:34.628
Inderjit Dhillon: And in some sense it is a little expensive.


496
00:58:36.638 --> 00:58:41.868
Inderjit Dhillon: Okay? Because you have competition being cubic. And K,


497
00:58:50.078 --> 00:58:53.808
Inderjit Dhillon: okay, and you can have other alternatives to solve this.


498
00:58:54.908 --> 00:59:00.638
Inderjit Dhillon: And we'll see some of these methods later. Okay, so one is going to be


499
00:59:01.198 --> 00:59:08.307
Inderjit Dhillon: a method that we will actually talk about quite a bit later on, which is stochastic gradient descent.


500
00:59:09.948 --> 00:59:14.958
Inderjit Dhillon: Okay, oops and Spd.


501
00:59:15.338 --> 00:59:20.388
Inderjit Dhillon: Then we can also use other methods like coordinate, decent.


502
00:59:23.438 --> 00:59:24.378
Inderjit Dhillon: and so on.


503
00:59:24.618 --> 00:59:27.904
Inderjit Dhillon: Okay, but


504
00:59:29.858 --> 00:59:44.018
Inderjit Dhillon: you'll see that this problem actually exposes different aspects of read regression that we have talked about. Okay, and you will see that, for example, you will not be able to do it without


505
00:59:44.298 --> 00:59:52.178
Inderjit Dhillon: rich regularization. You just will get an error when you try to do it right, because the matrix might be too small.


506
00:59:54.468 --> 00:59:57.978
Inderjit Dhillon: or a particular matrices that you get might not be in workflow.


507
00:59:59.258 --> 01:00:01.678
Inderjit Dhillon: So any questions.


508
01:00:03.598 --> 01:00:08.778
Hormoz Shahrzad: So if we could use Svd, maybe just


509
01:00:09.638 --> 01:00:17.658
Hormoz Shahrzad: like, multiply the W. By R. And then characterize it.


510
01:00:19.438 --> 01:00:22.427
Inderjit Dhillon: Sorry but but you cannot use the Svd. Right? Because.


511
01:00:22.890 --> 01:00:28.437
Hormoz Shahrzad: I'm just saying, let's let's suppose that we could use Sv. Then.


512
01:00:29.818 --> 01:00:34.768
Hormoz Shahrzad: It would be like factorizing the


513
01:00:36.228 --> 01:00:39.658
Hormoz Shahrzad: the harder mart matrix times, the the R.


514
01:00:41.028 --> 01:00:46.488
Inderjit Dhillon: Yeah, I mean, I guess what you're saying is that, can I just take w dot R.


515
01:00:46.778 --> 01:00:48.438
Hormoz Shahrzad: Which means that.


516
01:00:48.788 --> 01:00:52.948
Inderjit Dhillon: On the position that you don't know any ratings. You replace them by zeros right.


517
01:00:53.588 --> 01:00:54.567
Hormoz Shahrzad: Yeah, something like that.


518
01:00:54.568 --> 01:01:00.017
Inderjit Dhillon: People actually have done that. And actually, that's a good point. People have actually used that to try to initialize this


519
01:01:00.498 --> 01:01:01.548
Inderjit Dhillon: iteration.


520
01:01:04.008 --> 01:01:06.897
Hormoz Shahrzad: Come up with the new one, or m. 1, or something right.


521
01:01:06.898 --> 01:01:14.170
Inderjit Dhillon: Yeah, exactly. Exactly. And the the issue is that you know, Svd is actually a pretty complicated


522
01:01:16.358 --> 01:01:19.618
Inderjit Dhillon: computation, which is kind of expensive


523
01:01:20.108 --> 01:01:37.017
Inderjit Dhillon: right. And many times what ends up happening is that you know you want to do this over millions, or even billions of users and millions or 100 million movies. And then it just becomes computationally intractable to do the Svd.


524
01:01:38.628 --> 01:01:40.538
Hormoz Shahrzad: Yeah, yeah, makes sense.


525
01:01:40.728 --> 01:01:45.348
Inderjit Dhillon: Nope, okay, any other questions on this?


526
01:01:47.658 --> 01:01:59.548
Inderjit Dhillon: Okay? So I'm actually going to stop a little bit early. And I mentioned a couple of times that we will give you your your 1st homework is being given out today.


527
01:01:59.828 --> 01:02:02.348
Inderjit Dhillon: and it'll be due next Friday.


528
01:02:02.498 --> 01:02:06.777
Inderjit Dhillon: So now the Ta will come online and kind of just give you some.


529
01:02:07.158 --> 01:02:07.968
Inderjit Dhillon: Oh.


530
01:02:08.738 --> 01:02:22.287
Inderjit Dhillon: talk a little bit about, you know, some material that might help you in, you know, doing the homework right. We all know that. Well, most of you have said that you all of you, have said that you know Python.


531
01:02:22.488 --> 01:02:30.848
Inderjit Dhillon: and this homework will involve programming in python, and then I'll hand it over to Nilesh, who can talk a little bit more


532
01:02:30.998 --> 01:02:32.838
Inderjit Dhillon: about what this homework might involve.


533
01:02:34.098 --> 01:02:37.388
Nilesh Gupta: Thanks, Andaji, you can hear me right.


534
01:02:37.858 --> 01:02:38.518
Hormoz Shahrzad: Yes.


535
01:02:39.068 --> 01:02:40.188
Inderjit Dhillon: Yeah, I can hear you.


536
01:02:41.338 --> 01:02:43.918
Nilesh Gupta: So let me start sharing.


537
01:02:50.168 --> 01:03:00.867
Nilesh Gupta: So I'll because this is the 1st homework. I'll briefly go over like some of the logistics, and where you can find the homework, and like, what do we expect from it?


538
01:03:01.078 --> 01:03:10.768
Nilesh Gupta: So if you go on the course web page like you click on homeworks, homework one here. It's it takes you to this canvas link


539
01:03:10.938 --> 01:03:12.917
Nilesh Gupta: where we have released the homework


540
01:03:13.098 --> 01:03:19.017
Nilesh Gupta: so essentially like it contains one Pdf document, which is the your problem statement.


541
01:03:19.288 --> 01:03:25.877
Nilesh Gupta: and it also comes up with a Jupyter notebook which is yours kind of your starting point. You need to modify it.


542
01:03:26.408 --> 01:03:33.678
Nilesh Gupta: and the answers that the questions ask. You need to directly annotate this notebook


543
01:03:34.228 --> 01:03:36.188
Nilesh Gupta: and like, give the answers in them


544
01:03:36.458 --> 01:03:40.657
Nilesh Gupta: along with the code and submit the annotated notebook here on canvas.


545
01:03:40.908 --> 01:03:50.357
Nilesh Gupta: So the data set that will be required for this homework is going to be available like in in the files, folder like, I've given all the links here, but like, if you just want to see


546
01:03:50.838 --> 01:04:00.157
Nilesh Gupta: all the files related to homework one, they are here in the files. If you go under Hw. One. That's where you can find all the files for this homework.


547
01:04:00.718 --> 01:04:07.537
Nilesh Gupta: So maybe let's take a brief look at what the homework currently looks like.


548
01:04:07.808 --> 01:04:10.988
Nilesh Gupta: So this is the Pdf that will open up


549
01:04:11.398 --> 01:04:20.827
Nilesh Gupta: has 2 questions. And essentially, it's just a 1 question, which is the Netflix problem like a like a simpler version of the Netflix problem.


550
01:04:21.008 --> 01:04:26.498
Nilesh Gupta: and you have to do some experiments with it. And then there's like a simple.


551
01:04:26.678 --> 01:04:33.868
Nilesh Gupta: least squares regression problem which you just have to answer. You don't have to do any experiment or run any code on it.


552
01:04:34.248 --> 01:04:38.148
Nilesh Gupta: So this is your problem. Pdf.


553
01:04:38.288 --> 01:04:49.627
Nilesh Gupta: and if you go to the starter notebook. That's how it looks. This is also self-sufficient. By the way, like everything that is there in the Pdf. Is also there in this notebook. So you don't have to go back and forth.


554
01:04:50.538 --> 01:04:56.028
Nilesh Gupta: so so it uses you will be coding this homework in numpy


555
01:04:56.228 --> 01:05:03.308
Nilesh Gupta: you'll be using like only these imports you need. You don't need anything more than that.


556
01:05:04.338 --> 01:05:13.627
Nilesh Gupta: And here we'll be loading the data which is given on the canvas. And then there are these boilerplates that has been given to you.


557
01:05:14.188 --> 01:05:29.137
Nilesh Gupta: and what the arguments are, and what these functions are supposed to return. That's described. You need to remove these arrays, not implemented errors and just write your own code here.


558
01:05:29.738 --> 01:05:34.487
Nilesh Gupta: Similarly. So there is, solve ridge, update U update N


559
01:05:34.778 --> 01:05:38.348
Nilesh Gupta: train, and then your code for evaluation.


560
01:05:38.608 --> 01:05:48.797
Nilesh Gupta: And then all of these subsequent questions are here like, write the code here which is required for that question, and answer it here in the Markdown.


561
01:05:49.218 --> 01:05:52.677
Nilesh Gupta: or like. Whatever you feel is you're comfortable with.


562
01:05:53.338 --> 01:06:01.927
Nilesh Gupta: so yeah, that's your template file. And maybe we can quickly take a look at how does our data looks like?


563
01:06:02.117 --> 01:06:04.727
Nilesh Gupta: So if you load it.


564
01:06:06.138 --> 01:06:06.648
Nilesh Gupta: So


565
01:06:07.748 --> 01:06:17.198
Nilesh Gupta: so this is the training time rating matrix, which is train underscore. R. Dot. Npy, and it's of the shape 2,000 cross 1, 8, 2, 1


566
01:06:17.378 --> 01:06:21.097
Nilesh Gupta: essentially like 2,000 users and 1 8, 2, 1 movies.


567
01:06:21.588 --> 01:06:26.477
Nilesh Gupta: Your validation ray set also, is of the rating.


568
01:06:26.718 --> 01:06:35.527
Nilesh Gupta: A matrix is also of the same shape, just that it has non-zeros at different places. So these are the places that you want to validate on


569
01:06:36.178 --> 01:06:51.528
Nilesh Gupta: so quickly like this is how the training matrix looks like it's kind of a sparse matrix which has like at some places it has like these ratings on it. If we


570
01:06:51.737 --> 01:06:53.858
Nilesh Gupta: like, take a closer look here.


571
01:06:58.248 --> 01:07:05.187
Nilesh Gupta: yeah, so this is like a subset of this matrix. And as you can see, like, these values are between 0 to 5,


572
01:07:05.608 --> 01:07:11.007
Nilesh Gupta: the blue ones being 0. And at some places you have these ratings given to you.


573
01:07:12.308 --> 01:07:13.498
Nilesh Gupta: So


574
01:07:14.877 --> 01:07:21.508
Nilesh Gupta: yeah, that's mostly it that I wanted to talk about. Of course, like, if there are any questions like


575
01:07:21.978 --> 01:07:26.537
Nilesh Gupta: come to the officers and will help you.


576
01:07:27.197 --> 01:07:30.187
Nilesh Gupta: And other than this in the


577
01:07:30.758 --> 01:07:37.778
Nilesh Gupta: forms initial survey that we did like. You guys have posted that, like all, all of you are familiar with Python


578
01:07:37.888 --> 01:07:40.868
Nilesh Gupta: and some of its libraries. So


579
01:07:41.028 --> 01:07:58.918
Nilesh Gupta: we are not going to give any tutorial on it. But of course, like, if you have, if you feel like, you are having problems with it definitely come to the office hours, and there is a write up here on the course web page, which is on getting started with python for Ml.


580
01:07:59.048 --> 01:08:06.468
Nilesh Gupta: It has, like a brief kind of a tutorial on, and a description of all of the


581
01:08:06.588 --> 01:08:16.948
Nilesh Gupta: things that we'll be using later on this, not later on this course, but like anything python related that we'll be using for the coding assignments.


582
01:08:17.318 --> 01:08:22.747
Nilesh Gupta: So python, numpy matplot lab conda notebooks


583
01:08:23.452 --> 01:08:34.027
Nilesh Gupta: it is like not too verbose, but like for example, on notebooks and all. There is a sample notebook which gives you a taste of what you can do.


584
01:08:34.388 --> 01:08:42.118
Nilesh Gupta: So yeah, if if you don't feel comfortable with these terms, or don't know like have not worked with them before.


585
01:08:42.298 --> 01:08:49.547
Nilesh Gupta: So like, try going over it. And if you're still having troubles, yeah, please come to the officers.


586
01:08:50.108 --> 01:08:51.757
Nilesh Gupta: We'll be very happy to help.


587
01:08:54.718 --> 01:08:57.208
Nilesh Gupta: So any questions.


588
01:08:57.568 --> 01:08:58.648
Nilesh Gupta: The homework.


589
01:08:59.508 --> 01:09:00.398
Hormoz Shahrzad: Thank you.


590
01:09:02.318 --> 01:09:03.018
Inderjit Dhillon: Okay.


591
01:09:03.018 --> 01:09:11.168
Inderjit Dhillon: any questions? Thanks, Nilesh and Nilesh. Maybe you can just repeat what your the time for your office hours.


592
01:09:11.408 --> 01:09:15.927
Nilesh Gupta: Right. My office hours are Tuesday and Thursday 11 to noon.


593
01:09:16.898 --> 01:09:19.697
Inderjit Dhillon: Yeah. So any questions you have about.


594
01:09:20.558 --> 01:09:35.498
Inderjit Dhillon: you know, python, or doing your homework, and later on we will talk about pytorch, and so on, and you know this will be a great place to kind of ask. You'll see that you'll rely heavily on Miles


595
01:09:35.598 --> 01:09:39.268
Inderjit Dhillon: as we move through the homeworks and onto the projects.


596
01:09:40.028 --> 01:09:44.228
Inderjit Dhillon: So meanwhile, any questions.


597
01:09:45.488 --> 01:09:50.418
Inderjit Dhillon: So with this, we kind of conclude talking about regression.


598
01:09:50.698 --> 01:10:02.817
Inderjit Dhillon: And next lecture we'll start talking about, you know the other predictive problem in supervised learning, which is classification. When, for example, you may want to predict whether your email is


599
01:10:02.968 --> 01:10:22.467
Inderjit Dhillon: spam or not, you want to predict whether Xi belongs to one class or the other class. And then, of course, we have the multi-class extension of that problem, where, instead of one class. You might have multiple classes. Okay? So if there are no questions.


600
01:10:22.898 --> 01:10:24.238
Inderjit Dhillon: no questions.


601
01:10:25.448 --> 01:10:29.498
Inderjit Dhillon: Okay, then we'll see you on Wednesday. Huh? And meanwhile


602
01:10:29.738 --> 01:10:32.248
Inderjit Dhillon: the homework will be released to you very soon.


603
01:10:32.908 --> 01:10:34.308
Nilesh Gupta: It's already online.


604
01:10:34.498 --> 01:10:37.918
Inderjit Dhillon: It's already on. Okay. Great. So the homework is already online.


605
01:10:38.429 --> 01:10:44.917
Inderjit Dhillon: And good luck in you doing the homework. Remember, it's due next week on Friday.


606
01:10:45.968 --> 01:10:48.027
Inderjit Dhillon: Okay, thank you. Bye.



---- END OF LECTURE -------- START OF LECTURE 7 ----
WEBVTT

1
00:00:01.438 --> 00:00:05.928
Inderjit Dhillon: Okay, so let's start today's lecture.


2
00:00:06.888 --> 00:00:11.527
Inderjit Dhillon: Okay? So we have. Concluded talking about regression.


3
00:00:11.658 --> 00:00:28.308
Inderjit Dhillon: and we are going to move on to classification in regression. We realized, at least in least squares regression. We realized that you know, we needed to do like a review of linear algebra, which I did, and hopefully that was helpful to most of you.


4
00:00:28.878 --> 00:00:30.538
Inderjit Dhillon: Oh, in


5
00:00:31.848 --> 00:00:50.607
Inderjit Dhillon: As we go through more and more of machine learning, supervised machine learning, we'll see that we need other concepts, too, especially from probability theory. So before actually starting to talk about classification. I'm actually going to give a lecture on probability theory background that we will need


6
00:00:50.808 --> 00:00:55.927
Inderjit Dhillon: for hoops that we will need for this class.


7
00:01:08.108 --> 00:01:17.828
Inderjit Dhillon: I'll talk about probability, ability, theory background.


8
00:01:19.958 --> 00:01:23.078
Inderjit Dhillon: So we'll spend today talking about it. And next time


9
00:01:23.288 --> 00:01:29.957
Inderjit Dhillon: when we start the 1st lecture on classification, we'll kind of realize that we kind of need it almost right away.


10
00:01:32.678 --> 00:01:37.387
Inderjit Dhillon: Probabilistic modeling is a very important


11
00:01:37.568 --> 00:01:48.478
Inderjit Dhillon: concept in machine learning. And we could also have used an integration. But we will see very quickly that we will need it in.


12
00:01:49.739 --> 00:01:56.648
Inderjit Dhillon: discussing various classification approaches. So let's assume that we have just 2 random variables, for now


13
00:01:57.438 --> 00:02:04.137
Inderjit Dhillon: we have random variables X and Y, and right now I'll just assume


14
00:02:04.288 --> 00:02:11.977
Inderjit Dhillon: that these random variables are discrete. So they take values x, 1 x, 2 through XM,


15
00:02:12.218 --> 00:02:18.448
Inderjit Dhillon: and y takes values y, 1 y, 2 through YL.


16
00:02:18.948 --> 00:02:26.475
Inderjit Dhillon: So when I have these 2 discrete random variables, I can actually look at their joint distribution and think of it as


17
00:02:27.278 --> 00:02:28.398
Inderjit Dhillon: or table.


18
00:02:28.677 --> 00:02:30.868
Inderjit Dhillon: That's why I have joint distribution.


19
00:02:31.548 --> 00:02:38.747
Inderjit Dhillon: P. Of XY, right? And I can represent it. I can look at the table over here.


20
00:02:41.098 --> 00:02:43.523
Inderjit Dhillon: The columns are indexed by


21
00:02:44.238 --> 00:02:53.897
Inderjit Dhillon: the X values or the X random variable. The y's the rows are indexed by y, so the different values that y can take is y, 1


22
00:02:54.348 --> 00:03:00.848
Inderjit Dhillon: y, 2 YJ, what else?


23
00:03:01.588 --> 00:03:10.767
Inderjit Dhillon: And x is x 1 x 2 XIMX. M. Okay.


24
00:03:10.958 --> 00:03:18.097
Inderjit Dhillon: And so, for example, if I am looking for the Jf row over here, YJ.


25
00:03:18.528 --> 00:03:22.068
Inderjit Dhillon: And the Ith column right. This is.


26
00:03:22.838 --> 00:03:30.438
Inderjit Dhillon: I will. If I want to be very particular, I will say that this is P. Probability of x equal to Xi


27
00:03:30.798 --> 00:03:33.467
Inderjit Dhillon: and Y, equal to. Yj.


28
00:03:33.798 --> 00:03:38.437
Inderjit Dhillon: okay, and many times we don't want to have so much, such a cumbersome rotation.


29
00:03:38.568 --> 00:03:43.478
Inderjit Dhillon: And we'll just say that this is probability of Xi comma Whitejack.


30
00:03:51.998 --> 00:03:52.898
Inderjit Dhillon: Okay?


31
00:03:53.838 --> 00:04:03.328
Inderjit Dhillon: And now, suppose you know I have dealing with these run variables that can take these discrete values. You know, I can kind of have end trials.


32
00:04:04.878 --> 00:04:08.568
Inderjit Dhillon: Okay? And in the end trials I can record.


33
00:04:08.698 --> 00:04:15.398
Inderjit Dhillon: And Ij as the number of times


34
00:04:18.488 --> 00:04:24.648
Inderjit Dhillon: in these trials we observe that X equal to Xi


35
00:04:25.108 --> 00:04:29.127
Inderjit Dhillon: and Y equal to yj, okay. And as


36
00:04:29.358 --> 00:04:33.817
Inderjit Dhillon: I start doing more and more trials that as N tends to infinity.


37
00:04:34.108 --> 00:04:44.938
Inderjit Dhillon: you know what this is that Pxi Yj will be equal to. So this estimate will converse to the actual probability values. Okay?


38
00:04:45.168 --> 00:04:53.647
Inderjit Dhillon: So when we are talking about random variables, there are 2 important concepts that are important.


39
00:04:53.818 --> 00:05:10.967
Inderjit Dhillon: One is called the marginal distribution. So even though I have a joint distribution, I might be interested in what is Px equal to XY. So I'm marginalizing over all the Y outcomes. That's why this is also called as the marginal distribution.


40
00:05:16.658 --> 00:05:20.437
Inderjit Dhillon: Okay, and that is going to be just the summation.


41
00:05:22.428 --> 00:05:30.008
Inderjit Dhillon: Okay, so remember the Y takes the values y 1 through. Yl, so I look at P of x equal to xi.


42
00:05:30.708 --> 00:05:33.277
Inderjit Dhillon: y. Equal to Yj.


43
00:05:33.698 --> 00:05:39.048
Inderjit Dhillon: right? And I actually have some over sorry this should be J equal to 1, 2.


44
00:05:40.578 --> 00:05:47.548
Inderjit Dhillon: Okay, so I'm marginalizing over Y, and I get the marginal distribution. P. Of x equal to XY.


45
00:05:47.838 --> 00:05:51.467
Inderjit Dhillon: And now, if I look at kind of the details about


46
00:05:51.688 --> 00:05:55.108
Inderjit Dhillon: what it is right, this is going to be summation of


47
00:05:55.708 --> 00:06:03.167
Inderjit Dhillon: J equal to one through L. If I have N trials right? This is each of these values, remember, is


48
00:06:03.528 --> 00:06:06.438
Inderjit Dhillon: NIJ, divided by N,


49
00:06:11.428 --> 00:06:16.927
Inderjit Dhillon: okay, now, N is kind of a constant, so I can keep get it outside.


50
00:06:17.028 --> 00:06:21.697
Inderjit Dhillon: And then summation of Nij, I can call as Ci.


51
00:06:22.858 --> 00:06:26.397
Inderjit Dhillon: okay. So, for example, over here in my


52
00:06:26.578 --> 00:06:32.718
Inderjit Dhillon: table over here, if I record the number Nij in each cell, right?


53
00:06:32.908 --> 00:06:43.668
Inderjit Dhillon: Then I can actually list the kind of cumulative sums as c, 1, c. 2, CICM.


54
00:06:43.988 --> 00:06:50.137
Inderjit Dhillon: Okay, and clearly see, I is equal to the sum


55
00:06:50.238 --> 00:06:56.428
Inderjit Dhillon: along this entire column. Right? So, summation of Nij.


56
00:06:57.208 --> 00:07:00.777
Inderjit Dhillon: where my J is going from one through.


57
00:07:01.388 --> 00:07:03.417
Inderjit Dhillon: Okay? And that's what's coming out over here.


58
00:07:05.588 --> 00:07:10.857
Inderjit Dhillon: So this turns out kind of gives you one important rule.


59
00:07:13.048 --> 00:07:15.348
Inderjit Dhillon: Okay, which is kind of the sum rule.


60
00:07:23.638 --> 00:07:29.358
Inderjit Dhillon: And then the other than marginal probabilities. An important concept is


61
00:07:32.518 --> 00:07:36.647
Inderjit Dhillon: conditional probability. Okay? So conditional probability.


62
00:07:46.578 --> 00:07:53.618
Inderjit Dhillon: So suppose I'm looking at the conditional probability of Y equal to Yj, given


63
00:07:54.408 --> 00:07:59.398
Inderjit Dhillon: that, I've already observed that x equal to excellent.


64
00:08:00.118 --> 00:08:05.528
Inderjit Dhillon: and that will be denoted by P of y, equal to Yj.


65
00:08:05.858 --> 00:08:08.507
Inderjit Dhillon: given that X equal to xy


66
00:08:08.988 --> 00:08:13.828
Inderjit Dhillon: right? And if you think about the table over here, right?


67
00:08:14.008 --> 00:08:19.827
Inderjit Dhillon: So this is kind of saying that you are now only really concentrating on this part


68
00:08:19.938 --> 00:08:25.098
Inderjit Dhillon: of the joint distribution you're looking at that X is already given to me. Xi.


69
00:08:25.318 --> 00:08:28.908
Inderjit Dhillon: And then what is the probability, then, that I get this?


70
00:08:29.348 --> 00:08:43.658
Inderjit Dhillon: So instead of being Nij divided by N, that is the probability of xi comma yj, given N, it'll be nij given by the divided by the marginal distribution. Right? So it's actually going to be equal to


71
00:08:44.128 --> 00:08:45.698
Inderjit Dhillon: MIJ.


72
00:08:45.998 --> 00:08:46.728
Inderjit Dhillon: Why.


73
00:08:52.018 --> 00:08:55.098
Inderjit Dhillon: NIJ. Divided by CR.


74
00:08:57.198 --> 00:09:09.398
Inderjit Dhillon: Okay, and just like we, you know, had this more concise form for the joint distribution. We'll also write this as P. Of Yj given Xi.


75
00:09:11.088 --> 00:09:14.008
Inderjit Dhillon: So it's what we see is that B


76
00:09:14.628 --> 00:09:23.338
Inderjit Dhillon: of Yj given Xi is Nij divided by Ci right, as of course, the number of trials becomes larger and larger.


77
00:09:23.528 --> 00:09:28.438
Inderjit Dhillon: And so let me just write that again. P. Of Yj, given Xi.


78
00:09:28.868 --> 00:09:29.758
Inderjit Dhillon: Okay.


79
00:09:30.108 --> 00:09:38.068
Inderjit Dhillon: Oh, sorry. The remember the joint distribution. P. Of YJ


80
00:09:38.558 --> 00:09:43.168
Inderjit Dhillon: comma xi is equal to Ni divided by M.


81
00:09:43.938 --> 00:09:44.778
Inderjit Dhillon: Okay.


82
00:09:44.878 --> 00:09:49.328
Inderjit Dhillon: And IJ, and I can say that this is equal to NIJ.


83
00:09:49.688 --> 00:09:53.528
Inderjit Dhillon: Divided by Ci. Times. Ci, divided by.


84
00:09:54.228 --> 00:10:08.427
Inderjit Dhillon: So it's still Nij, divided by Ci. C. By N. But now I've written it in terms of well, the conditional distribution and the marginal distribution. So if you like, again, let me kind of just


85
00:10:08.708 --> 00:10:12.857
Inderjit Dhillon: write this out. P of y, equal to Yj


86
00:10:13.418 --> 00:10:20.768
Inderjit Dhillon: x, equal to Xi. Right. This is equal to over here. This part.


87
00:10:22.228 --> 00:10:25.158
Inderjit Dhillon: right, which is the same as this part over here.


88
00:10:26.658 --> 00:10:31.288
Inderjit Dhillon: Okay, so this is equal to P of y, equal to Yj


89
00:10:31.508 --> 00:10:34.597
Inderjit Dhillon: condition on X equal to Xi.


90
00:10:35.178 --> 00:10:39.607
Inderjit Dhillon: And this is the marginal distribution. Remember, we saw this above.


91
00:10:40.178 --> 00:10:42.997
Inderjit Dhillon: this is P of x, equal to xi.


92
00:10:43.158 --> 00:10:49.708
Inderjit Dhillon: Okay, so this is P of X, equal to excellent.


93
00:10:52.928 --> 00:10:53.888
Inderjit Dhillon: Okay.


94
00:10:54.028 --> 00:10:57.597
Inderjit Dhillon: So this is often known as the product rule.


95
00:10:58.568 --> 00:11:02.258
Inderjit Dhillon: If you, for example, take a look at the book by Chris Bishop.


96
00:11:02.708 --> 00:11:06.567
Inderjit Dhillon: then he will also have a section devoted to this.


97
00:11:07.398 --> 00:11:09.948
Inderjit Dhillon: This is the product rule.


98
00:11:11.878 --> 00:11:16.917
Inderjit Dhillon: Okay? So just to summarize. Right? I have 2 rules.


99
00:11:17.058 --> 00:11:19.138
Inderjit Dhillon: This is the sum rule.


100
00:11:20.068 --> 00:11:23.297
Inderjit Dhillon: I'm just writing it now in terms of the random variables.


101
00:11:25.558 --> 00:11:33.338
Inderjit Dhillon: Right? You get the marginal distribution by marginalizing or summing over all the Y outcomes.


102
00:11:33.818 --> 00:11:36.687
Inderjit Dhillon: Okay, so this is the sum rule.


103
00:11:39.908 --> 00:11:42.467
Inderjit Dhillon: and then we get the protocol


104
00:11:46.628 --> 00:11:50.668
Inderjit Dhillon: P of X normal y is equal to P of


105
00:11:51.028 --> 00:12:00.157
Inderjit Dhillon: y given x times B of that's this is the sum rule.


106
00:12:01.948 --> 00:12:03.998
Inderjit Dhillon: And this is the call queue.


107
00:12:07.178 --> 00:12:13.108
Inderjit Dhillon: Okay? Now, we could have conditioned either on X or we could have conditioned on y


108
00:12:13.718 --> 00:12:17.448
Inderjit Dhillon: right. So if I look at P of x comma y.


109
00:12:18.818 --> 00:12:21.447
Inderjit Dhillon: right. What I've written above is


110
00:12:21.868 --> 00:12:29.298
Inderjit Dhillon: that I have this equal to P. Of y. Given x pfx.


111
00:12:30.098 --> 00:12:32.878
Inderjit Dhillon: But I could easily have conditioned over the other variable


112
00:12:33.198 --> 00:12:37.228
Inderjit Dhillon: 1st right? So I could have written this as P. Of.


113
00:12:37.628 --> 00:12:40.938
Inderjit Dhillon: I can write this as Px. Given y times. Py.


114
00:12:42.098 --> 00:12:45.207
Inderjit Dhillon: okay. And that gives us the famous base.


115
00:12:46.878 --> 00:12:51.878
Inderjit Dhillon: So this is used a lot in machine learning, especially when you do probabilistic machine learning.


116
00:12:52.248 --> 00:12:54.687
Inderjit Dhillon: right? So base rule is.


117
00:12:54.898 --> 00:12:59.347
Inderjit Dhillon: But if I want to compute PY, given x.


118
00:13:00.238 --> 00:13:05.998
Inderjit Dhillon: okay, I will basically write this. So remember, py given X is over here


119
00:13:07.118 --> 00:13:09.338
Inderjit Dhillon: because this is something that I want to compute.


120
00:13:10.258 --> 00:13:16.887
Inderjit Dhillon: Then I can write this as Px. Given y. Times. Py divided by Px.


121
00:13:18.788 --> 00:13:27.287
Inderjit Dhillon: Be of X, given y fine speaker divided by


122
00:13:30.968 --> 00:13:35.998
Inderjit Dhillon: okay, so many times, this is known as Prior.


123
00:13:37.328 --> 00:13:43.867
Inderjit Dhillon: So, for example, we'll see that, you know. Suppose we want to look at like classification of an email into spam


124
00:13:44.058 --> 00:13:48.487
Inderjit Dhillon: or not spam. In that case Y is a discrete variable that takes values


125
00:13:48.658 --> 00:13:51.518
Inderjit Dhillon: either in spam or not spam.


126
00:13:51.658 --> 00:14:15.978
Inderjit Dhillon: And then there might be a prior distribution that you know, hey? Maybe most emails are actually normal and not spam. And so py equal to spam might be much lower than py, not equal to spam. So that's something that we can either guess from the data, or we may have some reason to give a kind of different prior.


127
00:14:16.358 --> 00:14:19.688
Inderjit Dhillon: and this is known as the posterior.


128
00:14:21.638 --> 00:14:32.287
Inderjit Dhillon: So in the email, example, the X might be, you know, some kind of data. Right? So Xi, yi pairs.


129
00:14:32.498 --> 00:14:40.458
Inderjit Dhillon: And what you want to do know is the posterior which is given the data. What is the probability that it is?


130
00:14:41.280 --> 00:14:52.547
Inderjit Dhillon: that the email, for example, is spam or not spam. But what really helps us in making the decision is P of x normal x, given y, right? So that is known as


131
00:14:52.798 --> 00:14:57.218
Inderjit Dhillon: either the evidence or the data likelihood.


132
00:15:05.018 --> 00:15:05.718
Inderjit Dhillon: Okay.


133
00:15:06.888 --> 00:15:19.308
Inderjit Dhillon: So, for example, which which will model. Right? Given the spam emails, what do the spam email distribution looks like, given normal emails, what are normal emails?


134
00:15:20.078 --> 00:15:22.298
Inderjit Dhillon: look like, okay?


135
00:15:22.428 --> 00:15:33.237
Inderjit Dhillon: And if you notice over here in the denominator, we have P of X, right? That's the marginal distribution. And I can actually substitute Px


136
00:15:33.487 --> 00:15:36.228
Inderjit Dhillon: over here. Right? So I get


137
00:15:36.538 --> 00:15:41.928
Inderjit Dhillon: that this is equal to Bx. So this the numerator, as above


138
00:15:42.458 --> 00:15:46.248
Inderjit Dhillon: Bxy Bx given y times by.


139
00:15:46.468 --> 00:15:53.398
Inderjit Dhillon: and then the summation over y, which is P of X, Rama. Y,


140
00:15:54.468 --> 00:16:01.028
Inderjit Dhillon: okay. And I can now write the denominator as again.


141
00:16:01.708 --> 00:16:05.436
Inderjit Dhillon: this and I can apply the


142
00:16:06.628 --> 00:16:09.677
Inderjit Dhillon: product rule over here. And I can say that this is


143
00:16:09.858 --> 00:16:13.178
Inderjit Dhillon: B of X given YBY,


144
00:16:13.638 --> 00:16:16.178
Inderjit Dhillon: so you can see that the numerator


145
00:16:16.308 --> 00:16:24.418
Inderjit Dhillon: is one particular x equal to x, and the denominator is summing over all the particular. Y's


146
00:16:25.368 --> 00:16:29.337
Inderjit Dhillon: okay. So that is a base rule that comes up quite a lot.


147
00:16:29.528 --> 00:16:43.377
Inderjit Dhillon: So I've given you like a very quick introduction. So let's actually look at a particular example. Okay, so but before we will do that, you know, just one kind of notion which is, comes up again. Quite a lot


148
00:16:43.538 --> 00:16:46.598
Inderjit Dhillon: is the notion of independence.


149
00:16:47.658 --> 00:16:51.677
Inderjit Dhillon: Okay, so 2 random variables are independent of each other. If


150
00:16:52.218 --> 00:16:58.717
Inderjit Dhillon: pxi comma Yj can just be factored as pxi times. Pj.


151
00:16:59.118 --> 00:17:00.628
Inderjit Dhillon: E. Of y. Jason.


152
00:17:01.368 --> 00:17:16.597
Inderjit Dhillon: So just in terms of the marginals, and you don't need any it actually is. The the joint distribution is like the product distribution of the margins. So, for example, and in particular, this implies that P of Yj.


153
00:17:17.958 --> 00:17:21.277
Inderjit Dhillon: given XI is just equal to P of y.


154
00:17:22.288 --> 00:17:25.991
Inderjit Dhillon: so it doesn't. Really, the conditional distribution is,


155
00:17:27.028 --> 00:17:30.927
Inderjit Dhillon: you know, does not depend actually upon Xi.


156
00:17:31.098 --> 00:17:36.417
Inderjit Dhillon: Okay, so this is kind of the notion of independence. Let's take an exam


157
00:17:37.928 --> 00:17:49.778
Inderjit Dhillon: that illustrates many of the above things right, and how we can infer some probabilities given this information about the joint distribution


158
00:17:52.098 --> 00:17:59.851
Inderjit Dhillon: or the marginal distribution. Okay, so let's take an example where we have 2 boxes. Okay? So


159
00:18:00.588 --> 00:18:03.508
Inderjit Dhillon: so we have 2 boxes.


160
00:18:04.938 --> 00:18:13.058
Inderjit Dhillon: This will be blue android, and we have 2 types of fruit.


161
00:18:15.388 --> 00:18:19.788
Inderjit Dhillon: This will be apples and oranges.


162
00:18:24.298 --> 00:18:29.958
Inderjit Dhillon: Okay, so let me draw like red box.


163
00:18:40.628 --> 00:18:41.898
Inderjit Dhillon: I go to books.


164
00:18:46.338 --> 00:18:48.538
Inderjit Dhillon: Okay? And then I have a blue box.


165
00:19:07.368 --> 00:19:16.827
Inderjit Dhillon: Okay? And suppose somebody says, Hey, you know we are given that. You know, the probability of choosing the red is going to be is point 4,


166
00:19:17.648 --> 00:19:24.417
Inderjit Dhillon: which is 2 divided by 5, and the probability of choosing the blue box is point 6.


167
00:19:25.028 --> 00:19:27.118
Inderjit Dhillon: The probabilities should sum up to one.


168
00:19:27.568 --> 00:19:29.518
Inderjit Dhillon: This is 3 things.


169
00:19:29.878 --> 00:19:35.848
Inderjit Dhillon: Okay? And now suppose I have. In the red box?


170
00:19:36.228 --> 00:19:45.868
Inderjit Dhillon: Okay, suppose I have, you know, 2 apples and 6 oranges.


171
00:19:47.848 --> 00:19:56.448
Inderjit Dhillon: And suppose in the red box I have 3 apples plus 6 oranges.


172
00:19:58.588 --> 00:20:01.477
Inderjit Dhillon: Okay, so the orange color is here.


173
00:20:01.788 --> 00:20:04.067
Inderjit Dhillon: So oranges?


174
00:20:05.878 --> 00:20:07.817
Inderjit Dhillon: Well, I guess I can have.


175
00:20:10.338 --> 00:20:19.068
Inderjit Dhillon: Why should we? Well, let's go with Green, let's say, suppose they are Granny Smith apples, green apples.


176
00:20:19.288 --> 00:20:29.138
Inderjit Dhillon: Okay, so just to draw it, we have 2 apples over here, and then we have


177
00:20:35.308 --> 00:20:39.578
Inderjit Dhillon: how many oranges, 6 oranges, and then the other one


178
00:20:40.268 --> 00:20:47.057
Inderjit Dhillon: actually in my notes. I don't want to get change things too much from my notes. I think we have one orange.


179
00:20:59.648 --> 00:21:03.948
Inderjit Dhillon: We have one orange we've heard.


180
00:21:09.488 --> 00:21:18.048
Inderjit Dhillon: Okay, so what is the probably


181
00:21:24.428 --> 00:21:28.587
Inderjit Dhillon: what is the probability of an apple given? Red box?


182
00:21:33.488 --> 00:21:35.328
Inderjit Dhillon: Okay, come on, somebody help me out.


183
00:21:39.255 --> 00:21:40.328
Vishal Thyagarajan: One over 4.


184
00:21:41.738 --> 00:21:47.007
Inderjit Dhillon: Is one over 4, right? So that's 2 divided by 8. There are 8


185
00:21:47.168 --> 00:21:49.277
Inderjit Dhillon: pieces of fruit in the red box.


186
00:21:50.133 --> 00:21:52.858
Inderjit Dhillon: Somebody be be ready


187
00:21:52.968 --> 00:21:58.288
Inderjit Dhillon: in the red box. We have oranges, somebody else.


188
00:22:01.768 --> 00:22:07.258
Inderjit Dhillon: Oh, you want to communicate by chat. Okay, leadership says threefold. So.


189
00:22:07.258 --> 00:22:14.348
Rishab Maheshwari: That was wrong. I was thinking about the last problem. I I saw the Red Arrow. I confused the numbers.


190
00:22:15.118 --> 00:22:17.917
Inderjit Dhillon: Okay. But is this not correct probability of


191
00:22:18.965 --> 00:22:22.087
Inderjit Dhillon: Orange given a red box? Is that 3 fourths.


192
00:22:22.088 --> 00:22:23.847
Rishab Maheshwari: Oh, that that is yes, that is.


193
00:22:23.848 --> 00:22:25.257
Inderjit Dhillon: It is 3 phones. Right? Yeah.


194
00:22:29.498 --> 00:22:31.447
Inderjit Dhillon: So this is threefold.


195
00:22:32.478 --> 00:22:39.128
Inderjit Dhillon: Okay, somebody else. Probability of apple. Given a blue box.


196
00:22:40.948 --> 00:22:43.587
Inderjit Dhillon: Somebody I haven't heard from so far in class.


197
00:22:44.248 --> 00:22:45.238
Letizia Fazzini: Reports.


198
00:22:47.018 --> 00:22:48.527
Inderjit Dhillon: Sorry. Could you just repeat that.


199
00:22:50.238 --> 00:22:51.397
Haoran Niu: 3, 0, 1, 4.


200
00:22:51.558 --> 00:22:52.637
Inderjit Dhillon: 3 fourths. Right.


201
00:22:54.638 --> 00:22:57.528
Inderjit Dhillon: Probability of apple, green apple.


202
00:22:57.928 --> 00:22:59.407
Inderjit Dhillon: Given a blue box.


203
00:22:59.808 --> 00:23:01.558
Inderjit Dhillon: Okay. So there are 3 apples


204
00:23:01.728 --> 00:23:07.827
Inderjit Dhillon: out of 4, and then probability of orange given a blue box is.


205
00:23:08.098 --> 00:23:08.997
Joseph Stanley: One out of 4.


206
00:23:08.998 --> 00:23:16.827
Inderjit Dhillon: One out of 4. Thank you. Okay. So now I can start writing this now in a two-dimensional table, right? Just like we have for X and y.


207
00:23:17.068 --> 00:23:21.928
Inderjit Dhillon: So let's suppose my box is is the one random variable


208
00:23:22.058 --> 00:23:26.207
Inderjit Dhillon: towards another random, variable. There's a red box, there's a blue box.


209
00:23:26.478 --> 00:23:28.508
Inderjit Dhillon: there's an apple. There's an orange.


210
00:23:30.858 --> 00:23:31.798
Inderjit Dhillon: Okay.


211
00:23:31.968 --> 00:23:34.128
Inderjit Dhillon: Now, do we know the joint distribution?


212
00:23:36.488 --> 00:23:37.618
Inderjit Dhillon: What is?


213
00:23:37.858 --> 00:23:43.357
Inderjit Dhillon: So let's suppose that 4 things right. P of a comma r


214
00:23:44.738 --> 00:23:48.528
Inderjit Dhillon: be of joint distribution of orange and red.


215
00:23:49.128 --> 00:23:58.908
Inderjit Dhillon: join distribution of apple and blue, join distribution of orange. And so


216
00:24:03.498 --> 00:24:08.827
Inderjit Dhillon: somebody wants to tell me. Give me the formula for PA. Comma r.


217
00:24:09.528 --> 00:24:11.547
Inderjit Dhillon: Let's suppose we apply Bayes rule.


218
00:24:12.298 --> 00:24:15.518
Haoran Niu: Is a PA times. PA given R.


219
00:24:17.248 --> 00:24:21.188
Inderjit Dhillon: P. Times a. Did you say it?


220
00:24:21.688 --> 00:24:24.917
Haoran Niu: PA. Given R. Times, PR.


221
00:24:25.218 --> 00:24:25.888
Inderjit Dhillon: Perfect.


222
00:24:26.808 --> 00:24:29.618
Inderjit Dhillon: I can also have written it as P. Of


223
00:24:31.098 --> 00:24:38.608
Inderjit Dhillon: a P. Of R. Given a. Let's come to that later. But but we know this right P of a given r is what


224
00:24:39.568 --> 00:24:45.318
Inderjit Dhillon: 1 4th from above, and p of r is 2 fifths.


225
00:24:48.108 --> 00:24:51.438
Inderjit Dhillon: So that is one dense.


226
00:24:52.848 --> 00:24:56.827
Inderjit Dhillon: What is PO. Give comma. R. The joint distribution.


227
00:24:57.498 --> 00:25:01.068
Inderjit Dhillon: Similarly, P. Of O. Given R.


228
00:25:01.618 --> 00:25:10.318
Inderjit Dhillon: Lines. Spiel, oh, okay. P. Of R is 2 fifths as before.


229
00:25:10.898 --> 00:25:14.588
Inderjit Dhillon: and P. Of all given r is 3 fourths.


230
00:25:15.578 --> 00:25:16.988
Inderjit Dhillon: Will that become?


231
00:25:17.578 --> 00:25:22.578
Inderjit Dhillon: No, is equal to 3 times.


232
00:25:24.258 --> 00:25:27.647
Inderjit Dhillon: Now, what about this form? Is this formula wrong or right?


233
00:25:36.618 --> 00:25:37.977
Inderjit Dhillon: It is correct, right?


234
00:25:38.718 --> 00:25:40.738
Inderjit Dhillon: But do we know what P. Of A is?


235
00:25:42.968 --> 00:25:49.107
Inderjit Dhillon: Actually, we don't right. It's not kind of straightforward, because the process involves picking the


236
00:25:49.498 --> 00:25:52.548
Inderjit Dhillon: box 1st and then picking the fruit from the box.


237
00:25:52.658 --> 00:26:04.298
Inderjit Dhillon: Okay, but we can try to. We will try to infer that right. We'll try to see that the sum rule and the product rule will allow us to figure out what P of a is and what P of R given is


238
00:26:04.688 --> 00:26:16.877
Inderjit Dhillon: okay. But meanwhile we have P of apple given the boxes blue again, apply P of a given B,


239
00:26:17.668 --> 00:26:18.768
Inderjit Dhillon: be off.


240
00:26:18.948 --> 00:26:19.628
Inderjit Dhillon: Great.


241
00:26:20.798 --> 00:26:23.538
Inderjit Dhillon: Okay. P. Of B is 3 fifths


242
00:26:24.278 --> 00:26:30.958
Inderjit Dhillon: and P of a given B is over here. 3 fourths, right?


243
00:26:35.708 --> 00:26:48.257
Inderjit Dhillon: Okay, that becomes 9 by 20 and pay off orange. Given blue Boards.


244
00:26:48.438 --> 00:26:50.538
Inderjit Dhillon: Be off the box.


245
00:26:51.138 --> 00:26:57.668
Inderjit Dhillon: We have blue boxes, 3 fifths, and this is one slides.


246
00:26:59.068 --> 00:27:01.398
Inderjit Dhillon: So it's being divided by it.


247
00:27:04.608 --> 00:27:11.288
Inderjit Dhillon: So I can start filling. Actually, this join distribution table. Right?


248
00:27:11.578 --> 00:27:15.098
Inderjit Dhillon: So, PA comma r is 1 10.th


249
00:27:16.248 --> 00:27:19.968
Inderjit Dhillon: The O comma r is 3 pence.


250
00:27:21.128 --> 00:27:26.997
Inderjit Dhillon: This is 9 20, th and this is 3 clinics.


251
00:27:28.988 --> 00:27:34.698
Inderjit Dhillon: and you should verify that we were given. That P. Of R is


252
00:27:35.198 --> 00:27:40.247
Inderjit Dhillon: 2 fifths right? So if I add these 2 up right, if I add


253
00:27:40.728 --> 00:27:45.498
Inderjit Dhillon: P of RA comma r plus p of O comma r.


254
00:27:45.998 --> 00:27:49.648
Inderjit Dhillon: What does it add up to? 4 tenths, which is 2 fifths


255
00:27:50.678 --> 00:27:53.078
Inderjit Dhillon: right? And that'd be of our.


256
00:27:53.578 --> 00:27:56.428
Inderjit Dhillon: And you can see that that's exactly what was given away.


257
00:27:56.948 --> 00:27:59.038
Inderjit Dhillon: So far I haven't made a mistake.


258
00:27:59.148 --> 00:27:59.848
Inderjit Dhillon: Yeah.


259
00:28:00.098 --> 00:28:08.638
Inderjit Dhillon: And this is B is 12, divided by 20, which is equal to


260
00:28:11.798 --> 00:28:16.577
Inderjit Dhillon: 3 divided by, and then this will be off.


261
00:28:17.638 --> 00:28:20.108
Inderjit Dhillon: and we will verify that this is the same.


262
00:28:20.238 --> 00:28:23.848
Inderjit Dhillon: Okay, what about probability of picking an apple? Now?


263
00:28:26.488 --> 00:28:27.738
Inderjit Dhillon: Okay, somebody.


264
00:28:31.518 --> 00:28:31.928
Inderjit Dhillon: That's so.


265
00:28:31.928 --> 00:28:36.607
Inderjit Dhillon: Remember that that was not. That's not right in front of us, right? So we actually have to


266
00:28:37.698 --> 00:28:39.748
Inderjit Dhillon: do a marginalization right.


267
00:28:40.428 --> 00:28:43.678
Haoran Niu: So we add up the PAR and pab.


268
00:28:43.998 --> 00:28:45.977
Inderjit Dhillon: Yes, so it is. PA


269
00:28:47.018 --> 00:28:50.967
Inderjit Dhillon: comma r plus PA comma B,


270
00:28:51.918 --> 00:28:56.718
Inderjit Dhillon: and that is 1 10 plus 9 twenties.


271
00:28:57.248 --> 00:29:00.707
Inderjit Dhillon: and that's equal to 11 credits.


272
00:29:01.468 --> 00:29:03.247
Inderjit Dhillon: So that's this sum.


273
00:29:06.878 --> 00:29:12.678
Inderjit Dhillon: And what about P of all?


274
00:29:14.008 --> 00:29:17.827
Inderjit Dhillon: Well, Pfo is p of O comma r


275
00:29:18.528 --> 00:29:22.828
Inderjit Dhillon: plus b of O, comma B is equal to


276
00:29:23.048 --> 00:29:27.138
Inderjit Dhillon: 3 10 plus 3 twentieths, and that is equal to


277
00:29:29.008 --> 00:29:37.167
Inderjit Dhillon: 9 20.th We could actually have also gotten it from here. Right? It's basically one minus 11 by 20 equal to 9 by 20.


278
00:29:37.788 --> 00:29:38.548
Inderjit Dhillon: Okay.


279
00:29:44.658 --> 00:29:49.568
Inderjit Dhillon: what about? Let's say this.


280
00:29:49.708 --> 00:29:52.488
Inderjit Dhillon: Think that we were talking about P. Of


281
00:29:52.918 --> 00:29:55.458
Inderjit Dhillon: that. The suppose you are have picked.


282
00:29:55.708 --> 00:29:58.028
Inderjit Dhillon: You observe that you've picked an orange.


283
00:30:02.858 --> 00:30:07.457
Inderjit Dhillon: Suppose you're observed you picked in orange? What is the probability that you pick the?


284
00:30:07.848 --> 00:30:11.588
Inderjit Dhillon: Oh, you'll pick a red box


285
00:30:12.448 --> 00:30:15.528
Inderjit Dhillon: given orange. Right? So again, we can apply base rule.


286
00:30:15.858 --> 00:30:19.418
Inderjit Dhillon: So that is, P. Of o, comma. Given R.


287
00:30:20.468 --> 00:30:23.488
Inderjit Dhillon: P. Of r, divided by P. Of O,


288
00:30:24.658 --> 00:30:26.687
Inderjit Dhillon: what is B, of o, given r.


289
00:30:29.148 --> 00:30:30.188
Vishal Thyagarajan: Be tense.


290
00:30:31.318 --> 00:30:33.927
Inderjit Dhillon: The 4th right over here.


291
00:30:34.158 --> 00:30:36.517
Vishal Thyagarajan: Oh, sorry. Yeah. Sorry. I'm in 2, 4, yeah.


292
00:30:36.518 --> 00:30:39.637
Inderjit Dhillon: P. Of O. Given R is 3 fourths. Remember.


293
00:30:39.858 --> 00:30:43.988
Inderjit Dhillon: if you have a red box, then there are 6 oranges out of 8,


294
00:30:44.298 --> 00:30:46.698
Inderjit Dhillon: so that's why it is 2 faults.


295
00:30:50.708 --> 00:30:51.838
Inderjit Dhillon: Times.


296
00:30:51.998 --> 00:31:00.678
Inderjit Dhillon: Oh, I could also have written this. Oh, it's okay. What is Pfr P. Of r is


297
00:31:01.488 --> 00:31:08.397
Inderjit Dhillon: 2 fifths divided by P. Of O, which is 9 20.


298
00:31:08.748 --> 00:31:12.338
Inderjit Dhillon: I could actually have just, you know, said that this is equal to


299
00:31:14.738 --> 00:31:20.567
Inderjit Dhillon: p of O comma R. And I'd already done this. So I kind of repeated it. But it's okay.


300
00:31:20.958 --> 00:31:24.698
Inderjit Dhillon: right? So this is equal to it's the same as this quantity over here.


301
00:31:26.708 --> 00:31:28.728
Inderjit Dhillon: Right? This is the same as this.


302
00:31:31.468 --> 00:31:34.918
Inderjit Dhillon: Well, so that is equal to 3 tenths


303
00:31:35.798 --> 00:31:41.608
Inderjit Dhillon: over 9 twentieths. Right? So that's equal to oh.


304
00:31:43.818 --> 00:31:49.838
Inderjit Dhillon: 3, 10 times 20 by 9, and that's equal to 2 thirds.


305
00:31:53.048 --> 00:32:02.738
Inderjit Dhillon: Okay? So you've seen like that, you know, it's all very consistent now, right? If the you have the sum rule. And then you have the product rule


306
00:32:02.908 --> 00:32:06.038
Inderjit Dhillon: and they give us the very important baseline.


307
00:32:13.088 --> 00:32:14.737
Inderjit Dhillon: Any questions so far.


308
00:32:19.868 --> 00:32:24.467
Inderjit Dhillon: Okay, we've talked a little bit about discrete random variables right now.


309
00:32:25.408 --> 00:32:34.847
Inderjit Dhillon: So let's talk a little bit about probabilities with continuous variables, and then we'll talk about the ubiquitous Gaussian distribution that comes up a lot.


310
00:32:35.008 --> 00:32:39.697
Inderjit Dhillon: Okay, so and we will talk about the multivated cost and distribution which


311
00:32:39.877 --> 00:32:42.198
Inderjit Dhillon: some of you might have seen. Some of you might not have seen.


312
00:32:42.618 --> 00:32:45.658
Inderjit Dhillon: Okay, so probabilities


313
00:32:49.598 --> 00:32:52.968
Inderjit Dhillon: with respect to continuous variables.


314
00:32:58.428 --> 00:33:03.707
Inderjit Dhillon: Okay, so we'll have X belongs to R. So right now, just let's take


315
00:33:04.302 --> 00:33:07.578
Inderjit Dhillon: well, we can take either X belongs to R,


316
00:33:08.047 --> 00:33:14.137
Inderjit Dhillon: or later on we'll see that we'll actually look at where the continuous variable is d dimension.


317
00:33:14.478 --> 00:33:15.148
Inderjit Dhillon: Okay?


318
00:33:15.518 --> 00:33:19.138
Inderjit Dhillon: So we have. You know, we have something called the Pdf.


319
00:33:19.728 --> 00:33:29.268
Inderjit Dhillon: Which is the probability for the loop of oh, boy, I'm really sorry.


320
00:33:30.458 --> 00:33:35.408
Inderjit Dhillon: Oh, probability!


321
00:33:37.008 --> 00:33:38.538
Inderjit Dhillon: Nonstic function!


322
00:33:42.988 --> 00:33:49.848
Inderjit Dhillon: And we denoted by capital of Px. Sorry little Px. Typically right? And we'll


323
00:33:50.328 --> 00:33:52.777
Inderjit Dhillon: it's a function. It's a continuous function.


324
00:33:53.108 --> 00:33:56.367
Inderjit Dhillon: V of X is greater than equal 0 right?


325
00:33:56.568 --> 00:34:08.118
Inderjit Dhillon: And it adds up to one. So what does it mean for a continuous, variable? It basically is that the integral suppose the domain is minus infinity plus infinity.


326
00:34:08.528 --> 00:34:12.158
Inderjit Dhillon: then the integral is one.


327
00:34:12.688 --> 00:34:17.187
Inderjit Dhillon: So that's the analog of the sum of the random offer discrete, random, variable.


328
00:34:17.318 --> 00:34:21.928
Inderjit Dhillon: Okay, so similar to the thumb rule


329
00:34:25.228 --> 00:34:28.237
Inderjit Dhillon: and the product rule that we had for


330
00:34:28.778 --> 00:34:33.318
Inderjit Dhillon: continuous and for discrete random variables. We have the sum rule, which is


331
00:34:33.798 --> 00:34:44.867
Inderjit Dhillon: that the marginal distribution is when we integrate out the other variables. So suppose we have a 2 dimensional join a joint distribution between X and Y,


332
00:34:45.098 --> 00:34:47.678
Inderjit Dhillon: right? We marginalize over y.


333
00:34:49.238 --> 00:34:56.178
Inderjit Dhillon: that's kind of saying that we analogous to summing over all the different possible values of Y,


334
00:34:56.628 --> 00:35:03.887
Inderjit Dhillon: and my product rule is that my joint distribution is the conditional distribution


335
00:35:04.688 --> 00:35:08.658
Inderjit Dhillon: times the marginal. So this is, oh.


336
00:35:08.808 --> 00:35:12.187
Inderjit Dhillon: can take the conditional distribution given X.


337
00:35:12.728 --> 00:35:20.608
Inderjit Dhillon: And or you can do Bx given y times. P, okay.


338
00:35:22.638 --> 00:35:27.637
Inderjit Dhillon: So you know, I didn't talk about it really in terms of in the


339
00:35:28.128 --> 00:35:37.178
Inderjit Dhillon: discrete case. But you know, you have the mean of a random, variable right? So expectations


340
00:35:40.948 --> 00:35:42.308
Inderjit Dhillon: or memes.


341
00:35:42.918 --> 00:35:49.217
Inderjit Dhillon: Okay, that is so. Suppose I have a function of a random, variable F of X.


342
00:35:49.738 --> 00:35:54.897
Inderjit Dhillon: Then I have in the discrete case. It would be


343
00:35:55.670 --> 00:35:58.628
Inderjit Dhillon: sorry. Maybe I won't write it here.


344
00:35:58.908 --> 00:36:00.478
Inderjit Dhillon: The discrete case.


345
00:36:01.068 --> 00:36:09.417
Inderjit Dhillon: This is going to be integral of E of x times. F of X. Dx.


346
00:36:10.648 --> 00:36:17.918
Inderjit Dhillon: Okay? And in the discrete case, it would have been submission of XEX. Fx.


347
00:36:25.288 --> 00:36:35.408
Inderjit Dhillon: Okay. In particular, I have that expectation of the random variable is equal to X. Dx phoenix.


348
00:36:38.498 --> 00:36:47.198
Inderjit Dhillon: Okay, in addition to expectation of the meal, there's another important concept called variance.


349
00:36:48.798 --> 00:36:57.257
Inderjit Dhillon: Okay? By the way, note that expectation has this property, that expectation of X plus y.


350
00:36:57.798 --> 00:37:00.767
Inderjit Dhillon: It's linear is expectation of X


351
00:37:02.098 --> 00:37:11.167
Inderjit Dhillon: plus the expectation of Y, and you can see that easily from the integral, because the integral will become the one integral will become the


352
00:37:11.705 --> 00:37:17.348
Inderjit Dhillon: sum of 2 integrals. Okay, so now suppose that again, I have a function of X,


353
00:37:17.528 --> 00:37:27.637
Inderjit Dhillon: F of X, okay, my variance is going to be the expected sum squared deviation from the mean.


354
00:37:28.228 --> 00:37:30.167
Inderjit Dhillon: Okay? So so I said, a lot.


355
00:37:30.358 --> 00:37:33.498
Inderjit Dhillon: So expectation. That's the 1st part.


356
00:37:34.178 --> 00:37:37.737
Inderjit Dhillon: square deviation is F of X


357
00:37:38.528 --> 00:37:46.088
Inderjit Dhillon: from the mean mean is expectation of F of X, okay. Square.


358
00:37:50.868 --> 00:37:56.377
Inderjit Dhillon: Okay. Now, if I take the quantity within the brackets, if I


359
00:37:56.768 --> 00:38:03.007
Inderjit Dhillon: open it up, I get F of expectation of F of x squared, plus.


360
00:38:03.658 --> 00:38:10.357
Inderjit Dhillon: Well, you know, it's basically a minus B square, right? So I have a square plus. I have this


361
00:38:10.548 --> 00:38:13.298
Inderjit Dhillon: mean of F of X.


362
00:38:14.488 --> 00:38:21.137
Inderjit Dhillon: Just be careful where you put the squares and where you put the brackets right minus. I have 2 of


363
00:38:21.478 --> 00:38:25.048
Inderjit Dhillon: this part, which is F of x times. This part


364
00:38:26.688 --> 00:38:30.818
Inderjit Dhillon: F of x times expectation of F of


365
00:38:34.718 --> 00:38:37.788
Inderjit Dhillon: and remember, since expectation is linear.


366
00:38:38.148 --> 00:38:42.918
Inderjit Dhillon: I have expectation of F of X square.


367
00:38:43.818 --> 00:38:48.278
Inderjit Dhillon: Okay, plus, I have expectation of


368
00:38:49.658 --> 00:38:55.448
Inderjit Dhillon: expectation of F of X. I'm just expanding out. Thanks. Again.


369
00:38:55.598 --> 00:38:58.878
Inderjit Dhillon: I could probably go quicker. But I just want to show all the steps


370
00:38:59.358 --> 00:39:07.298
Inderjit Dhillon: right? So 2 of I have expectation of F of X lines, expectation of associates.


371
00:39:10.068 --> 00:39:14.338
Inderjit Dhillon: Okay, this 1st part remains the same. F of X.


372
00:39:16.358 --> 00:39:26.017
Inderjit Dhillon: The next part is basically expectation of expectation, which is a constant. So I get rid of the outer expectation or inner expectation whatever. You


373
00:39:27.578 --> 00:39:31.058
Inderjit Dhillon: okay? And then over here I have


374
00:39:31.268 --> 00:39:41.407
Inderjit Dhillon: 2 times. This is kind of a constant. So I have expectation of F of X appearing times the expectation of F of x. Right. So 2 times


375
00:39:41.888 --> 00:39:48.927
Inderjit Dhillon: expectation of F of X appearing twice right. So these 2 give me


376
00:39:49.558 --> 00:39:52.687
Inderjit Dhillon: expectation of F of X square


377
00:39:54.618 --> 00:40:01.658
Inderjit Dhillon: minus. I take the expectation of F of X, and then I square it.


378
00:40:01.798 --> 00:40:18.907
Inderjit Dhillon: Okay, so very important to notice where the squares are right. So the square inside is inside the expectation. And here you're taking square off the expectation. Okay? So in particular, you probably have seen this formula in, you know, of course, in statistics, that if you have a variance of X,


379
00:40:19.388 --> 00:40:29.928
Inderjit Dhillon: then I have. It is expectation. If I if I go back to this original formula, right? I've just replace F of X is the F is the identity function.


380
00:40:30.078 --> 00:40:32.788
Inderjit Dhillon: So it's expectation of X minus


381
00:40:36.248 --> 00:40:39.748
Inderjit Dhillon: So it's a deviation of each observation X


382
00:40:39.938 --> 00:40:47.388
Inderjit Dhillon: from its me. Okay, so it's X minus E of X script.


383
00:40:49.298 --> 00:40:54.998
Inderjit Dhillon: And like what we did above, you can go through all the steps in particular for this. But you know we've already done this above.


384
00:40:55.238 --> 00:41:00.248
Inderjit Dhillon: this is expectation of X squared minus.


385
00:41:01.168 --> 00:41:03.877
Inderjit Dhillon: You take the expectation and then you square it.


386
00:41:10.848 --> 00:41:11.768
Inderjit Dhillon: Okay.


387
00:41:13.978 --> 00:41:26.778
Inderjit Dhillon: in addition to variance, when we have 2 or more random variables. And in particular, we, for example, if I have a 2 dimensional random variable or a d dimensional, random, variable.


388
00:41:28.118 --> 00:41:31.248
Inderjit Dhillon: Another thing that comes up is the covariance.


389
00:41:36.078 --> 00:41:40.228
Inderjit Dhillon: Okay? And many times we'll use 12 as the


390
00:41:40.418 --> 00:41:47.397
Inderjit Dhillon: denote the covariance. So the covariance of XY is basically


391
00:41:48.488 --> 00:42:01.908
Inderjit Dhillon: how this they kind of behave from the deviation from the mean. So x minus expectation of x, for


392
00:42:02.738 --> 00:42:08.997
Inderjit Dhillon: be a little careful times, y, minus expectation.


393
00:42:13.088 --> 00:42:13.988
Inderjit Dhillon: Okay?


394
00:42:14.678 --> 00:42:22.138
Inderjit Dhillon: And then, if you again kind of go through the similar exercise as before, what you will find is that this is expectation of


395
00:42:22.398 --> 00:42:28.348
Inderjit Dhillon: XY. Minus expectation of X expectation as well.


396
00:42:29.888 --> 00:42:34.567
Inderjit Dhillon: Right? So covariance of X with itself is just going to be the variance.


397
00:42:34.818 --> 00:42:40.387
Inderjit Dhillon: and that's what you'll get. You'll get expectation of X squared minus expectation of x whole square.


398
00:42:40.598 --> 00:42:43.118
Inderjit Dhillon: and that is, if X equal to y.


399
00:42:44.978 --> 00:42:49.788
Inderjit Dhillon: Now, we've noticed you know, we talked about a little bit about independent


400
00:42:50.098 --> 00:42:56.408
Inderjit Dhillon: random variables. Right? So what if X and Y or independent?


401
00:43:00.418 --> 00:43:02.098
Inderjit Dhillon: Okay, we have.


402
00:43:03.548 --> 00:43:10.238
Inderjit Dhillon: If that's the case, then I have p of x comma y is equal to px times BY.


403
00:43:10.978 --> 00:43:14.698
Inderjit Dhillon: So if I have covariance of x comma y.


404
00:43:15.068 --> 00:43:20.347
Inderjit Dhillon: Remember the covariance of x comma y is x expectation of XY.


405
00:43:20.678 --> 00:43:25.097
Inderjit Dhillon: Minus expectation of x expectation of why.


406
00:43:25.288 --> 00:43:31.887
Inderjit Dhillon: And if you think about what this is right, this is basically p of X, comma y


407
00:43:32.418 --> 00:43:37.508
Inderjit Dhillon: times XY integral dy. Dx.


408
00:43:38.488 --> 00:43:43.698
Inderjit Dhillon: right? And if this breaks up into px, PY.


409
00:43:44.468 --> 00:43:51.258
Inderjit Dhillon: I can write this as Xbx times. Xp, y.


410
00:43:52.138 --> 00:43:57.008
Inderjit Dhillon: okay, and that's expectation of X time. Expectation of what?


411
00:43:58.518 --> 00:43:59.328
Inderjit Dhillon: Okay?


412
00:44:00.948 --> 00:44:05.178
Inderjit Dhillon: And if I do this, then what do I get that covariance of XY


413
00:44:05.618 --> 00:44:09.757
Inderjit Dhillon: is equal to expectation of x times. Expectation of y


414
00:44:10.178 --> 00:44:14.178
Inderjit Dhillon: minus expectation of x times. Expectation of y.


415
00:44:14.788 --> 00:44:20.268
Inderjit Dhillon: so it's equal to 0. Okay, if oops NYR.


416
00:44:21.698 --> 00:44:22.688
Inderjit Dhillon: Independent.


417
00:44:25.458 --> 00:44:27.948
Inderjit Dhillon: Okay, so the covariance is 0.


418
00:44:28.078 --> 00:44:36.788
Inderjit Dhillon: If my 2 variables are independent. Right? So, covariance of XY, equal 0 F.


419
00:44:37.568 --> 00:44:41.618
Inderjit Dhillon: X and YR, I'm different.


420
00:44:55.678 --> 00:44:59.907
Inderjit Dhillon: Okay, now let's talk a little bit about Dawson distribution


421
00:45:00.488 --> 00:45:03.375
Inderjit Dhillon: and what my goal is to


422
00:45:05.528 --> 00:45:14.667
Inderjit Dhillon: Tell you what a multivariate Gaussian distribution is right. I don't know how many of you have seen that almost all of you have seen. I'm sure all of you have seen


423
00:45:14.778 --> 00:45:24.028
Inderjit Dhillon: what a univariate Gaussian distribution is okay. So let's start with that. But we will. What? Towards? By the end of class, we will talk about the


424
00:45:24.178 --> 00:45:26.017
Inderjit Dhillon: multivated costume distribution.


425
00:45:27.248 --> 00:45:29.958
Inderjit Dhillon: So I have a Gaussian distribution.


426
00:45:34.888 --> 00:45:37.928
Inderjit Dhillon: Okay? And that's also called the normal display.


427
00:45:51.748 --> 00:45:55.958
Inderjit Dhillon: Okay, so we'll assume in the univariate case that X belongs to R,


428
00:45:56.798 --> 00:45:59.748
Inderjit Dhillon: okay, and the Gaussian distribution. X.


429
00:46:00.458 --> 00:46:05.687
Inderjit Dhillon: Okay, so there are 2 parameters on a Gaussian distribution, mean and variance.


430
00:46:06.668 --> 00:46:10.967
Inderjit Dhillon: Okay? So given particular X and Sigma Square.


431
00:46:11.158 --> 00:46:18.498
Inderjit Dhillon: And many times we'll just, you know not write the given part, we'll just say Pfx or the Pdf.


432
00:46:18.708 --> 00:46:23.117
Inderjit Dhillon: Is given by this particular formula, one divided by square root 2 pi


433
00:46:23.378 --> 00:46:33.258
Inderjit Dhillon: times Sigma E to the power minus half x minus mu square, divided by Sigma Square.


434
00:46:34.538 --> 00:46:42.678
Inderjit Dhillon: Remember that mu is the well, not remember. But mu is the name or expectations.


435
00:46:45.278 --> 00:46:48.907
Inderjit Dhillon: Okay, and Signal Square is the variance.


436
00:46:49.998 --> 00:46:56.908
Inderjit Dhillon: So, for example, if you take this Pdf, and you actually calculate the integral of X times, Px.


437
00:46:57.158 --> 00:47:06.178
Inderjit Dhillon: you will get me, and if you compute the integral of you know. oh.


438
00:47:06.428 --> 00:47:10.697
Inderjit Dhillon: that corresponds to the variance. Then you will get sigma squared.


439
00:47:11.578 --> 00:47:13.648
Inderjit Dhillon: Okay? So in particular.


440
00:47:14.748 --> 00:47:16.278
Inderjit Dhillon: Xpx.


441
00:47:17.078 --> 00:47:18.148
Inderjit Dhillon: Dx.


442
00:47:18.898 --> 00:47:22.618
Inderjit Dhillon: Minus infinity to infinity is equal to mu.


443
00:47:23.508 --> 00:47:25.718
Inderjit Dhillon: This is the expectation of X.


444
00:47:26.498 --> 00:47:31.378
Inderjit Dhillon: And then, if you compute expectation of X minus mu square.


445
00:47:31.568 --> 00:47:38.297
Inderjit Dhillon: Remember, this is the same as expectation of X square minus mu squared.


446
00:47:38.798 --> 00:47:44.528
Inderjit Dhillon: That's expectation of X Square.


447
00:47:44.918 --> 00:47:47.337
Inderjit Dhillon: This is equal to Sigma Square.


448
00:47:49.308 --> 00:47:54.200
Inderjit Dhillon: so that I'm pretty sure that all of you have seen. But now let's move to the


449
00:47:55.458 --> 00:48:02.627
Inderjit Dhillon: to the multivariate case. So now suppose I have X, which is a D dimensional. Vector.


450
00:48:03.398 --> 00:48:10.988
Inderjit Dhillon: okay, so X has multiple components x, 1, next to XD,


451
00:48:12.208 --> 00:48:17.508
Inderjit Dhillon: so I think you should be able to clearly see that if I do expectation of X


452
00:48:18.108 --> 00:48:24.848
Inderjit Dhillon: that's going to be expectation of mu, one x. 1, which is mu one.


453
00:48:25.268 --> 00:48:30.588
Inderjit Dhillon: Let's denote that by mu one expectation of x. 2, which we do not by mu, 2,


454
00:48:31.148 --> 00:48:36.217
Inderjit Dhillon: and expectation of xt, which we do not buy new D,


455
00:48:36.478 --> 00:48:46.328
Inderjit Dhillon: and this we'll call as Mu. So remember, Mu is a vector Mu belongs to our okay?


456
00:48:47.598 --> 00:48:53.978
Inderjit Dhillon: And let's just look at the univariate variances.


457
00:48:54.238 --> 00:49:06.258
Inderjit Dhillon: Right? So if I look at Xi minus mu, I square, let's say, these are Sigma, I squared.


458
00:49:07.638 --> 00:49:09.947
Inderjit Dhillon: Yeah. So there are D of these


459
00:49:13.388 --> 00:49:16.188
Inderjit Dhillon: variances for each component of the vector.


460
00:49:17.058 --> 00:49:20.157
Inderjit Dhillon: but what we are interested in is a joint modeling.


461
00:49:20.478 --> 00:49:25.508
Inderjit Dhillon: So remember, X is an Rd, so really, X is a vector.


462
00:49:25.688 --> 00:49:29.007
Inderjit Dhillon: so the joint distribution of X is.


463
00:49:29.268 --> 00:49:35.057
Inderjit Dhillon: So if I think about the kind of notation we had before.


464
00:49:35.788 --> 00:49:42.627
Inderjit Dhillon: That's it. We are looking for the joint distribution of each of the components of the vector X.


465
00:49:45.178 --> 00:49:54.857
Inderjit Dhillon: So what is the multivariate Gaussian distribution. Well, instead of going straight to the Gaussian distribution, let's let's take a simple example first, st


466
00:49:56.138 --> 00:50:00.658
Inderjit Dhillon: and then we will extend it to the multivariate Gaussium distribution.


467
00:50:02.108 --> 00:50:05.427
Inderjit Dhillon: So let me 1st take.


468
00:50:05.978 --> 00:50:10.838
Inderjit Dhillon: let's assume that the Xi's are all independent of each other.


469
00:50:12.628 --> 00:50:21.547
Inderjit Dhillon: Suppose Xi is independent of Xj


470
00:50:22.218 --> 00:50:24.528
Inderjit Dhillon: for all, I not equal to.


471
00:50:25.578 --> 00:50:29.137
Inderjit Dhillon: Okay. So they are all independent of each other. And we know


472
00:50:29.358 --> 00:50:33.367
Inderjit Dhillon: that the joint distribution in that case is just the product.


473
00:50:34.028 --> 00:50:41.108
Inderjit Dhillon: Okay? So maybe I'll just write it in case you're not familiar with this notation, but it is


474
00:50:41.668 --> 00:50:44.428
Inderjit Dhillon: bfx one vfx, 2


475
00:50:45.638 --> 00:50:50.538
Inderjit Dhillon: u of XD. That's the product of this, and that is denoted by the symbol.


476
00:50:50.648 --> 00:50:56.107
Inderjit Dhillon: This is the product I equal to one to D of pure function.


477
00:50:58.378 --> 00:51:02.398
Inderjit Dhillon: Okay. And now let's write it out right. What is P. Of xi.


478
00:51:03.528 --> 00:51:05.278
Inderjit Dhillon: Well, Xi. Is.


479
00:51:05.998 --> 00:51:09.028
Inderjit Dhillon: But you know it's a Gaussian distribution.


480
00:51:09.438 --> 00:51:15.367
Inderjit Dhillon: And we saw that this was the formula, or P of xr.


481
00:51:16.638 --> 00:51:18.228
Inderjit Dhillon: So p. Of xi.


482
00:51:18.848 --> 00:51:22.927
Inderjit Dhillon: Is going to be one divided by square root, 2 pi sigma, one


483
00:51:24.238 --> 00:51:27.348
Inderjit Dhillon: times E to the power x minus mu.


484
00:51:27.828 --> 00:51:35.528
Inderjit Dhillon: one divided by Sigma, one square. Right? So, for for so this will be one divided by


485
00:51:37.188 --> 00:51:42.177
Inderjit Dhillon: square root 2 pi times, sigma one.


486
00:51:43.258 --> 00:51:48.718
Inderjit Dhillon: But let's just look at that. This thing which is before the exponential right? How many times will it appear?


487
00:51:49.858 --> 00:51:51.877
Inderjit Dhillon: There are DD times right?


488
00:51:52.318 --> 00:52:00.287
Inderjit Dhillon: There are d terms over here, so this will appear d times, and then


489
00:52:00.528 --> 00:52:04.497
Inderjit Dhillon: this will be Sigma one sigma, 2, 2, sigma, 2.


490
00:52:09.178 --> 00:52:14.757
Inderjit Dhillon: Okay. And then the thing within the exponential. It will be a product of


491
00:52:15.068 --> 00:52:17.608
Inderjit Dhillon: E to the power so same as here.


492
00:52:18.598 --> 00:52:22.167
Inderjit Dhillon: same as here. I'm just rewriting it down over here.


493
00:52:22.328 --> 00:52:27.117
Inderjit Dhillon: E to the power minus one half of XI


494
00:52:27.348 --> 00:52:33.188
Inderjit Dhillon: minus mu, I square, divided by Sigma I square.


495
00:52:34.368 --> 00:52:37.358
Inderjit Dhillon: and the product is from I equal to one through B,


496
00:52:39.328 --> 00:52:42.147
Inderjit Dhillon: okay, so I will rewrite this


497
00:52:43.018 --> 00:52:48.088
Inderjit Dhillon: as one divided by 2 to the power pi d. By 2.


498
00:52:49.538 --> 00:52:52.357
Inderjit Dhillon: I haven't really done anything. I just take the square root


499
00:52:52.728 --> 00:52:57.727
Inderjit Dhillon: right, written it as power half and written it like this.


500
00:52:58.288 --> 00:53:03.377
Inderjit Dhillon: And then I have Sigma one sigma, 2, 2, Sigma d.


501
00:53:07.868 --> 00:53:08.818
Inderjit Dhillon: Okay.


502
00:53:09.348 --> 00:53:15.748
Inderjit Dhillon: And now you need to notice one thing. This is the product of exponentials.


503
00:53:17.008 --> 00:53:23.048
Inderjit Dhillon: Right? So if I have E to the power, a times E to the power B,


504
00:53:23.608 --> 00:53:26.968
Inderjit Dhillon: that's equal to E to the power a plus p.


505
00:53:28.318 --> 00:53:34.988
Inderjit Dhillon: So what happens is that I can basically turn this product into a sum if I take it into the exponent.


506
00:53:35.438 --> 00:53:38.678
Inderjit Dhillon: So this becomes E to the power minus one half


507
00:53:39.238 --> 00:53:43.418
Inderjit Dhillon: summation of Xi minus mu i


508
00:53:43.998 --> 00:53:46.788
Inderjit Dhillon: square, divided by Sigma. I square


509
00:53:50.068 --> 00:53:51.637
Inderjit Dhillon: any questions so far.


510
00:53:54.938 --> 00:53:58.598
Inderjit Dhillon: and remember the summation is I equal to one today.


511
00:54:10.258 --> 00:54:16.337
Inderjit Dhillon: So what I can do is I can actually take matrix sigma


512
00:54:17.073 --> 00:54:19.402
Inderjit Dhillon: by the way, sorry, because


513
00:54:20.748 --> 00:54:25.268
Inderjit Dhillon: I don't think I can avoid it. But you know, this is a matrix. Sigma.


514
00:54:26.178 --> 00:54:31.088
Inderjit Dhillon: okay? Whereas this is the noted notation for a summation.


515
00:54:31.518 --> 00:54:35.637
Inderjit Dhillon: Okay? But I'm actually collecting things in the matrix sigma, which is


516
00:54:35.948 --> 00:54:39.317
Inderjit Dhillon: Sigma, one square sigma, 2 square


517
00:54:40.168 --> 00:54:43.458
Inderjit Dhillon: sigma D squared. So it's a diagonal matrix


518
00:54:45.438 --> 00:54:48.508
Inderjit Dhillon: with the variances on the diagonal


519
00:54:49.088 --> 00:54:53.368
Inderjit Dhillon: and everything in the off diagonal is 0.


520
00:54:54.818 --> 00:54:57.127
Inderjit Dhillon: What is the determinant of this matrix.


521
00:55:02.786 --> 00:55:05.297
Vishal Thyagarajan: Like the products of the diagonals.


522
00:55:05.688 --> 00:55:09.507
Inderjit Dhillon: Determinant of this matrix is the product of the diagonal. So it's


523
00:55:09.808 --> 00:55:14.998
Inderjit Dhillon: Sigma, one square sigma, 2 square sigma d square.


524
00:55:15.668 --> 00:55:21.018
Inderjit Dhillon: So it's Sigma one sigma 2 times sigma d squirrel.


525
00:55:21.948 --> 00:55:34.068
Inderjit Dhillon: So this means that I can write this as one divided by 2 pi d. Divided by 2


526
00:55:35.218 --> 00:55:40.978
Inderjit Dhillon: times the square root of the determinant.


527
00:55:45.348 --> 00:55:47.468
Inderjit Dhillon: And remember, the determinant is a number.


528
00:55:57.658 --> 00:55:58.798
Inderjit Dhillon: Is that clear?


529
00:56:00.368 --> 00:56:07.328
Inderjit Dhillon: Okay? And now, what about this term over here?


530
00:56:08.758 --> 00:56:10.178
Inderjit Dhillon: This term here?


531
00:56:11.658 --> 00:56:18.467
Inderjit Dhillon: Xi, minus Mu. I square, divided by Sigma Square.


532
00:56:22.508 --> 00:56:28.588
Vishal Thyagarajan: Maybe you can turn like Xi and mu I into vectors, and you can multiply them


533
00:56:29.568 --> 00:56:36.927
Vishal Thyagarajan: by. You cannot take, like the dot product of that of the vector minus the x vector minus the


534
00:56:37.118 --> 00:56:41.978
Vishal Thyagarajan: mean vector squared, you can take that vector and you can multiply it by the inverse of


535
00:56:42.138 --> 00:56:43.598
Vishal Thyagarajan: the sigma matrix.


536
00:56:43.748 --> 00:56:46.988
Inderjit Dhillon: Okay, yeah, yeah. So let's let's go through it slowly.


537
00:56:47.178 --> 00:56:55.798
Inderjit Dhillon: So suppose I have X minus mu transpose times X minus, mu.


538
00:56:56.178 --> 00:56:59.148
Inderjit Dhillon: okay, remember, now, X minus mu is a vector huh


539
00:56:59.558 --> 00:57:02.297
Inderjit Dhillon: X minus mu is equal to


540
00:57:02.698 --> 00:57:05.658
Inderjit Dhillon: Xi minus mu, I sorry x


541
00:57:06.968 --> 00:57:16.918
Inderjit Dhillon: X minus mu is a, vector and that is x, 1 minus u, 1 x 2 minus mu, 2


542
00:57:17.348 --> 00:57:19.697
Inderjit Dhillon: to XD, minus mu d.


543
00:57:21.208 --> 00:57:25.688
Inderjit Dhillon: So if I write x minus mu transpose times x minus Mu.


544
00:57:26.058 --> 00:57:30.987
Inderjit Dhillon: That is well, we can see it's going to be the sum of the squares, right?


545
00:57:31.388 --> 00:57:33.338
Inderjit Dhillon: The information of I.


546
00:57:33.558 --> 00:57:36.577
Inderjit Dhillon: Xi minus mu I square.


547
00:57:37.288 --> 00:57:39.817
Inderjit Dhillon: So it's starting to look a little like this.


548
00:57:41.738 --> 00:57:44.508
Inderjit Dhillon: except I don't have sigma, I square.


549
00:57:44.688 --> 00:57:49.177
Inderjit Dhillon: Okay? So then, if my sigma is diagonal


550
00:57:50.508 --> 00:58:02.287
Inderjit Dhillon: X minus mu transpose times, sigma inverse X minus mu is formation of I Xi, minus


551
00:58:02.758 --> 00:58:06.707
Inderjit Dhillon: mu, I square, divided by Sigma Square.


552
00:58:08.678 --> 00:58:12.488
Inderjit Dhillon: Okay? And remember that sigma is diagonal.


553
00:58:15.098 --> 00:58:22.228
Inderjit Dhillon: Let me just write it again. Right? Sigma. One square sigma, 2 square to not do. Score


554
00:58:26.028 --> 00:58:28.118
Inderjit Dhillon: this matrix signal.


555
00:58:29.688 --> 00:58:35.607
Inderjit Dhillon: Yeah. I apologize because you have to realize that there are 2 sigmas out here. Right? So just by context.


556
00:58:35.788 --> 00:58:40.637
Inderjit Dhillon: this is the matrix of the variances. And this is the summation sign side.


557
00:58:41.078 --> 00:58:44.818
Inderjit Dhillon: That's I equal to one. Today. I equal one to D.


558
00:58:49.618 --> 00:58:54.638
Inderjit Dhillon: Okay, so this says that my joint distribution in the case


559
00:58:55.338 --> 00:59:00.868
Inderjit Dhillon: when I have independence between excise is.


560
00:59:01.228 --> 00:59:07.707
Inderjit Dhillon: I can write it as one divided by 2 pi d. Divided by 2


561
00:59:08.368 --> 00:59:12.388
Inderjit Dhillon: determinant of sigma to the power half


562
00:59:13.018 --> 00:59:16.628
Inderjit Dhillon: times E to the power of minus one half


563
00:59:17.208 --> 00:59:22.167
Inderjit Dhillon: x minus mu transpose sigma inverse X minus mu.


564
00:59:32.548 --> 00:59:35.067
Inderjit Dhillon: Okay, so I just wanted to kind of just show you


565
00:59:35.298 --> 00:59:39.707
Inderjit Dhillon: that in the case of independence I can write it like this.


566
00:59:40.238 --> 00:59:43.028
Inderjit Dhillon: Okay? Now let's look at the general case.


567
00:59:46.408 --> 00:59:47.838
Inderjit Dhillon: General case.


568
00:59:48.098 --> 00:59:53.458
Inderjit Dhillon: Okay, in the general case, I'm not assuming that the excise are independent.


569
00:59:53.918 --> 00:59:57.528
Inderjit Dhillon: So I have X belongs to Rd.


570
00:59:58.708 --> 01:00:06.968
Inderjit Dhillon: okay? And let me. So I have a multivariate Gaussian distribution.


571
01:00:13.508 --> 01:00:19.458
Inderjit Dhillon: Okay? And the Pdf, what's it given by? Well, guess what?


572
01:00:20.358 --> 01:00:26.077
Inderjit Dhillon: It's going to be identical to this, except Sigma will not be the matrix. Sigma will not be dialed


573
01:00:26.848 --> 01:00:40.768
Inderjit Dhillon: one divided by 2 pi b. By 2 determinant of sigma to the powerhouse E to the power minus


574
01:00:41.348 --> 01:00:47.978
Inderjit Dhillon: one half x minus mu transpose sigma inverse x minus mu.


575
01:00:50.018 --> 01:00:52.028
Inderjit Dhillon: Okay, and where where.


576
01:00:55.258 --> 01:00:59.098
Inderjit Dhillon: as before, mu is the memes.


577
01:01:02.428 --> 01:01:07.008
Inderjit Dhillon: that is, mu is equal to expectation of X.


578
01:01:08.008 --> 01:01:15.208
Inderjit Dhillon: Okay, and that's just the same as before me. One, me 2 room ud


579
01:01:17.168 --> 01:01:20.557
Inderjit Dhillon: near. I was equal to expectation of outside.


580
01:01:21.458 --> 01:01:27.208
Inderjit Dhillon: But my matrix sigma is the


581
01:01:28.088 --> 01:01:33.057
Inderjit Dhillon: remember earlier, it was a diagonal or diagonal matrix with variances on it.


582
01:01:33.908 --> 01:01:37.338
Inderjit Dhillon: But in general it'll be the covariance matrix.


583
01:01:48.088 --> 01:01:58.238
Inderjit Dhillon: that is, it's defined to be expectation of X minus mu minus mu. That's place.


584
01:01:59.038 --> 01:02:15.568
Inderjit Dhillon: And in particular, Sigma, IJ is the covariance between Xi. And let's do it.


585
01:02:17.198 --> 01:02:22.698
Inderjit Dhillon: Okay? So the covariance matrix has the following property, remember, it is going to be a


586
01:02:23.318 --> 01:02:26.977
Inderjit Dhillon: D by d matrix, right? That's pretty easy to see


587
01:02:28.218 --> 01:02:38.127
Inderjit Dhillon: it is going to be symmetric, symmetric. Okay? And is also all the development.


588
01:02:39.698 --> 01:02:42.027
Inderjit Dhillon: Remember what positive, definite means


589
01:02:42.538 --> 01:02:51.958
Inderjit Dhillon: that it is all. The quadratic form is greater than equal to all quadratic forms are greater than equal to 0. Which means that


590
01:02:52.078 --> 01:02:55.347
Inderjit Dhillon: the eigenvalues of this matrix are all.


591
01:02:56.888 --> 01:03:00.918
Inderjit Dhillon: They're not only real because it's a symmetric matrix, but they're also


592
01:03:01.388 --> 01:03:05.098
Inderjit Dhillon: greater than 0. So that means, if I have, sigma


593
01:03:05.438 --> 01:03:12.718
Inderjit Dhillon: is v lambda, V transpose right, which is the eigenvalue decomposition.


594
01:03:17.538 --> 01:03:24.328
Inderjit Dhillon: the only set of matrices for which eigenvalue, decomposition, and the singular value decomposition are identical are the


595
01:03:24.968 --> 01:03:30.817
Inderjit Dhillon: some positive, definite matrices. Positive semi-definite it's either you can call it the Svd. Or


596
01:03:31.981 --> 01:03:38.508
Inderjit Dhillon: eigenvalue decomposition, you know. So Lambda is diagonal.


597
01:03:40.268 --> 01:03:46.708
Inderjit Dhillon: Okay, Lambda, I I is great than 0, and


598
01:03:52.308 --> 01:03:54.298
Inderjit Dhillon: the matrices VR.


599
01:03:56.738 --> 01:04:06.787
Inderjit Dhillon: orthogonal orthogonal matrices. Right? So they are the orthogonal matrices, orthogonal matrix matrices, where the columns correspond to the eigenvectors of sigma


600
01:04:07.198 --> 01:04:13.578
Inderjit Dhillon: and then what happens. So so we have Sigma inverse here in this formula.


601
01:04:18.078 --> 01:04:24.888
Inderjit Dhillon: Okay, so Sigma inverse can be written as V. Lambda, inverse. V transpose.


602
01:04:26.788 --> 01:04:27.848
Inderjit Dhillon: Okay?


603
01:04:29.228 --> 01:04:34.237
Inderjit Dhillon: And so the quadratic form, half of X minus mu transpose


604
01:04:34.678 --> 01:04:36.798
Inderjit Dhillon: thing that appears in the exponent


605
01:04:38.568 --> 01:04:41.408
Inderjit Dhillon: of the Gaussian distribution over here.


606
01:04:42.818 --> 01:04:43.858
Inderjit Dhillon: Okay.


607
01:04:47.418 --> 01:04:53.417
Inderjit Dhillon: that I can write as half X minus mu transpose v.


608
01:04:54.508 --> 01:04:59.628
Inderjit Dhillon: Lambda. Inverse V transpose x minus v.


609
01:05:01.728 --> 01:05:05.747
Inderjit Dhillon: Okay, so what I can do is I can actually form another variable


610
01:05:06.608 --> 01:05:09.177
Inderjit Dhillon: Z, which is equal to this.


611
01:05:12.888 --> 01:05:16.658
Inderjit Dhillon: or maybe yeah, I won't do that.


612
01:05:17.818 --> 01:05:18.518
Inderjit Dhillon: Go ahead.


613
01:05:21.128 --> 01:05:24.537
Inderjit Dhillon: Okay, let me just kind of use a different color.


614
01:05:25.228 --> 01:05:28.937
Inderjit Dhillon: Now let me say that I'll make this equal to 0.


615
01:05:30.528 --> 01:05:36.787
Inderjit Dhillon: Okay, so I have. Z is equal to V transpose x minus mu.


616
01:05:38.368 --> 01:05:42.437
Inderjit Dhillon: which means that half of X minus mu


617
01:05:42.908 --> 01:05:50.558
Inderjit Dhillon: transport just from copying from above is equal to one half of


618
01:05:51.358 --> 01:05:54.867
Inderjit Dhillon: ZO, my 2 is starting to look like a Z.


619
01:05:55.298 --> 01:05:56.168
Inderjit Dhillon: Sorry.


620
01:05:56.898 --> 01:05:59.338
Inderjit Dhillon: Okay, it's equal to one half


621
01:05:59.648 --> 01:06:04.457
Inderjit Dhillon: of I don't think I made it any better, but it is Z.


622
01:06:05.488 --> 01:06:13.177
Inderjit Dhillon: Let me type vitamin. C transpose lambda inverse times. Z,


623
01:06:20.418 --> 01:06:21.288
Inderjit Dhillon: okay?


624
01:06:22.828 --> 01:06:28.173
Inderjit Dhillon: And if you notice like, if I just ask you, like, what are the contours of


625
01:06:29.227 --> 01:06:35.338
Inderjit Dhillon: equal probability density, then this will be P. Of x equal to C, which is


626
01:06:35.648 --> 01:06:42.688
Inderjit Dhillon: that so it implies that you know I have half of Z transpose


627
01:06:43.168 --> 01:06:47.068
Inderjit Dhillon: lambda inverse. Z is equal to some C line.


628
01:06:47.838 --> 01:06:55.258
Inderjit Dhillon: Okay? And if you think about it hard enough, or you can just expand it. You'll see that this is equal to summation of


629
01:06:55.748 --> 01:07:05.068
Inderjit Dhillon: ZI square, divided by Sigma, I square the lambda is well.


630
01:07:06.908 --> 01:07:10.478
Inderjit Dhillon: well, let me call. Lambda is equal to lambda, one through lambda. D.


631
01:07:11.097 --> 01:07:13.608
Inderjit Dhillon: Okay, then, in this case it is


632
01:07:14.248 --> 01:07:18.257
Inderjit Dhillon: lambda. T right. But remember that lam sorry, lambda. I


633
01:07:18.758 --> 01:07:26.158
Inderjit Dhillon: remember that lambda I's are all greater than equal to 0. So this is. And if you know, if you


634
01:07:28.807 --> 01:07:31.178
Inderjit Dhillon: this is just the equation of an ellipse.


635
01:07:39.727 --> 01:07:43.607
Inderjit Dhillon: Okay, so what that means is that in Z,


636
01:07:44.428 --> 01:07:48.138
Inderjit Dhillon: right? My, and this is ellipse centered at 0.


637
01:07:49.918 --> 01:07:55.018
Inderjit Dhillon: Okay, so this particular case is, you know, E of X equal to some. C.


638
01:07:55.908 --> 01:07:56.828
Inderjit Dhillon: Okay.


639
01:07:57.568 --> 01:08:05.797
Inderjit Dhillon: the reason I'm drawing it is but that many times when you see a Gaussian distribution, you will see the following picture, which is


640
01:08:05.988 --> 01:08:08.681
Inderjit Dhillon: that. And when you look at things like


641
01:08:09.118 --> 01:08:19.788
Inderjit Dhillon: principal components analysis that you might have heard about right, you'll actually see a picture like this, not not the same as what I've drawn over here, because this is centered at the origin, right?


642
01:08:20.504 --> 01:08:25.438
Inderjit Dhillon: Sorry for the bad picture. By the way, this is supposed to be. These are all supposed to be


643
01:08:25.648 --> 01:08:27.298
Inderjit Dhillon: symmetric ellipses.


644
01:08:27.438 --> 01:08:33.798
Inderjit Dhillon: Okay, so let me see if I can draw it a little bit better. But but or or similar figure not centered at 0 and not


645
01:08:33.998 --> 01:08:40.868
Inderjit Dhillon: access parallel right? So whenever you see things like Pca or so you'll see. Maybe I'll draw it below.


646
01:08:41.328 --> 01:08:44.868
Inderjit Dhillon: You'll see something like, you know there's a center over here.


647
01:08:56.528 --> 01:08:59.588
Inderjit Dhillon: I don't know if it's my drawing is any better. But


648
01:08:59.888 --> 01:09:02.667
Inderjit Dhillon: you know this is v. 1.


649
01:09:03.488 --> 01:09:05.738
Inderjit Dhillon: This is v. 2.


650
01:09:06.288 --> 01:09:14.068
Inderjit Dhillon: And remember, I'm drawing it in 2 dimensions. But there are d orthogonal vectors over here. Right?


651
01:09:14.198 --> 01:09:22.317
Inderjit Dhillon: So this is P of X equal to C, and why is this kind of you know? So this is going to be Mu.


652
01:09:25.118 --> 01:09:26.108
Inderjit Dhillon: okay.


653
01:09:26.598 --> 01:09:30.898
Inderjit Dhillon: So if you notice that my transformation was this right?


654
01:09:32.188 --> 01:09:35.518
Inderjit Dhillon: So that kind of said that I made the origin equal to 0,


655
01:09:36.068 --> 01:09:41.597
Inderjit Dhillon: and then I did the orthogonal transformation. So when I undo that I get this.


656
01:09:41.718 --> 01:09:45.057
Inderjit Dhillon: so this is how sometimes how the Pdf.


657
01:09:47.178 --> 01:09:55.007
Inderjit Dhillon: Or you know, contour plot of Pdf. Of Dawson.


658
01:09:56.408 --> 01:10:02.987
Inderjit Dhillon: okay. And these sigmas are supposed to represent. So these ellipse, this ellipse.


659
01:10:03.588 --> 01:10:08.088
Inderjit Dhillon: in some sense, represents the covariance matrix right? It would be a


660
01:10:08.618 --> 01:10:14.368
Inderjit Dhillon: it would. These would all be circles. If all the singular Well or Eigenvalues were identical.


661
01:10:14.608 --> 01:10:18.098
Inderjit Dhillon: Right? So the special case is that if all.


662
01:10:23.888 --> 01:10:25.637
Inderjit Dhillon: if, for example.


663
01:10:29.098 --> 01:10:37.648
Inderjit Dhillon: it looks like this right, then this is kind of saying that my sigma is diagonal


664
01:10:38.668 --> 01:10:40.081
Inderjit Dhillon: right, and


665
01:10:44.678 --> 01:10:57.768
Inderjit Dhillon: and that all my eigenvalues are equal. So, for example, when you draw from a Gaussian, which is, you know, unit variance in each dimension, all the variables are independent. Then the covariance matrix would be equal to the identity.


666
01:10:59.138 --> 01:11:05.628
Inderjit Dhillon: Okay, so this is the control plot of Pdf of multimedias.


667
01:11:10.748 --> 01:11:18.558
Inderjit Dhillon: Okay, any questions about this. So you know, I. So so just to recap. You know. I started off by.


668
01:11:18.728 --> 01:11:23.399
Inderjit Dhillon: you know. Come somewhat slowly. Talking about discrete


669
01:11:24.701 --> 01:11:30.708
Inderjit Dhillon: joint distributions, discrete random variables, and we saw that there was a sum rule and a product rule.


670
01:11:30.888 --> 01:11:38.298
Inderjit Dhillon: and then we can extend it to probabilities, probability, density functions where we have continuous variables.


671
01:11:38.468 --> 01:11:41.268
Inderjit Dhillon: Similar concepts mean variance.


672
01:11:42.593 --> 01:11:51.288
Inderjit Dhillon: some rule product rule. And then we saw a particular case of distribution, which, of course, is ubiquitous.


673
01:11:51.488 --> 01:11:53.688
Inderjit Dhillon: The Gaussian distribution.


674
01:11:53.958 --> 01:12:00.328
Inderjit Dhillon: Now many books will give you this particular formula directly.


675
01:12:00.958 --> 01:12:07.187
Inderjit Dhillon: and they'll say that, hey, you know, this is the mean, and this is the covariance matrix. But I wanted to build intuition


676
01:12:07.578 --> 01:12:15.977
Inderjit Dhillon: right? So we showed that the in the case where all the because many of you have seen the univariate calcium distribution


677
01:12:16.388 --> 01:12:23.748
Inderjit Dhillon: right? That if the variables are identical. Then you can write the Pdf


678
01:12:24.338 --> 01:12:46.538
Inderjit Dhillon: of this in the same way as you write the Pdf of a general multivariate Gaussian. Of course Sigma is special over here. It's diagonal, right? And remember that earlier, we also showed that when X and Y are independent, where did I show it here when x and y are when Xi and xt are independent, that the covariance of Xi and xt are is equal to 0.


679
01:12:47.078 --> 01:12:49.848
Inderjit Dhillon: Okay? And so you can kind of make sense


680
01:12:50.288 --> 01:12:56.444
Inderjit Dhillon: of why sigma is diagonal ratio in general. It is it is


681
01:12:58.468 --> 01:13:05.108
Inderjit Dhillon: the expectation of the covariance. In general. It is a d by d dense matrix.


682
01:13:05.368 --> 01:13:06.161
Inderjit Dhillon: And


683
01:13:08.950 --> 01:13:26.058
Inderjit Dhillon: you can represent the Pdf like this. And when we come to we'll probably briefly mentioned the principal components. Analysis that's going to be kind of finding the principal components, which are v. 1 and v. 2, which are with some books, will say it's the direction of maximum variance


684
01:13:26.198 --> 01:13:27.678
Inderjit Dhillon: of the data.


685
01:13:29.088 --> 01:13:34.958
Inderjit Dhillon: Okay? So with that kind of, we conclude this again, background lecture on


686
01:13:35.405 --> 01:13:39.120
Inderjit Dhillon: property theory. And so from next time we'll start talking about


687
01:13:39.568 --> 01:13:47.047
Inderjit Dhillon: classification, and we'll see how the modeling that we did, for example, today will straight away. Come in in the next lecture.


688
01:13:48.118 --> 01:13:49.658
Inderjit Dhillon: Any questions.


689
01:13:55.818 --> 01:14:02.152
Inderjit Dhillon: Okay, is there no more question? Is there no questions then? We conclude, and


690
01:14:02.698 --> 01:14:15.897
Inderjit Dhillon: we will meet on Monday. So remember, there's a homework. Hope you have a good weekend, and then from Monday onwards we'll start talking about classification see you on Monday. Thanks bye.


691
01:14:16.628 --> 01:14:17.478
Hormoz Shahrzad: Thank you.


692
01:14:20.058 --> 01:14:20.798
Ritesh Thakur: Thank you.



---- END OF LECTURE -------- START OF LECTURE 8 ----
WEBVTT

1
00:00:03.829 --> 00:00:05.389
Inderjit Dhillon: We took


2
00:00:06.649 --> 00:00:12.608
Inderjit Dhillon: one lecture and did a review on linear algebra. And last time I did a review on


3
00:00:12.929 --> 00:00:23.328
Inderjit Dhillon: just, you know, simple probability theory. And then, towards the end of the lecture we talked about, you know the multivariate normal distribution, or the multivariate Gaussian distribution.


4
00:00:23.439 --> 00:00:29.959
Inderjit Dhillon: And today I'll talk about, you know, the 1st kind of lecture on classification. Look at some


5
00:00:30.199 --> 00:00:39.769
Inderjit Dhillon: basic methods. We'll end up working, looking at logistic regression and support vector machines over the next coming weeks.


6
00:00:40.359 --> 00:00:45.179
Inderjit Dhillon: But today is kind of the 1st lecture on classification. So let's start.


7
00:00:45.929 --> 00:00:51.529
Inderjit Dhillon: okay. So today, I will talk about classification.


8
00:00:52.318 --> 00:01:01.718
Inderjit Dhillon: So very similar. Setup, identical setup essentially to the regression problem. We have a training set


9
00:01:03.219 --> 00:01:14.098
Inderjit Dhillon: that's made of Xi's and yis, so x, 1, y. One x, 2 y. 2 XNYM,


10
00:01:14.199 --> 00:01:20.428
Inderjit Dhillon: okay. And xi, or, you know, think of them as some d dimensional vectors feature vectors


11
00:01:20.699 --> 00:01:24.099
Inderjit Dhillon: the goal, of course, here is that


12
00:01:24.699 --> 00:01:31.519
Inderjit Dhillon: we don't have yis to be real numbers. We have yis to be essentially corresponding to classes.


13
00:01:31.899 --> 00:01:36.939
Inderjit Dhillon: So the goal is to learn function. F,


14
00:01:40.369 --> 00:01:41.658
Inderjit Dhillon: that predicts


15
00:01:45.839 --> 00:01:47.799
Inderjit Dhillon: the class label.


16
00:01:49.139 --> 00:01:58.529
Inderjit Dhillon: Last label, why of some new point X,


17
00:02:01.439 --> 00:02:05.859
Inderjit Dhillon: okay, so let's take a, you know, simple motivating example.


18
00:02:07.799 --> 00:02:10.419
Inderjit Dhillon: Okay, let's consider the case where X


19
00:02:10.959 --> 00:02:14.959
Inderjit Dhillon: is just a 2 dimensional. Vector so just 2 features.


20
00:02:15.329 --> 00:02:20.269
Inderjit Dhillon: And suppose you know the distribution of sorry.


21
00:02:22.379 --> 00:02:32.449
Inderjit Dhillon: Okay, suppose the distribution of points is the following, suppose you know these crosses or represent the X's


22
00:02:32.959 --> 00:02:39.298
Inderjit Dhillon: that belong to Class One, and the circles are


23
00:02:39.899 --> 00:02:49.119
Inderjit Dhillon: the X's that belong to plus 2. Okay, so this, let's, this is c, 1, this is seed 2.


24
00:02:50.609 --> 00:02:51.579
Inderjit Dhillon: Okay.


25
00:02:52.629 --> 00:02:59.899
Inderjit Dhillon: So you know, just give having drawn the simple figure, what would be a good


26
00:03:00.958 --> 00:03:07.749
Inderjit Dhillon: way to distinguish between these 2 classes any thoughts.


27
00:03:11.729 --> 00:03:16.828
Inderjit Dhillon: So remember, you know, I basically just want a rule or a decision surface


28
00:03:17.209 --> 00:03:19.749
Inderjit Dhillon: saying, You know, what would I?


29
00:03:19.909 --> 00:03:23.709
Inderjit Dhillon: How would I recognize the 2 classes? So given a new X


30
00:03:23.919 --> 00:03:27.969
Inderjit Dhillon: right? Suppose a new X comes to be here.


31
00:03:28.869 --> 00:03:30.988
Inderjit Dhillon: what class would it give? Would you give it.


32
00:03:33.249 --> 00:03:34.769
Hormoz Shahrzad: e, 1 c, one.


33
00:03:35.169 --> 00:03:40.699
Inderjit Dhillon: See one right, and then given some point over here, I would give it.


34
00:03:41.329 --> 00:03:42.128
Hormoz Shahrzad: Me too.


35
00:03:42.129 --> 00:03:48.479
Inderjit Dhillon: Say 2 right? What if I now start getting somewhere in between?


36
00:03:50.569 --> 00:03:53.039
Inderjit Dhillon: So what would be a good decision rule?


37
00:03:53.539 --> 00:03:58.599
Inderjit Dhillon: So that you decide whether a new point X, which is given by 2 coordinates.


38
00:03:58.779 --> 00:04:01.948
Inderjit Dhillon: belongs to one class versus the other.


39
00:04:03.289 --> 00:04:05.418
Hormoz Shahrzad: Maybe the K. Nearest neighbors.


40
00:04:06.739 --> 00:04:13.079
Inderjit Dhillon: K. Nearest neighbors. That's possible. Yes, but you know I'm giving you. I've given you kind of this


41
00:04:13.389 --> 00:04:19.238
Inderjit Dhillon: visual right over here. There's a bunch of points here. There's a bunch of points here, and you might say, Hey, look!


42
00:04:20.039 --> 00:04:23.779
Inderjit Dhillon: Maybe I can draw like a line between these 2,


43
00:04:24.519 --> 00:04:33.499
Inderjit Dhillon: and everything on this side belongs to Class c, 1. And everything on this side belongs to class C, 2. Okay, so that could be like a 1st


44
00:04:33.759 --> 00:04:36.559
Inderjit Dhillon: kind of simple rule that we try and get


45
00:04:38.419 --> 00:04:42.638
Inderjit Dhillon: right. And if you think about this right? So suppose now.


46
00:04:42.919 --> 00:04:52.449
Inderjit Dhillon: I have. These points have mean to be M. 2, so mean


47
00:04:53.879 --> 00:05:00.068
Inderjit Dhillon: of C. 2. And when I say mean of C. 2, I mean mean of all the training points in C 2.


48
00:05:00.339 --> 00:05:03.188
Inderjit Dhillon: And then suppose I have this over here.


49
00:05:03.439 --> 00:05:06.288
Inderjit Dhillon: which is, you know, the mean of


50
00:05:06.579 --> 00:05:08.988
Inderjit Dhillon: c 1. So let's call it m, 1.


51
00:05:10.149 --> 00:05:14.149
Inderjit Dhillon: Okay, so mean, vector so this is the mean of all the X's.


52
00:05:22.939 --> 00:05:24.429
Inderjit Dhillon: you know. See what?


53
00:05:25.639 --> 00:05:26.389
Inderjit Dhillon: Okay?


54
00:05:27.899 --> 00:05:32.449
Inderjit Dhillon: And then what would be kind of the equation of this line?


55
00:05:35.299 --> 00:05:41.649
Inderjit Dhillon: Okay, well, it's equidistant from. I might actually choose it to be equidistant from m. 1 and m. 2.


56
00:05:41.969 --> 00:05:46.858
Inderjit Dhillon: Right at this distance is the same as this distance


57
00:05:47.859 --> 00:05:51.589
Inderjit Dhillon: right? And so then, in this case.


58
00:05:52.139 --> 00:05:58.499
Inderjit Dhillon: this equation or the locus of all points that is equidistant from m. 1


59
00:06:00.469 --> 00:06:03.019
Inderjit Dhillon: and M. 2. I can write like this.


60
00:06:05.119 --> 00:06:13.019
Inderjit Dhillon: Okay, the squares are not needed. But it'll simplify things if I remove the squares. Okay, so this is the locus of


61
00:06:14.779 --> 00:06:18.268
Inderjit Dhillon: points which are equidistant.


62
00:06:21.529 --> 00:06:24.379
Inderjit Dhillon: 2 m. 1 and M. 2.


63
00:06:25.679 --> 00:06:27.878
Inderjit Dhillon: Okay. And if I then expand it out.


64
00:06:28.239 --> 00:06:36.278
Inderjit Dhillon: I can write this as x minus m, 1 transpose x minus m, 1


65
00:06:37.409 --> 00:06:42.748
Inderjit Dhillon: equals x minus m, 2. Transpose x minus m, 2.


66
00:06:44.439 --> 00:06:48.408
Inderjit Dhillon: Okay, again, let me expand it out. I'll get X transpose x


67
00:06:51.309 --> 00:06:57.769
Inderjit Dhillon: minus 2 x transpose m, 1 plus m. 1. Transpose m. 1,


68
00:06:58.209 --> 00:07:00.729
Inderjit Dhillon: and then the right hand side is very similar.


69
00:07:01.569 --> 00:07:05.648
Inderjit Dhillon: except I have M. 2 instead of m. 1.


70
00:07:11.139 --> 00:07:16.509
Inderjit Dhillon: Okay, and remember that the locus of. And so let me just kind of you know this kind of cancels out


71
00:07:17.978 --> 00:07:25.109
Inderjit Dhillon: right? And let me just take, you know, M. 2 minus m. 1 to one side, so I'll get m. 2 minus m, 1


72
00:07:25.939 --> 00:07:32.149
Inderjit Dhillon: transpose X. Right. Remember, M. Transpose X is the same as X transpose M.


73
00:07:32.689 --> 00:07:37.358
Inderjit Dhillon: Right? And I'm kind of dividing by 2 throughout, and then if I


74
00:07:37.809 --> 00:07:43.119
Inderjit Dhillon: so if I take this to this side, I'll take this also to this side.


75
00:07:43.659 --> 00:07:48.299
Inderjit Dhillon: and so I'll get one half off M.


76
00:07:49.759 --> 00:07:54.839
Inderjit Dhillon: One square minus M, 2 score.


77
00:07:55.619 --> 00:07:58.469
Inderjit Dhillon: Yeah, the locus of all the points is


78
00:07:59.769 --> 00:08:01.859
Inderjit Dhillon: where this is equal to 0.


79
00:08:06.229 --> 00:08:10.779
Inderjit Dhillon: Okay. And now, if you think about it, so this is.


80
00:08:11.289 --> 00:08:14.508
Inderjit Dhillon: you know, I can think of a function Y of x.


81
00:08:15.009 --> 00:08:27.589
Inderjit Dhillon: which is basically this, which is m, 2, minus m, 1 transpose x plus one half and one square


82
00:08:28.759 --> 00:08:31.009
Inderjit Dhillon: minus M. 2 square.


83
00:08:32.819 --> 00:08:37.338
Inderjit Dhillon: Okay? And this is, you know, I can think this is the locus of that line, right


84
00:08:38.359 --> 00:08:46.158
Inderjit Dhillon: locus of all those points that are equidistant from m. 1 to M. 2. And I call this the. You know, this is the decision surface.


85
00:08:50.699 --> 00:09:01.088
Inderjit Dhillon: and so far we haven't actually invoked any probabilistic modeling. We kind of just doing it out of intuition. So far, right? But the important thing to note is that this has the form


86
00:09:02.659 --> 00:09:10.129
Inderjit Dhillon: offer what's called a hyperplane. W transpose X plus W, naught.


87
00:09:11.879 --> 00:09:15.118
Inderjit Dhillon: Okay? So if you think back to regression.


88
00:09:15.639 --> 00:09:19.498
Inderjit Dhillon: right, we basically want would get a linear rule


89
00:09:20.209 --> 00:09:22.588
Inderjit Dhillon: W transpose X plus W. Naught


90
00:09:22.909 --> 00:09:28.209
Inderjit Dhillon: such that it fit the corresponding real label. Why?


91
00:09:28.509 --> 00:09:34.689
Inderjit Dhillon: And we are kind of doing a similar thing by just this very simple modeling that we did


92
00:09:35.189 --> 00:09:37.789
Inderjit Dhillon: right. We had kind of task. c. 1.


93
00:09:38.679 --> 00:09:40.529
Inderjit Dhillon: We had class C, 2,


94
00:09:41.219 --> 00:09:45.349
Inderjit Dhillon: and we just did a very simple modeling, saying, Hey, you know, let's just take the mean


95
00:09:45.849 --> 00:09:49.058
Inderjit Dhillon: of class one mean of class 2.


96
00:09:49.659 --> 00:09:53.019
Inderjit Dhillon: Let me think about joining those 2 points, and then


97
00:09:53.839 --> 00:09:57.909
Inderjit Dhillon: the locus of all the points, right, which are equidistant.


98
00:09:58.469 --> 00:09:59.259
Inderjit Dhillon: Right


99
00:09:59.469 --> 00:10:06.788
Inderjit Dhillon: is given by this equation. And so I can get a very simple decision rule right? So this is, I call this a


100
00:10:07.289 --> 00:10:10.289
Inderjit Dhillon: decision surface. And the reason is because


101
00:10:10.409 --> 00:10:18.739
Inderjit Dhillon: you could get a very simple decision rule, saying, if YX is greater than 0 white.


102
00:10:19.289 --> 00:10:22.806
Inderjit Dhillon: then, basically, what I'm saying is that


103
00:10:23.479 --> 00:10:26.649
Inderjit Dhillon: kind of the left hand side over here


104
00:10:26.859 --> 00:10:30.839
Inderjit Dhillon: is greater than the right hand side, right? The distance from m. 1


105
00:10:30.979 --> 00:10:37.858
Inderjit Dhillon: is greater than the distance to from M. 2, and in this case I would say that X belongs to class 2,


106
00:10:38.739 --> 00:10:41.608
Inderjit Dhillon: and if YX is less than 0,


107
00:10:42.019 --> 00:10:44.458
Inderjit Dhillon: then X belongs to plus. c 1.


108
00:10:45.929 --> 00:10:56.919
Inderjit Dhillon: Okay. And the hyperplane has this formula right? Where W is M. 2 minus m. 1.


109
00:10:58.049 --> 00:11:08.158
Inderjit Dhillon: Okay. And W. Naught is of m. 1 square minus M. 2 square.


110
00:11:10.549 --> 00:11:23.939
Inderjit Dhillon: So you know, we've kind of done very kind of simple modeling right now, and as a result. My coefficients have been actually very small. The the coefficients have been very simple. Okay. If you remember, if you go back to


111
00:11:24.039 --> 00:11:28.539
Inderjit Dhillon: the regression problem, remember that we basically got a least squares problem.


112
00:11:28.939 --> 00:11:36.579
Inderjit Dhillon: And then we basically got the normal equations. And you had to find the W using the normal equations which involved.


113
00:11:36.759 --> 00:11:46.208
Inderjit Dhillon: you know, solving a system of linear equations. And then you could solve those linear system of linear equations in multiple ways. Right? We could use


114
00:11:46.419 --> 00:11:56.579
Inderjit Dhillon: Koleski decomposition we could use. QR. Decomposition the numerically, the best way would be to do the Svd. But it'll be much more kind of expensive


115
00:11:56.799 --> 00:11:58.179
Inderjit Dhillon: right here.


116
00:11:58.499 --> 00:12:05.409
Inderjit Dhillon: I've just done a very, very simple modeling. Okay? And the solution is again, now a linear classifier.


117
00:12:05.829 --> 00:12:10.209
Inderjit Dhillon: and in this case, in this very simple modeling, the computation is very simple


118
00:12:10.629 --> 00:12:17.769
Inderjit Dhillon: to get the linear classified, but as we see that, you know, we'll be able to develop better methods. And again.


119
00:12:17.869 --> 00:12:33.038
Inderjit Dhillon: the name of the game would be, how do you basically get a good linear classifier? Which means, How do you find good coefficients w naught w 1 through WD. When d when X is


120
00:12:33.319 --> 00:12:45.439
Inderjit Dhillon: data mixed. Okay? But first, st I kind of just want you to kind of visualize what is happening over here. Have you? I guess most many of you might have heard the the


121
00:12:47.029 --> 00:12:52.529
Inderjit Dhillon: term hyperplane. Okay, so hyperplane is basically just a linear surface


122
00:12:52.799 --> 00:12:57.518
Inderjit Dhillon: of dimension d minus one. So if I am in d dimensions.


123
00:12:57.819 --> 00:13:03.208
Inderjit Dhillon: then this is in dimension d. Minus one right? So, for example.


124
00:13:03.379 --> 00:13:07.299
Inderjit Dhillon: right in one dimension, sorry in 2 dimensions. This is just a line.


125
00:13:08.649 --> 00:13:12.869
Inderjit Dhillon: But if you are in 3 dimensions this would be a two-dimensional plane.


126
00:13:13.479 --> 00:13:20.888
Inderjit Dhillon: and then in higher d dimensions it would be a D minus one dimensional linear surface.


127
00:13:21.499 --> 00:13:28.365
Inderjit Dhillon: Okay, so let's just look at what the geometry is, because later on, when we talk a little bit about


128
00:13:28.969 --> 00:13:37.289
Inderjit Dhillon: maximizing the margin and so on. Then we will need to understand the geometry of what is going on right? So it's just a


129
00:13:37.877 --> 00:13:41.729
Inderjit Dhillon: hyperplane. Right? So let me just take in this


130
00:13:42.549 --> 00:13:46.988
Inderjit Dhillon: a hyperplane that has this form. W transpose X plus W.


131
00:13:47.119 --> 00:13:48.059
Inderjit Dhillon: Not.


132
00:13:48.289 --> 00:13:54.209
Inderjit Dhillon: And the decision surface is that this is equal to 0. Right? So again, I'm going to just visualize it


133
00:13:54.349 --> 00:13:57.253
Inderjit Dhillon: in 2 dimensions. But many of these,


134
00:13:58.663 --> 00:14:05.299
Inderjit Dhillon: will basically extend to higher dimensions. Right? So, W is some vector like this.


135
00:14:07.359 --> 00:14:17.969
Inderjit Dhillon: Okay? And the hyperplane. Okay, is essentially normal to this. So this is called the normal.


136
00:14:18.429 --> 00:14:22.948
Inderjit Dhillon: That means perpendicular to the hyperplane.


137
00:14:29.139 --> 00:14:33.718
Inderjit Dhillon: Okay, so why is it the normal? How can we justify it? Right? So, for example, so


138
00:14:33.949 --> 00:14:38.458
Inderjit Dhillon: if I draw a normal surface, it'll be of this form.


139
00:14:39.039 --> 00:14:42.878
Inderjit Dhillon: right? So this is 90 degrees


140
00:14:43.779 --> 00:14:52.639
Inderjit Dhillon: right? And let's see right? If I have this X right in d dimensions.


141
00:14:52.969 --> 00:14:55.629
Inderjit Dhillon: If X lies on the hyperplane.


142
00:14:58.499 --> 00:15:04.348
Inderjit Dhillon: then it clearly satisfies this equation, right? So that means it satisfies.


143
00:15:05.209 --> 00:15:07.089
Inderjit Dhillon: W, transpose X,


144
00:15:07.429 --> 00:15:16.359
Inderjit Dhillon: plus W, naught equal to 0. I can think of it a little bit geometrically. And I can say, okay, let me divide this by the 2 norm


145
00:15:17.249 --> 00:15:20.268
Inderjit Dhillon: of W. So now it becomes a unit, vector


146
00:15:21.299 --> 00:15:25.538
Inderjit Dhillon: plus W, naught divided by the 2 norm of W.


147
00:15:28.949 --> 00:15:32.968
Inderjit Dhillon: Okay, so let me write this as in this form.


148
00:15:37.149 --> 00:15:41.138
Inderjit Dhillon: that this is equal to minus of this.


149
00:15:42.039 --> 00:15:42.949
Inderjit Dhillon: Okay?


150
00:15:43.109 --> 00:15:48.698
Inderjit Dhillon: So again, right now, it's kind of algebraic. But you see that this is a


151
00:15:49.079 --> 00:15:54.619
Inderjit Dhillon: projection of X onto the normal to the hyperplane.


152
00:15:54.778 --> 00:15:55.489
Inderjit Dhillon: Okay?


153
00:15:56.548 --> 00:16:03.349
Inderjit Dhillon: In particular, if I have that 2 points x, 1 and x. 2 lie on the hyperplane.


154
00:16:09.788 --> 00:16:14.135
Inderjit Dhillon: Okay, then I have that. w. 1 transpose. Sorry.


155
00:16:15.599 --> 00:16:22.759
Inderjit Dhillon: it's x 1 and x 2. So W. Transpose x, 1, plus W naught equals


156
00:16:23.259 --> 00:16:34.629
Inderjit Dhillon: 0. And so does W transpose x 2 plus w, which means that W transpose


157
00:16:34.949 --> 00:16:38.239
Inderjit Dhillon: x, 1 minus x 2 equal to 0.


158
00:16:38.929 --> 00:16:41.889
Inderjit Dhillon: Okay, so again, if I have.


159
00:16:42.339 --> 00:16:46.248
Inderjit Dhillon: you know a point x 1 that lies on this hyperplane.


160
00:16:46.889 --> 00:16:50.668
Inderjit Dhillon: I have a point x 2 that lies on the cyberplane.


161
00:16:51.489 --> 00:16:57.819
Inderjit Dhillon: This vector right is x 2 minus x 1.


162
00:16:59.389 --> 00:17:01.859
Inderjit Dhillon: And that's why we say that this is normal.


163
00:17:03.279 --> 00:17:14.798
Inderjit Dhillon: because you can see that it is normal to any x 2 minus x 1. So anything lying on the the hyperplane W is normal to that. Okay, so this implies


164
00:17:15.499 --> 00:17:21.819
Inderjit Dhillon: the W is normal or perpendicular.


165
00:17:25.819 --> 00:17:36.968
Inderjit Dhillon: 2 little the points on, but generally we'll just say it is normal to the hyper.


166
00:17:41.479 --> 00:17:42.449
Inderjit Dhillon: Okay?


167
00:17:42.679 --> 00:17:45.699
Inderjit Dhillon: And then, if I look at this closely.


168
00:17:51.179 --> 00:17:53.929
Inderjit Dhillon: this is saying that when I project any X,


169
00:17:54.549 --> 00:17:59.259
Inderjit Dhillon: right? So this could be some random X, for example, x, 1 or x 2.


170
00:17:59.439 --> 00:18:03.799
Inderjit Dhillon: It's saying that when I project this, this is the projection.


171
00:18:05.089 --> 00:18:12.748
Inderjit Dhillon: Okay, so this length over here, how much is that?


172
00:18:13.939 --> 00:18:15.809
Inderjit Dhillon: Well, it's given over here.


173
00:18:18.319 --> 00:18:19.758
Inderjit Dhillon: This is the projection.


174
00:18:20.039 --> 00:18:24.658
Inderjit Dhillon: And so this length is equal to minus W. Naught


175
00:18:25.839 --> 00:18:28.719
Inderjit Dhillon: divided by the 2 norm of term.


176
00:18:30.019 --> 00:18:37.759
Inderjit Dhillon: Okay. So this is often called the signed distance


177
00:18:40.969 --> 00:18:42.369
Inderjit Dhillon: to the hyperplane.


178
00:18:43.819 --> 00:18:48.749
Inderjit Dhillon: So note that W. Naught divided by norm of W is a scalar.


179
00:18:49.219 --> 00:18:54.628
Inderjit Dhillon: Okay? w. 1 not might be positive or negative. And that's why we say that this is the


180
00:18:54.739 --> 00:18:58.568
Inderjit Dhillon: signed distance from the origin to the hyperplane.


181
00:18:59.769 --> 00:19:02.858
Inderjit Dhillon: Sign distance. Oh, from the origin. Sorry.


182
00:19:06.559 --> 00:19:11.929
Inderjit Dhillon: I mean. Sorry. Let me just we do this.


183
00:19:12.529 --> 00:19:14.859
Inderjit Dhillon: It's the sign distance from


184
00:19:18.489 --> 00:19:23.729
Inderjit Dhillon: origin to the hyperlink.


185
00:19:25.739 --> 00:19:27.079
Inderjit Dhillon: Okay? So


186
00:19:28.179 --> 00:19:39.469
Inderjit Dhillon: so now you can kind of start. Try to start kind of understanding the geometry. Let me get. Just give a you know, couple of quick examples. You know, when we do come to.


187
00:19:39.709 --> 00:19:43.328
Inderjit Dhillon: for example, support vector machines and so on. It'll actually help


188
00:19:43.569 --> 00:19:47.109
Inderjit Dhillon: to understand this geometry. Right? So suppose I have.


189
00:19:47.249 --> 00:19:51.459
Inderjit Dhillon: you know, x, 1 plus x, 2 equal to one, a simple hyperplane.


190
00:19:51.719 --> 00:19:56.358
Inderjit Dhillon: Okay, over here, my W is the vector, 1 1,


191
00:19:57.089 --> 00:20:00.508
Inderjit Dhillon: and W, naught is minus one.


192
00:20:00.909 --> 00:20:03.319
Inderjit Dhillon: Okay, so what does it? The picture look like?


193
00:20:03.449 --> 00:20:06.908
Inderjit Dhillon: Well, I have 1 1. Sorry.


194
00:20:07.199 --> 00:20:11.449
Inderjit Dhillon: i, 1 1. So it is basically a vector, like this?


195
00:20:12.259 --> 00:20:18.769
Inderjit Dhillon: Right? So that is W, and then my normal to the hyperplane is this.


196
00:20:20.679 --> 00:20:23.498
Inderjit Dhillon: that means I have a perpendicular over here.


197
00:20:23.669 --> 00:20:28.988
Inderjit Dhillon: Okay, and then I have.


198
00:20:29.629 --> 00:20:39.069
Inderjit Dhillon: So this is W transpose X, plus W. Naught equal to 0,


199
00:20:39.779 --> 00:20:47.598
Inderjit Dhillon: and that's the same as 1 1 transpose x 1 x 2


200
00:20:51.239 --> 00:20:53.449
Inderjit Dhillon: minus one equals.


201
00:20:56.539 --> 00:20:57.229
Inderjit Dhillon: Hmm!


202
00:20:58.999 --> 00:21:01.119
Inderjit Dhillon: And what is this distance? Now


203
00:21:02.719 --> 00:21:07.469
Inderjit Dhillon: this distance? Now remember, that's the sign distance of the hyperplane


204
00:21:08.539 --> 00:21:11.359
Inderjit Dhillon: to the origin, and that is


205
00:21:11.909 --> 00:21:16.728
Inderjit Dhillon: minus W. Naught divided by norm of W, and the 2 norm.


206
00:21:17.369 --> 00:21:20.959
Inderjit Dhillon: Okay. W. Naught is minus one


207
00:21:21.989 --> 00:21:24.629
Inderjit Dhillon: W is equal to 1 1.


208
00:21:25.819 --> 00:21:28.649
Inderjit Dhillon: So this is so.


209
00:21:31.299 --> 00:21:34.079
Inderjit Dhillon: The 2 norm of W is square root of 2.


210
00:21:34.369 --> 00:21:39.898
Inderjit Dhillon: Okay. And so W. Naught divided by norm of W


211
00:21:41.239 --> 00:21:46.938
Inderjit Dhillon: is equal to one divided by square root. 2. Okay. So this is one divided by square root 2.


212
00:21:51.639 --> 00:21:52.549
Inderjit Dhillon: Okay?


213
00:21:54.839 --> 00:21:58.479
Inderjit Dhillon: So hopefully, that's clear. Any questions on what I've done.


214
00:22:00.729 --> 00:22:15.249
Inderjit Dhillon: What have I done so far? Right? So I basically didn't really do any modeling, or did very simple modeling, and just said, Hey, you know, this is intuitively one way to get a decision surface got the decision surface and realized


215
00:22:15.399 --> 00:22:18.888
Inderjit Dhillon: that the decision surface is essentially a hyperplane.


216
00:22:19.099 --> 00:22:27.728
Inderjit Dhillon: and that we just spent like a few minutes to try to understand kind of the geometry behind the hyperplane.


217
00:22:27.969 --> 00:22:33.638
Inderjit Dhillon: Okay, so now let's try to do some actual probabilistic modeling.


218
00:22:33.819 --> 00:22:38.149
Inderjit Dhillon: And we'll see that when we do this probabilistic modeling, we actually


219
00:22:38.329 --> 00:22:48.959
Inderjit Dhillon: get something which is, you know, little bit more general than what we did over here, right where we got this particular decision surface. Okay, so let's


220
00:22:49.159 --> 00:22:51.648
Inderjit Dhillon: let's now do some probabilistic modeling.


221
00:22:53.829 --> 00:22:55.679
Inderjit Dhillon: So suppose I have.


222
00:22:56.285 --> 00:23:00.668
Inderjit Dhillon: The class c 1, and I have the class. C, 2.


223
00:23:02.539 --> 00:23:12.709
Inderjit Dhillon: What I'm really interested in is finding probability. I can think about the classification problem as trying to find the probability of c, 1 given X


224
00:23:14.629 --> 00:23:17.728
Inderjit Dhillon: and probability of C, 2. Given X.


225
00:23:20.779 --> 00:23:23.948
Inderjit Dhillon: Okay. So given my test point of the new point X,


226
00:23:24.149 --> 00:23:28.138
Inderjit Dhillon: I want to find the probability of c, 1 versus the probability.


227
00:23:28.409 --> 00:23:29.608
Inderjit Dhillon: C. 2.


228
00:23:30.599 --> 00:23:31.429
Inderjit Dhillon: Okay.


229
00:23:31.729 --> 00:23:36.839
Inderjit Dhillon: Where I put this, let me go for things. Okay?


230
00:23:36.949 --> 00:23:44.239
Inderjit Dhillon: So now let's take a look back at our previous lecture. Right? Recall the previous lecture.


231
00:23:44.479 --> 00:23:46.679
Inderjit Dhillon: and let's look at Bayes. Rule.


232
00:23:46.949 --> 00:23:47.689
Inderjit Dhillon: Okay.


233
00:23:48.119 --> 00:23:50.598
Inderjit Dhillon: So P of c, 1 given X


234
00:23:51.479 --> 00:23:59.049
Inderjit Dhillon: is equal to P of x, given c, 1 plc, one divided by P of X.


235
00:24:00.489 --> 00:24:04.029
Inderjit Dhillon: Okay, so that is what we are interested in.


236
00:24:04.689 --> 00:24:08.339
Inderjit Dhillon: This is often called the prior.


237
00:24:09.519 --> 00:24:11.869
Inderjit Dhillon: In this case. Class prior.


238
00:24:14.379 --> 00:24:18.168
Inderjit Dhillon: So it's the prior probability of belonging to a particular class.


239
00:24:18.499 --> 00:24:23.329
Inderjit Dhillon: This is what we are interested in, right, which is the posterior.


240
00:24:24.559 --> 00:24:27.188
Inderjit Dhillon: So that's what it's called. It's the posterior.


241
00:24:27.419 --> 00:24:30.978
Inderjit Dhillon: And this is the data likelihood.


242
00:24:34.429 --> 00:24:39.258
Inderjit Dhillon: So this is where the modeling comes in. Right. Given class c. 1.


243
00:24:39.639 --> 00:24:41.739
Inderjit Dhillon: What is the probability?


244
00:24:42.179 --> 00:24:45.229
Inderjit Dhillon: What is P of X given? c. 1.


245
00:24:46.249 --> 00:24:47.179
Inderjit Dhillon: Okay.


246
00:24:47.569 --> 00:24:54.899
Inderjit Dhillon: so similarly, I can do. PC, 2. Given X. And of course we are going to look at concrete examples


247
00:24:55.169 --> 00:24:57.879
Inderjit Dhillon: by modeling it in a particular way.


248
00:24:58.759 --> 00:25:04.139
Inderjit Dhillon: This is P. Of X, given C 2 times, p. Of c. 2.


249
00:25:04.819 --> 00:25:07.899
Inderjit Dhillon: The denominator is kind of immaterial, because


250
00:25:08.019 --> 00:25:12.749
Inderjit Dhillon: typically what we will do is we'll use something called the map rule.


251
00:25:15.199 --> 00:25:19.939
Inderjit Dhillon: because, remember, the goal is to finally assign a class to this X,


252
00:25:20.399 --> 00:25:25.149
Inderjit Dhillon: right? So the Mac map rule is the maximum


253
00:25:29.379 --> 00:25:35.199
Inderjit Dhillon: a posterior. Y, well, brother.


254
00:25:39.609 --> 00:25:45.269
Inderjit Dhillon: Okay, so what we will do is basically say that this is going to be.


255
00:25:46.059 --> 00:25:50.569
Inderjit Dhillon: I'm going to look at P. Of CI give an X


256
00:25:50.819 --> 00:25:57.749
Inderjit Dhillon: the PC. When the 2 class problem, PC, one, given act and PC. 2, given X.


257
00:25:57.929 --> 00:26:04.938
Inderjit Dhillon: And then I would look at the eye for which this is maximized.


258
00:26:05.159 --> 00:26:08.308
Inderjit Dhillon: And I'm going to assign that class to X.


259
00:26:11.099 --> 00:26:14.458
Inderjit Dhillon: Okay, so that's what we are going to do right? So


260
00:26:14.579 --> 00:26:18.748
Inderjit Dhillon: remember, this is just base which we studied last time.


261
00:26:19.419 --> 00:26:21.369
Inderjit Dhillon: but which we recall last time.


262
00:26:27.249 --> 00:26:33.969
Inderjit Dhillon: So now let's look at different ways of kind of modeling the data right?


263
00:26:34.079 --> 00:26:40.609
Inderjit Dhillon: And let's look at a particular kind of normally distributed model or a Gaussian model.


264
00:26:41.209 --> 00:26:44.589
Inderjit Dhillon: So suppose my X belongs to RD.


265
00:26:45.739 --> 00:26:49.038
Inderjit Dhillon: Okay, let's look at a particular Gaussian model.


266
00:26:55.129 --> 00:26:56.059
Inderjit Dhillon: Okay?


267
00:26:56.339 --> 00:27:05.299
Inderjit Dhillon: And the Gaussian model is saying that, hey? Let's think of each class or the points within each class to be distributed according to a


268
00:27:06.181 --> 00:27:08.529
Inderjit Dhillon: multivariate gaussium distribution.


269
00:27:08.779 --> 00:27:14.029
Inderjit Dhillon: So in particular, P of x, given. Ci.


270
00:27:14.919 --> 00:27:23.419
Inderjit Dhillon: okay, it's going to be, remember that for each class, I can get the mean vector, and the covariance matrix. Right.


271
00:27:23.829 --> 00:27:32.909
Inderjit Dhillon: so it is equal to one divided by 2 pi d. By 2 times the determinant


272
00:27:34.969 --> 00:27:41.779
Inderjit Dhillon: raised to the power half of the covariance matrix. Remember, I think last time I called this as debt of Sigma. But we'll have


273
00:27:41.889 --> 00:27:45.029
Inderjit Dhillon: different formulas. So this time I will just use


274
00:27:45.919 --> 00:27:51.089
Inderjit Dhillon: these 2 bars around the matrix sigma around the covariance matrix sigma.


275
00:27:51.389 --> 00:27:55.869
Inderjit Dhillon: and then I have e to the power minus half


276
00:27:58.779 --> 00:28:03.969
Inderjit Dhillon: x minus mu, i mu i being the mean of class.


277
00:28:04.119 --> 00:28:12.169
Inderjit Dhillon: I transpose sigma, I inverse X minus mu.


278
00:28:15.639 --> 00:28:21.649
Inderjit Dhillon: And let's think of this as the model of the data.


279
00:28:23.929 --> 00:28:28.469
Inderjit Dhillon: And maybe you know, maybe it's just too complicated right at the beginning to do


280
00:28:28.979 --> 00:28:34.109
Inderjit Dhillon: okay? So well. But let's let's let's see what happens. Right? So the map rule.


281
00:28:34.789 --> 00:28:37.308
Inderjit Dhillon: how will we use the map rule over here.


282
00:28:38.859 --> 00:28:44.549
Inderjit Dhillon: Right? We are going to do. The map rule is, Arg. Max off.


283
00:28:45.159 --> 00:28:47.708
Inderjit Dhillon: What did I say? P. Of CIX.


284
00:28:47.819 --> 00:28:52.609
Inderjit Dhillon: Might be of CI given X.


285
00:28:53.719 --> 00:28:56.669
Inderjit Dhillon: This is oops.


286
00:28:56.999 --> 00:28:58.909
Inderjit Dhillon: Now the logarithm.


287
00:28:59.439 --> 00:29:02.099
Inderjit Dhillon: The log function is a monotonic function.


288
00:29:02.889 --> 00:29:08.209
Inderjit Dhillon: So I can basically just say, log of pci, give an X,


289
00:29:08.729 --> 00:29:11.289
Inderjit Dhillon: because the odd Max is not going to change.


290
00:29:12.439 --> 00:29:17.078
Inderjit Dhillon: so I can take the arg Max of P. Or I can take the arg Max of log of P.


291
00:29:17.649 --> 00:29:21.708
Inderjit Dhillon: So let me take the log, because obviously there's an exponential sitting over here.


292
00:29:21.829 --> 00:29:24.419
Inderjit Dhillon: So it's a natural thing to do. Okay?


293
00:29:25.509 --> 00:29:31.318
Inderjit Dhillon: And then I expanded out by


294
00:29:32.679 --> 00:29:37.959
Inderjit Dhillon: base rule. Right? So I have log off. I basically use.


295
00:29:38.642 --> 00:29:40.909
Inderjit Dhillon: What is given over here?


296
00:29:43.029 --> 00:29:45.869
Inderjit Dhillon: Okay, log off.


297
00:29:46.089 --> 00:29:52.408
Inderjit Dhillon: P of X, given Ci plans B of CI,


298
00:29:55.419 --> 00:30:04.068
Inderjit Dhillon: okay, note that the denominator actually does not really now play a role, because all you're interested in is looking at the Arc Max, and both.


299
00:30:04.319 --> 00:30:07.479
Inderjit Dhillon: Pcr. One given x has P of x


300
00:30:08.379 --> 00:30:14.629
Inderjit Dhillon: and PC. 2. Given xsp. Of X in the denominator. So that's common. We don't need to worry about it.


301
00:30:15.309 --> 00:30:17.599
Inderjit Dhillon: Right? So this will be as well.


302
00:30:18.909 --> 00:30:29.709
Inderjit Dhillon: Okay? And then I'm taking the log of a product, and that becomes log of B


303
00:30:30.089 --> 00:30:39.439
Inderjit Dhillon: X given ci plus log of B of Cnn, okay?


304
00:30:41.049 --> 00:30:43.458
Inderjit Dhillon: And this is what we have.


305
00:30:44.929 --> 00:30:49.529
Inderjit Dhillon: Right? So if I do a Gaussian model.


306
00:30:54.759 --> 00:30:57.528
Inderjit Dhillon: then I have that log of


307
00:30:58.159 --> 00:31:01.598
Inderjit Dhillon: Px. Given CI is equal to.


308
00:31:01.809 --> 00:31:05.209
Inderjit Dhillon: I basically take a log of this quality, right?


309
00:31:05.399 --> 00:31:07.919
Inderjit Dhillon: So what is the log of this quantity?


310
00:31:08.899 --> 00:31:15.858
Inderjit Dhillon: This is going to become equal to minus D by 2 log of 2 pi.


311
00:31:17.969 --> 00:31:27.399
Inderjit Dhillon: minus one half log of the determinant of Sigma. I okay.


312
00:31:27.739 --> 00:31:30.148
Inderjit Dhillon: And taking log of this is


313
00:31:30.759 --> 00:31:36.049
Inderjit Dhillon: minus one half. This is why we took Lot so that this term becomes simpler.


314
00:31:39.469 --> 00:31:41.268
Inderjit Dhillon: And this is this quadratic form.


315
00:31:52.869 --> 00:31:53.809
Inderjit Dhillon: Okay?


316
00:31:54.709 --> 00:31:56.719
Inderjit Dhillon: And the decision surface


317
00:32:01.579 --> 00:32:05.619
Inderjit Dhillon: kind of analogous to what we did over here.


318
00:32:07.369 --> 00:32:11.539
Inderjit Dhillon: Right here was the decision surface.


319
00:32:12.429 --> 00:32:20.199
Inderjit Dhillon: So the decision surface is going to be the X's, where the


320
00:32:20.589 --> 00:32:26.589
Inderjit Dhillon: posterior is equal right, or the log of the posterior is equal. So


321
00:32:26.769 --> 00:32:29.348
Inderjit Dhillon: you can just say that it is equal to


322
00:32:36.429 --> 00:32:40.538
Inderjit Dhillon: and there are lots of, I guess, minus signs over here. Right?


323
00:32:40.669 --> 00:32:51.139
Inderjit Dhillon: So I can take the minus log of BC, 1 given X is equal to


324
00:32:51.829 --> 00:32:56.319
Inderjit Dhillon: minus log of BC, 2 kilomets.


325
00:32:56.779 --> 00:32:59.448
Inderjit Dhillon: Okay, maybe I'll move these outside back.


326
00:33:02.088 --> 00:33:02.799
Inderjit Dhillon: Yeah.


327
00:33:07.789 --> 00:33:10.199
Inderjit Dhillon: okay, I'm just taking local time. Right?


328
00:33:10.449 --> 00:33:14.309
Inderjit Dhillon: And if I do, the take the logs to be equal.


329
00:33:14.749 --> 00:33:17.319
Inderjit Dhillon: Right? What do I get? I get?


330
00:33:18.128 --> 00:33:24.159
Inderjit Dhillon: Yeah, I'm taking negative of this quantity right? But now I have c, 1 on the left side


331
00:33:24.289 --> 00:33:26.148
Inderjit Dhillon: C, 2 on the right side


332
00:33:26.878 --> 00:33:29.999
Inderjit Dhillon: I'll get. And this is now the decision. Surface


333
00:33:30.398 --> 00:33:43.148
Inderjit Dhillon: d. By 2 log, 2 pi plus one half log, determined, determined of sigma, one plus one half


334
00:33:43.709 --> 00:33:51.469
Inderjit Dhillon: x minus mu one transpose sigma inverse x minus mu, one.


335
00:33:52.259 --> 00:33:56.158
Inderjit Dhillon: Right? So remember that I'm doing P of c, 1. Given x.


336
00:33:56.779 --> 00:34:02.768
Inderjit Dhillon: This is Bx given ci, and then I have.


337
00:34:03.699 --> 00:34:08.288
Inderjit Dhillon: Well, I don't need to go far that far. But I have plus log of Pcr.


338
00:34:09.179 --> 00:34:13.839
Inderjit Dhillon: okay. And remember, I I took the negative of that. So it is


339
00:34:14.119 --> 00:34:16.778
Inderjit Dhillon: minus log of PC. 1.


340
00:34:21.359 --> 00:34:39.218
Inderjit Dhillon: And the decision surface is that this equals D by 2 log Dubai plus one half log


341
00:34:39.719 --> 00:34:49.329
Inderjit Dhillon: determinant of sigma, 2 plus one half x minus mu 2 transpose. I missed a sigma, one here.


342
00:34:50.099 --> 00:35:00.069
Inderjit Dhillon: sigma, 2 inverse x minus mu, 2 minus log of E of C. 2.


343
00:35:00.959 --> 00:35:04.828
Inderjit Dhillon: Now this is a constant that will cancel anything else. Cancels.


344
00:35:08.599 --> 00:35:10.929
Inderjit Dhillon: Oh, doesn't really right.


345
00:35:12.309 --> 00:35:21.809
Inderjit Dhillon: I have sigma one here quadratic form here. That depends on mu one sigma, one and


346
00:35:22.079 --> 00:35:24.449
Inderjit Dhillon: log of this is a class prior.


347
00:35:25.629 --> 00:35:32.009
Inderjit Dhillon: Okay, so that is the general decision surface. But let's take some special cases. Okay, so let's take


348
00:35:32.549 --> 00:35:35.179
Inderjit Dhillon: a special case. One.


349
00:35:35.939 --> 00:35:42.228
Inderjit Dhillon: Okay, which is kind of the most common case. Right? That's you'll say, okay, let me just assume for simplicity


350
00:35:42.609 --> 00:35:45.439
Inderjit Dhillon: that my Gaussians are uncorrelated.


351
00:35:48.369 --> 00:35:58.639
Inderjit Dhillon: That means that, you know. For class one sigma, one equals identity for class 2. Sigma equal to Sigma is the covariance matrix is identity.


352
00:35:59.009 --> 00:36:02.789
Inderjit Dhillon: If that happens, let's look at the decision source.


353
00:36:06.169 --> 00:36:12.808
Inderjit Dhillon: Then you'll see that storms run the left hand side.


354
00:36:13.649 --> 00:36:18.559
Inderjit Dhillon: start kind of cancelling with the terms on the right hand side.


355
00:36:21.579 --> 00:36:22.309
Inderjit Dhillon: Okay.


356
00:36:28.149 --> 00:36:30.049
Inderjit Dhillon: so let me just write it out again.


357
00:36:30.539 --> 00:36:36.539
Inderjit Dhillon: Sorry one half log off signal one.


358
00:36:38.179 --> 00:36:40.569
Inderjit Dhillon: Let me see if I can fit this into one line


359
00:36:43.869 --> 00:36:47.479
Inderjit Dhillon: X minus mu one bounce balls.


360
00:36:48.609 --> 00:36:51.719
Inderjit Dhillon: Sigma, one inverse x minus mu one.


361
00:37:01.939 --> 00:37:03.179
Inderjit Dhillon: Sorry about that.


362
00:37:05.769 --> 00:37:07.808
Inderjit Dhillon: Looks like I'm gonna run out of space. Huh?


363
00:37:10.189 --> 00:37:17.838
Inderjit Dhillon: Not too bad, plus or minus minus log of P. 0.


364
00:37:19.699 --> 00:37:24.979
Inderjit Dhillon: Okay, so if I plug in, so call.


365
00:37:24.979 --> 00:37:26.638
Hormoz Shahrzad: 2, the the last one.


366
00:37:27.034 --> 00:37:28.219
Inderjit Dhillon: Good. Thank you.


367
00:37:36.219 --> 00:37:41.959
Inderjit Dhillon: Vc, 2. Okay. Now, if I plugged in Sigma one equal to Sigma 2 right?


368
00:37:42.279 --> 00:37:47.349
Inderjit Dhillon: Well, determinant of identity is identity. Sorry it's 1,


369
00:37:47.629 --> 00:37:53.289
Inderjit Dhillon: right? So I'll basically get 0 over here right? I'll get 0 over here.


370
00:37:53.899 --> 00:37:56.209
Inderjit Dhillon: So I don't need to worry about that.


371
00:37:56.849 --> 00:38:01.998
Inderjit Dhillon: I'll get one half x minus mu one transpose.


372
00:38:03.419 --> 00:38:10.638
Inderjit Dhillon: And sigma, one is now identity. So, Sigma, one inverse identity of identities identity.


373
00:38:10.799 --> 00:38:13.719
Inderjit Dhillon: So I'll just get it's minus me one.


374
00:38:15.639 --> 00:38:29.209
Inderjit Dhillon: Then I get minus log of PC, one is equal to one half x minus mu, to transpose


375
00:38:29.949 --> 00:38:36.519
Inderjit Dhillon: X minus mu, 2 minus log. PC, 2.


376
00:38:37.929 --> 00:38:45.139
Inderjit Dhillon: Okay. And now, if I want to kind of get the expanded out just like before


377
00:38:45.349 --> 00:38:51.529
Inderjit Dhillon: I'll see that it is half of X transpose x


378
00:38:52.499 --> 00:38:58.419
Inderjit Dhillon: minus 2 mu. One transpose x plus mu one transpose mu. One.


379
00:38:59.399 --> 00:39:06.328
Inderjit Dhillon: It should start reminding you of something that you've kind of seen right minus log of PC, one


380
00:39:07.299 --> 00:39:13.589
Inderjit Dhillon: is equal to one half x transpose x minus 2 mu. 2 transpose x


381
00:39:14.149 --> 00:39:19.829
Inderjit Dhillon: plus mu, 2. Transpose mu, 2 minus log, ec. 2.


382
00:39:20.399 --> 00:39:25.669
Inderjit Dhillon: So the interesting thing that you will see, and then you'll realize in a little while is that


383
00:39:25.869 --> 00:39:36.839
Inderjit Dhillon: X transpose X cancels, which means that the quadratic part of the decision surface does not play a role.


384
00:39:36.999 --> 00:39:41.978
Inderjit Dhillon: And the decision surface is actually linear. Okay? And the decision surface is.


385
00:39:42.299 --> 00:39:46.399
Inderjit Dhillon: you know, if I just bring this over to this side.


386
00:39:46.499 --> 00:39:52.138
Inderjit Dhillon: There's a half over here kind of just doing some algebraic manipulations. You'll get


387
00:39:52.549 --> 00:40:00.269
Inderjit Dhillon: the decision surface is mu 2 minus mu one transpose x plus half.


388
00:40:01.589 --> 00:40:07.229
Inderjit Dhillon: This is new, one square, new one


389
00:40:07.379 --> 00:40:09.488
Inderjit Dhillon: square. Remember that if I don't


390
00:40:09.699 --> 00:40:13.148
Inderjit Dhillon: put a subscript over here, it's the L. 2 norm.


391
00:40:14.019 --> 00:40:16.178
Inderjit Dhillon: Okay, so it's the sum of squares


392
00:40:26.679 --> 00:40:31.439
Inderjit Dhillon: minus mu, 2 square from the other side.


393
00:40:31.709 --> 00:40:40.559
Inderjit Dhillon: Then I have a minus log of BC, 1 divided by PC, 2 equals 0.


394
00:40:45.189 --> 00:40:50.269
Inderjit Dhillon: Okay? So you will notice that again, my decision surface


395
00:40:55.929 --> 00:40:57.459
Inderjit Dhillon: as the form


396
00:41:00.779 --> 00:41:04.238
Inderjit Dhillon: W transpose X plus W, naught equal to 0.


397
00:41:05.019 --> 00:41:09.479
Inderjit Dhillon: And what's interesting is that this W transpose. X,


398
00:41:10.619 --> 00:41:16.208
Inderjit Dhillon: okay, is mu 2 minus mu, one transpose x, and that's exactly


399
00:41:16.969 --> 00:41:18.699
Inderjit Dhillon: what we have seen over here.


400
00:41:21.489 --> 00:41:24.138
Inderjit Dhillon: M, 2 minus m, 1 mu, 2 minus mu one.


401
00:41:24.929 --> 00:41:27.879
Inderjit Dhillon: And this term is also similar.


402
00:41:29.729 --> 00:41:30.679
Inderjit Dhillon: Okay.


403
00:41:35.909 --> 00:41:38.038
Inderjit Dhillon: this term is also similar.


404
00:41:38.759 --> 00:41:43.559
Inderjit Dhillon: So without really kind of realizing it.


405
00:41:44.109 --> 00:41:51.888
Inderjit Dhillon: what we had implicitly done over here was to model the classes probabilistically


406
00:41:52.649 --> 00:42:05.049
Inderjit Dhillon: where Class c, 1 was isotropic. That means it had a covariance matrix equal to identity. So was Class C, 2. And we were actually also assuming that the class priors were the same.


407
00:42:06.039 --> 00:42:10.428
Inderjit Dhillon: Okay, that PC, one equal to PC, 2, okay. But if we don't.


408
00:42:11.109 --> 00:42:17.988
Inderjit Dhillon: then we see that we actually need this adjustment penalty adjustment factor.


409
00:42:20.259 --> 00:42:21.119
Inderjit Dhillon: Okay.


410
00:42:26.979 --> 00:42:34.408
Inderjit Dhillon: so I have. W is equal to mu 2 minus mu one.


411
00:42:34.869 --> 00:42:44.459
Inderjit Dhillon: And I have. W. Naught is equal to one half me, one square minus. Move, 2 squares


412
00:42:46.159 --> 00:42:52.388
Inderjit Dhillon: minus log of PC, 1 given or divided by PC, 2.


413
00:42:56.919 --> 00:43:00.908
Inderjit Dhillon: Okay. And again, like I said, you know you can think of this as


414
00:43:04.499 --> 00:43:10.769
Inderjit Dhillon: that. I have, like Sigma. One equal to identity. Mean means. It's kind of like a spherical Gaussian.


415
00:43:11.209 --> 00:43:12.419
Inderjit Dhillon: I have


416
00:43:16.909 --> 00:43:20.398
Inderjit Dhillon: Sigma 2 equal to identity. If I have.


417
00:43:20.649 --> 00:43:25.138
Inderjit Dhillon: you know, some point over here. This is the mean of one


418
00:43:25.499 --> 00:43:28.238
Inderjit Dhillon: over here. This is the mean of 2.


419
00:43:28.989 --> 00:43:34.158
Inderjit Dhillon: Right? Then, I'm basically kind of joining these 2 points.


420
00:43:34.789 --> 00:43:37.669
Inderjit Dhillon: And the decision surface is a normal.


421
00:43:37.999 --> 00:43:44.318
Inderjit Dhillon: And the position of this normal or the position of this decision surface, where


422
00:43:44.439 --> 00:43:49.268
Inderjit Dhillon: you know, the normal is mu 2 minus mu one.


423
00:43:49.679 --> 00:43:54.229
Inderjit Dhillon: Okay, that actually depends on these quantities.


424
00:43:55.329 --> 00:43:57.139
Inderjit Dhillon: Okay, so if


425
00:43:57.279 --> 00:44:05.048
Inderjit Dhillon: if if class one is more likely, then this decision surface will shift to the right.


426
00:44:05.289 --> 00:44:10.628
Inderjit Dhillon: If Class 2 is more likely, then this decision surface will move to the left.


427
00:44:13.249 --> 00:44:17.528
Inderjit Dhillon: Okay, any questions.


428
00:44:22.729 --> 00:44:31.049
Inderjit Dhillon: Okay. Now, let's take the case where let's take a second case


429
00:44:32.789 --> 00:44:40.944
Inderjit Dhillon: where I don't assume that Sigma. One equal to Sigma 2 is the identity. But I'll make the simplest you know the


430
00:44:42.139 --> 00:44:43.979
Inderjit Dhillon: a simpler.


431
00:44:45.459 --> 00:44:50.858
Inderjit Dhillon: I'll make the assumption or I'll model it such that the covariances are equal.


432
00:44:52.349 --> 00:45:00.688
Inderjit Dhillon: So they're not identity. But the covariances are equal because we want to understand what happens to the decision surface over there.


433
00:45:01.819 --> 00:45:04.619
Inderjit Dhillon: Because if you really look at this formula. Right?


434
00:45:05.219 --> 00:45:08.378
Inderjit Dhillon: Let's reflect on this formula. This is a quadratic form.


435
00:45:10.439 --> 00:45:14.939
Inderjit Dhillon: This does not represent a linear surface wipe.


436
00:45:16.519 --> 00:45:22.559
Inderjit Dhillon: but when Sigma one was the identity, and Sigma 2 was the identity. What we found is


437
00:45:22.809 --> 00:45:26.039
Inderjit Dhillon: that this quadratic terms actually canceled.


438
00:45:27.219 --> 00:45:30.128
Inderjit Dhillon: And it actually gave us a linear decision surface.


439
00:45:31.559 --> 00:45:36.538
Inderjit Dhillon: So let's see what happens when I assume that Sigma, one equal to Sigma 2.


440
00:45:38.359 --> 00:45:42.089
Inderjit Dhillon: But not necessarily.


441
00:45:43.509 --> 00:45:46.558
Inderjit Dhillon: No. So so relief.


442
00:45:48.029 --> 00:45:53.298
Inderjit Dhillon: Why don't let's look at the decision surface.


443
00:46:00.199 --> 00:46:04.388
Inderjit Dhillon: Okay, so somewhere above I had the decision surface.


444
00:46:05.409 --> 00:46:06.599
Inderjit Dhillon: Let's see.


445
00:46:09.889 --> 00:46:12.078
Inderjit Dhillon: Well, I can repeat this part.


446
00:46:13.469 --> 00:46:16.788
Inderjit Dhillon: but at least let me make the simplifying thing where


447
00:46:17.219 --> 00:46:20.288
Inderjit Dhillon: you know. If Sigma one equals Sigma 2,


448
00:46:20.899 --> 00:46:25.429
Inderjit Dhillon: then this log of determinant will cancel with this log of determinant.


449
00:46:26.179 --> 00:46:28.368
Inderjit Dhillon: so I don't need to carry it through.


450
00:46:28.559 --> 00:46:34.159
Inderjit Dhillon: and I think this should be enough right. This captures. Well, I need to put in the Sigma inverse over here.


451
00:46:34.599 --> 00:46:38.968
Inderjit Dhillon: Sigma inverse over here, and then I'll capture this term.


452
00:46:39.239 --> 00:46:48.309
Inderjit Dhillon: and then I'll capture this game. Okay? So the decision surface is the following, which is one half


453
00:46:48.909 --> 00:46:51.939
Inderjit Dhillon: x minus mu, one transpose.


454
00:46:52.239 --> 00:46:58.119
Inderjit Dhillon: Sorry X minus new one transpose.


455
00:46:58.709 --> 00:47:02.259
Inderjit Dhillon: Well, let me just call it. If it's equal.


456
00:47:02.409 --> 00:47:04.708
Inderjit Dhillon: then we say that this is equal to Sigma.


457
00:47:10.059 --> 00:47:13.359
Inderjit Dhillon: So I have Sigma inverse.


458
00:47:13.619 --> 00:47:19.809
Inderjit Dhillon: That's minus mu one minus log of PC, one


459
00:47:20.499 --> 00:47:24.589
Inderjit Dhillon: is equal to one half x minus mu, 2.


460
00:47:25.659 --> 00:47:30.828
Inderjit Dhillon: Transpose sigma, inverse x minus mu, 2


461
00:47:31.739 --> 00:47:34.069
Inderjit Dhillon: minus log of PC, 2.


462
00:47:35.399 --> 00:47:41.289
Inderjit Dhillon: Okay, if I kind of simplify


463
00:47:41.439 --> 00:47:53.879
Inderjit Dhillon: right, let's do that. It's equal to one half x transpose sigma, inverse x minus 2


464
00:47:54.109 --> 00:47:59.629
Inderjit Dhillon: mu. One transpose sigma inverse X boy


465
00:48:01.609 --> 00:48:04.518
Inderjit Dhillon: and disease to take care of. So I need to be careful


466
00:48:05.029 --> 00:48:09.948
Inderjit Dhillon: plus mu one transpose sigma inverse, or m


467
00:48:10.079 --> 00:48:22.129
Inderjit Dhillon: keeping on doing sigma one. Okay, sigma inverse mu, one minus log of PC. One was equal to


468
00:48:23.659 --> 00:48:29.849
Inderjit Dhillon: one half x. Transpose sigma inverse x minus 2 mu. 2


469
00:48:30.239 --> 00:48:38.948
Inderjit Dhillon: sigma inverse x plus mu 2 transpose Sigma inverse Mu, 2 minus log of tc, 2.


470
00:48:39.829 --> 00:48:45.838
Inderjit Dhillon: Okay, I'm hoping I have not made a mistake with the indices. But please call me out if I have.


471
00:48:46.389 --> 00:48:54.889
Inderjit Dhillon: Okay. But the thing that you know this is all algebra. But you know, you need to understand what really comes out of the algebra there. What comes out of the algebra is


472
00:48:55.899 --> 00:48:58.718
Inderjit Dhillon: this is X transpose sigma inverse X.


473
00:49:00.229 --> 00:49:03.989
Inderjit Dhillon: This is also X transpose sigma inverse X, they'll actually cancel out.


474
00:49:04.689 --> 00:49:12.579
Inderjit Dhillon: So once again, we'll actually have a case where the quadratic term actually cancels out.


475
00:49:13.179 --> 00:49:18.379
Inderjit Dhillon: And we are actually left with a hyperplane. Okay? So


476
00:49:18.479 --> 00:49:23.018
Inderjit Dhillon: that's the kind of main point that I'm trying to get across.


477
00:49:23.269 --> 00:49:37.399
Inderjit Dhillon: This will basically mean that I have new 2 minus mu one transpose sigma inverse x plus one half


478
00:49:37.529 --> 00:49:38.559
Inderjit Dhillon: off.


479
00:49:40.129 --> 00:49:45.439
Inderjit Dhillon: I can actually write this as let's see, can I write this as mu one plus new 2


480
00:49:48.399 --> 00:49:54.139
Inderjit Dhillon: transpose sigma inverse new one minus metric?


481
00:49:56.129 --> 00:50:00.268
Inderjit Dhillon: Right? So you can check that I can write. Write it like this right? Because


482
00:50:00.539 --> 00:50:05.949
Inderjit Dhillon: the cross terms will kind of cancel out mu one with mu, 2 and mu 2 with new one.


483
00:50:07.169 --> 00:50:15.029
Inderjit Dhillon: right minus log of BC. 1 also, too Inconducy.


484
00:50:15.579 --> 00:50:19.369
Inderjit Dhillon: So once again I have.


485
00:50:19.839 --> 00:50:21.839
Inderjit Dhillon: Now this is W.


486
00:50:23.149 --> 00:50:25.439
Inderjit Dhillon: And this is W. Naught.


487
00:50:26.049 --> 00:50:27.389
Inderjit Dhillon: This is W. Sorry.


488
00:50:27.959 --> 00:50:29.758
Inderjit Dhillon: So W. Is different.


489
00:50:30.189 --> 00:50:38.759
Inderjit Dhillon: Right? So I have. W. Transpose x plus W. Naught equal to 0 where


490
00:50:40.339 --> 00:50:46.018
Inderjit Dhillon: W. Is equal to sigma inverse mu 2 minus mu one.


491
00:50:47.029 --> 00:50:51.919
Inderjit Dhillon: So it's not mu 2 minus mu one, but it is


492
00:50:52.089 --> 00:50:54.949
Inderjit Dhillon: sigma inverse times square, 2 minus mu one.


493
00:50:55.839 --> 00:50:57.766
Inderjit Dhillon: Okay, so let's


494
00:51:01.459 --> 00:51:02.579
Inderjit Dhillon: So am I?


495
00:51:04.479 --> 00:51:07.789
Inderjit Dhillon: Laptop is unplugged in. Let me just plug it in.


496
00:51:09.899 --> 00:51:11.640
Inderjit Dhillon: So this is kind of


497
00:51:14.309 --> 00:51:19.129
Inderjit Dhillon: Let's look at what it means again. Geometrically, right? So geometrically.


498
00:51:23.069 --> 00:51:26.588
Inderjit Dhillon: Remember the figure that I had last time where?


499
00:51:28.669 --> 00:51:31.119
Inderjit Dhillon: you know, we were trying to say that


500
00:51:31.829 --> 00:51:36.649
Inderjit Dhillon: actually, I don't really need the coordinate access over here right


501
00:51:38.709 --> 00:51:45.288
Inderjit Dhillon: when I say that you know some point, some multivariate Gaussian has covariance matrix.


502
00:51:45.839 --> 00:51:49.879
Inderjit Dhillon: right? We looked at kind of the


503
00:51:50.219 --> 00:51:55.918
Inderjit Dhillon: the way the distribution looks like. And we looked at the level sets of the distribution of Pfx.


504
00:51:56.109 --> 00:51:57.709
Inderjit Dhillon: And you know you can.


505
00:51:57.889 --> 00:52:04.239
Inderjit Dhillon: We did this right. That class c, 1 is this.


506
00:52:06.359 --> 00:52:12.648
Inderjit Dhillon: And you know it's kind of like you can think of this as an ellipse. I mean, it is an ellipse.


507
00:52:14.939 --> 00:52:19.649
Inderjit Dhillon: Okay, so this would be, let's say c. 1.


508
00:52:19.909 --> 00:52:23.708
Inderjit Dhillon: It has, let's say, mean new one.


509
00:52:24.399 --> 00:52:29.569
Inderjit Dhillon: and the fact that this is an ellipse is kind of captured by


510
00:52:29.889 --> 00:52:33.029
Inderjit Dhillon: that. It doesn't have my spherical shape.


511
00:52:34.029 --> 00:52:37.409
Inderjit Dhillon: It has an elliptical shape, and if you


512
00:52:37.519 --> 00:52:42.949
Inderjit Dhillon: remember last time we looked at you know, the way you can think about it is


513
00:52:43.059 --> 00:52:49.129
Inderjit Dhillon: that you can look at the covariance matrix, you can look at its eigenvalue, decomposition.


514
00:52:49.389 --> 00:53:07.668
Inderjit Dhillon: V. Lambda, v. Transpose, and this would be the direction of maximum variance, which is v. 1 that corresponds to the largest Eigenvector of Sigma one. And then you would have the smaller Eigenvector that correspond to the smaller Eigenvalues.


515
00:53:08.129 --> 00:53:15.999
Inderjit Dhillon: Okay, in this case, we are also assuming that, you know mute 2. You know the class C, 2.


516
00:53:18.299 --> 00:53:22.689
Inderjit Dhillon: The mean is different, right?


517
00:53:23.489 --> 00:53:25.459
Inderjit Dhillon: But the shape is the same.


518
00:53:27.329 --> 00:53:30.229
Inderjit Dhillon: And why is that? Because in this case


519
00:53:30.529 --> 00:53:35.568
Inderjit Dhillon: we assume that the covariance matrix was identical to


520
00:53:36.419 --> 00:53:39.219
Inderjit Dhillon: between the 2 classes. So Sigma 2


521
00:53:41.159 --> 00:53:43.568
Inderjit Dhillon: and the way they are tied to each other.


522
00:53:44.109 --> 00:53:51.419
Inderjit Dhillon: And our current analysis is that Sigma, one equal to Sigma, 2 equals.


523
00:53:53.209 --> 00:53:59.139
Inderjit Dhillon: Okay? And so if, for example, sigma 2 or mu 2 is this.


524
00:53:59.909 --> 00:54:02.231
Inderjit Dhillon: maybe I shouldn't touch on that


525
00:54:07.659 --> 00:54:09.758
Inderjit Dhillon: So if this is mu 2,


526
00:54:11.139 --> 00:54:15.068
Inderjit Dhillon: right? So mu remember, mu 2 minus mu, one is like this.


527
00:54:18.879 --> 00:54:23.398
Inderjit Dhillon: okay, but my decision surface, is not this.


528
00:54:23.749 --> 00:54:29.319
Inderjit Dhillon: Let me draw in a different color, right? If, before


529
00:54:29.749 --> 00:54:32.588
Inderjit Dhillon: my decision, surface would have been something like this.


530
00:54:39.159 --> 00:54:44.639
Inderjit Dhillon: let me just write it up if we ignored seeing what?


531
00:54:46.569 --> 00:54:52.489
Inderjit Dhillon: Okay? But now the decision surface is actually something which is


532
00:54:53.079 --> 00:54:55.428
Inderjit Dhillon: well, what is it? It is?


533
00:54:58.429 --> 00:55:01.149
Inderjit Dhillon: Let's see. What is Sigma? What is W


534
00:55:01.889 --> 00:55:04.569
Inderjit Dhillon: normal to the hyperplane? Is this?


535
00:55:05.259 --> 00:55:10.748
Inderjit Dhillon: So it's not mu 2 minus mu one. But it is sigma inverse times. Mu 2 minus mu one.


536
00:55:11.559 --> 00:55:20.498
Inderjit Dhillon: Okay? So generally, it'll be something which you know. So, maybe something like this.


537
00:55:21.849 --> 00:55:23.573
Inderjit Dhillon: how should I draw it?


538
00:55:27.309 --> 00:55:28.778
Inderjit Dhillon: something like this.


539
00:55:30.799 --> 00:55:34.059
Inderjit Dhillon: Okay. And this is too crowded.


540
00:55:34.179 --> 00:55:39.449
Inderjit Dhillon: But anyway, Sigma inverse me, too minus.


541
00:55:40.919 --> 00:55:46.439
Inderjit Dhillon: And the decision surface, remember, is actually perpendicular


542
00:55:47.349 --> 00:55:50.599
Inderjit Dhillon: to sigma inverse mu 2 minus mu one.


543
00:55:53.899 --> 00:55:56.809
Inderjit Dhillon: So this is my decision surface.


544
00:56:02.079 --> 00:56:09.269
Inderjit Dhillon: And of course, remember that this will be shifted to one side or the other, depending upon the class prior.


545
00:56:13.469 --> 00:56:16.629
Inderjit Dhillon: Okay, so you've now seen a case where


546
00:56:16.899 --> 00:56:21.468
Inderjit Dhillon: you know, by doing probabilistic modeling. So what have we done so far? Right. We have


547
00:56:23.349 --> 00:56:27.259
Inderjit Dhillon: just by intuition in the beginning we said, Hey, we'll just take


548
00:56:30.059 --> 00:56:31.389
Inderjit Dhillon: Take a


549
00:56:32.459 --> 00:56:43.818
Inderjit Dhillon: the mean and think of the decision surface as a perpendicular bisector of the line joining the 2 mean vectors. Okay? And we got a simple decision rule.


550
00:56:43.989 --> 00:56:47.879
Inderjit Dhillon: We saw that there was a geometric interpretation in terms of hyperplanes.


551
00:56:48.059 --> 00:56:53.698
Inderjit Dhillon: Then we try to do a little bit more of principled probabilistic modeling.


552
00:56:53.859 --> 00:56:57.329
Inderjit Dhillon: We modeled it simply in the beginning, as


553
00:56:57.669 --> 00:57:01.269
Inderjit Dhillon: you know, I have unit variance covariance


554
00:57:01.739 --> 00:57:09.138
Inderjit Dhillon: for both c. 1 and for both C. 2. And then we saw that we actually got a decision surface which was very, very close


555
00:57:09.349 --> 00:57:14.278
Inderjit Dhillon: to what we had developed, 1st by intuition other than the class prior


556
00:57:14.599 --> 00:57:22.188
Inderjit Dhillon: but one. This is a linear surface. So it's a linear decision boundary, right and very close to what we had by intuition.


557
00:57:22.589 --> 00:57:32.239
Inderjit Dhillon: But then we said, Okay, can we do? Maybe my data is does not have unit covariance.


558
00:57:32.889 --> 00:57:39.259
Inderjit Dhillon: Suppose there is a dependence between the different features that I have right.


559
00:57:39.409 --> 00:57:42.968
Inderjit Dhillon: and if there's a dependence, then maybe what I can do is


560
00:57:43.509 --> 00:57:49.978
Inderjit Dhillon: model it where Sigma? One equal to Sigma 2. But I'll still keep the covariances to be the same in both class


561
00:57:50.179 --> 00:57:51.558
Inderjit Dhillon: c, 1 and C. 2.


562
00:57:51.989 --> 00:57:58.638
Inderjit Dhillon: And at the beginning it looks like, Hey, you know, maybe this will be like a quadratic decision surface


563
00:57:59.338 --> 00:58:03.429
Inderjit Dhillon: but it turns out that you know things kind of


564
00:58:03.599 --> 00:58:11.089
Inderjit Dhillon: cancel out and you still get a linear decision surface. That means you still get your decision. Surface.


565
00:58:11.887 --> 00:58:14.379
Inderjit Dhillon: Surface to be a hyperplane.


566
00:58:16.029 --> 00:58:20.589
Inderjit Dhillon: Okay, but the hyperplane is no longer now like this.


567
00:58:21.769 --> 00:58:29.099
Inderjit Dhillon: It's no longer mu 2 minus mu one, and you know even your intuition would probably have said that this is not good.


568
00:58:30.529 --> 00:58:38.939
Inderjit Dhillon: right? Because, you know, if you don't want to classify some point over here some point over here


569
00:58:39.809 --> 00:58:44.499
Inderjit Dhillon: as belonging to c 1 which you would have done if you basically had


570
00:58:45.679 --> 00:58:53.068
Inderjit Dhillon: mu 2 minus mu, one mu, 2 minus mu mu one as the normal to the decision surface.


571
00:58:53.319 --> 00:58:59.509
Inderjit Dhillon: right? So you see that the decision surface that is actually sigma inverse mu, 2 minus mu one.


572
00:59:02.129 --> 00:59:06.149
Inderjit Dhillon: Okay, any questions.


573
00:59:11.109 --> 00:59:14.359
Inderjit Dhillon: Okay, let's come to the most general case now.


574
00:59:15.679 --> 00:59:22.119
Inderjit Dhillon: And what is the most general case where the covariances


575
00:59:25.909 --> 00:59:31.929
Inderjit Dhillon: Sigma one and Sigma 2 or different.


576
00:59:35.559 --> 00:59:44.169
Inderjit Dhillon: So Sigma, one could be, you know. Maybe the points are in Class c, 1, or distributed.


577
00:59:45.339 --> 00:59:50.648
Inderjit Dhillon: you know, like this as before.


578
00:59:54.779 --> 00:59:57.718
Inderjit Dhillon: Okay? And maybe Sigma 2 is totally different.


579
00:59:58.909 --> 01:00:00.878
Inderjit Dhillon: Maybe it's distributed like this.


580
01:00:07.239 --> 01:00:11.188
Inderjit Dhillon: Okay, so I have new one, Sigma, one over here.


581
01:00:11.489 --> 01:00:14.909
Inderjit Dhillon: So new one would be some right over here.


582
01:00:15.249 --> 01:00:18.278
Inderjit Dhillon: and this is Sigma one right? So the maximum


583
01:00:18.479 --> 01:00:23.618
Inderjit Dhillon: eigenvector of Sigma, one second maximum, and of course this could be in d dimensions.


584
01:00:23.959 --> 01:00:27.129
Inderjit Dhillon: Similarly, I have, let's say, mute 2.


585
01:00:27.899 --> 01:00:34.018
Inderjit Dhillon: And then I have Sigma, 2 maximum eigenvector of Sigma 2 next one, and so on.


586
01:00:34.939 --> 01:00:43.949
Inderjit Dhillon: Okay, so will the decision surface be linear. Now.


587
01:00:52.869 --> 01:00:55.719
Hormoz Shahrzad: I don't think so, because the a quadrant.


588
01:00:55.719 --> 01:00:57.129
Inderjit Dhillon: Yeah, so let's kind of just.


589
01:00:57.129 --> 01:00:59.238
Hormoz Shahrzad: Part doesn't get canceled out right.


590
01:00:59.239 --> 01:01:02.101
Inderjit Dhillon: Yeah. So let's mean, just kind kind of


591
01:01:03.779 --> 01:01:08.939
Inderjit Dhillon: repeat it. And actually, I need to be careful, right? Which one I copy because


592
01:01:10.879 --> 01:01:12.949
Inderjit Dhillon: this one I already made.


593
01:01:13.579 --> 01:01:19.218
Inderjit Dhillon: Oh, an assumption over here. That's Sigma, one equal to Sigma. Right?


594
01:01:19.479 --> 01:01:22.349
Inderjit Dhillon: Because I had this over here.


595
01:01:23.039 --> 01:01:29.369
Inderjit Dhillon: So let me kind of try to make sure that I copy the right thing.


596
01:01:30.089 --> 01:01:34.278
Inderjit Dhillon: and it is the decision surface


597
01:01:42.169 --> 01:01:49.208
Inderjit Dhillon: is one half log of the determinant of Sigma, one


598
01:01:49.989 --> 01:01:59.589
Inderjit Dhillon: plus one half x minus mu, one transpose sigma, one inverse x minus mu one.


599
01:02:00.419 --> 01:02:06.289
Inderjit Dhillon: It's equal to one half log of sigma, 2.


600
01:02:07.319 --> 01:02:11.339
Inderjit Dhillon: Okay. Oh, and then there was a minus log. P of c, 1 here.


601
01:02:12.389 --> 01:02:21.628
Inderjit Dhillon: Okay, plus one half x minus mu, 2. Transpose sigma, inverse x minus mu, 2


602
01:02:22.089 --> 01:02:26.959
Inderjit Dhillon: minus log of PC, 2 2 inverse. Right?


603
01:02:27.109 --> 01:02:33.619
Inderjit Dhillon: Sorry. Sorry I'm trying to hurry up. And thanks for catching catching this. Yeah.


604
01:02:34.139 --> 01:02:39.228
Inderjit Dhillon: okay, so you can see that you know the quadratic term over here.


605
01:02:39.569 --> 01:02:43.779
Inderjit Dhillon: If I expand this out, quadratic terms


606
01:02:49.039 --> 01:03:01.198
Inderjit Dhillon: is one half x transpose sigma, one inverse X and one half x transpose sigma, 2 endorse.


607
01:03:02.559 --> 01:03:08.889
Inderjit Dhillon: Okay, they will not cancel.


608
01:03:10.849 --> 01:03:16.028
Inderjit Dhillon: If Sigma one, it's not equal to Sigmato.


609
01:03:19.599 --> 01:03:20.792
Inderjit Dhillon: Okay? So


610
01:03:22.639 --> 01:03:37.659
Inderjit Dhillon: so then you'll basically have some kind of quadratic surface over here, you know, it can actually depend quite subtly on the shapes of Sigma one and Sigma 2. But you know the surface could be something like this, or or different.


611
01:03:38.849 --> 01:03:50.109
Inderjit Dhillon: Right? I think there's a book by Duda and Hart which shows different Sigma ones and Sigma twos, and then the different shapes of the the quadratic surface that you get


612
01:03:50.489 --> 01:03:51.289
Inderjit Dhillon: okay.


613
01:03:51.499 --> 01:03:56.269
Inderjit Dhillon: So so


614
01:03:57.609 --> 01:04:06.978
Inderjit Dhillon: so you can see that, you know, when you do probabilistic modeling things can actually get, you know, a little bit more complicated if you don't make simplifying assumptions. Okay.


615
01:04:07.099 --> 01:04:15.139
Inderjit Dhillon: so remember that many times, you know, you may not be.


616
01:04:15.489 --> 01:04:18.529
Inderjit Dhillon: You're given the training data, and that's all that you're given.


617
01:04:20.309 --> 01:04:24.989
Inderjit Dhillon: So this method will work well, right


618
01:04:25.599 --> 01:04:28.439
Inderjit Dhillon: if my modeling assumption is accurate.


619
01:04:29.169 --> 01:04:34.539
Inderjit Dhillon: Right? So, for example, my modeling assumption being that hey, the


620
01:04:34.669 --> 01:04:37.518
Inderjit Dhillon: you have a Gaussian model for each class.


621
01:04:38.119 --> 01:04:41.059
Inderjit Dhillon: and if that is indeed true, right.


622
01:04:41.279 --> 01:04:51.719
Inderjit Dhillon: and you can test it out by creating synthetic data that comes from this Gaussian model. Then, of course, you will get these surfaces right linear


623
01:04:52.410 --> 01:04:54.717
Inderjit Dhillon: linear and fairly simple. If


624
01:04:55.379 --> 01:04:57.939
Inderjit Dhillon: sigma, one equal to Sigma, 2 equal to I


625
01:04:58.139 --> 01:05:05.749
Inderjit Dhillon: linear, but with a covariance matrix where the normal to the hyperplane is sigma inverse mu, 2 minus mu one.


626
01:05:05.889 --> 01:05:09.939
Inderjit Dhillon: and in general quadratic. If sigma, one is not equal to Sigma. 2.


627
01:05:10.509 --> 01:05:11.379
Inderjit Dhillon: Okay.


628
01:05:12.339 --> 01:05:20.689
Inderjit Dhillon: But if you're only given sigma if you're only given the training data.


629
01:05:21.119 --> 01:05:25.436
Inderjit Dhillon: you know, it turns out that these methods can actually be,


630
01:05:26.299 --> 01:05:31.579
Inderjit Dhillon: can actually not perform that. Well, okay, so


631
01:05:33.229 --> 01:05:35.549
Inderjit Dhillon: what would be another kind of


632
01:05:35.659 --> 01:05:37.909
Inderjit Dhillon: drawback to this kind of approach?


633
01:05:42.549 --> 01:05:46.439
Inderjit Dhillon: Notice that in case one.


634
01:05:47.879 --> 01:05:52.729
Inderjit Dhillon: The surface is actually quite simple. simple, right?


635
01:05:54.819 --> 01:06:03.539
Inderjit Dhillon: But in case 2, you have this matrix sigma, where does Sigma come from?


636
01:06:05.249 --> 01:06:07.019
Inderjit Dhillon: Where does mu one come from?


637
01:06:09.239 --> 01:06:11.458
Hormoz Shahrzad: It's from the training data which.


638
01:06:11.459 --> 01:06:15.637
Inderjit Dhillon: The training data. Right? So you basically say, you can do something like


639
01:06:16.695 --> 01:06:22.898
Inderjit Dhillon: maximum likelihood estimate of mu, one maximum likelihood estimate of Mu 2.


640
01:06:22.999 --> 01:06:28.798
Inderjit Dhillon: And those are actually okay, because there's only a linear amount of parameters that you need to estimate.


641
01:06:29.359 --> 01:06:32.769
Inderjit Dhillon: The problem with kind of this approach is that


642
01:06:32.889 --> 01:06:37.149
Inderjit Dhillon: you actually then need to estimate a covariance matrix


643
01:06:38.009 --> 01:06:44.518
Inderjit Dhillon: right? And that estimator can actually be off them. And so, whatever advantage that you might have


644
01:06:44.689 --> 01:06:45.492
Inderjit Dhillon: using.


645
01:06:47.519 --> 01:06:55.659
Inderjit Dhillon: a more accurate model is then diminished by the fact that you need more and more training data to get an accurate estimator.


646
01:06:56.219 --> 01:07:21.009
Inderjit Dhillon: Okay, so even though kind of I've like introduced kind of probabilistic modeling. We'll see that these methods actually are not, you know, that competitive. But I think it's good to understand them right? Because if you think about a very natural way of trying to form a linear decision surface as we did in the beginning, then you can see that it actually follows from a modeling assumption.


647
01:07:21.483 --> 01:07:28.788
Inderjit Dhillon: And I also wanted to just exhibit this kind of way of doing things right like you have a probabilistic model of the data.


648
01:07:28.929 --> 01:07:33.582
Inderjit Dhillon: and then you use base rule and you get you get


649
01:07:34.828 --> 01:07:59.959
Inderjit Dhillon: you get a particular estimate of the parameters right of the W's, and we could actually also have done it with regression. So I did not do that. But you could formulate the problem such that, you know you have a multiv Gaussian, and you take the maximum likelihood estimator, and that gives you the least squares estimator over there. Right? So we've done this.


650
01:08:01.549 --> 01:08:08.898
Inderjit Dhillon: Next time we will move on to talk a little bit about other ways of doing classification.


651
01:08:09.059 --> 01:08:13.208
Inderjit Dhillon: We'll in particular talk about logistic regression


652
01:08:13.919 --> 01:08:22.558
Inderjit Dhillon: which you know, used to be one of the best ways of doing classification along with support vector machines. So we'll talk about


653
01:08:24.309 --> 01:08:25.114
Inderjit Dhillon: those.


654
01:08:26.379 --> 01:08:35.224
Inderjit Dhillon: well, we'll talk about the logistic Regression classifier and then we'll spend some more time later on doing


655
01:08:37.589 --> 01:08:39.279
Inderjit Dhillon: support vector machines.


656
01:08:39.609 --> 01:09:01.518
Inderjit Dhillon: And then, of course, you know, modern day, we use a lot of deep learning, so we'll start to now, talk a little bit about that. So next time we will start talking a little bit about not next next lecture, but I think next week we'll start introducing a little bit about deep learning. Talk a little bit about pytorch


657
01:09:01.669 --> 01:09:09.499
Inderjit Dhillon: and kind of get you started in that direction at the same time that we also look at some more classical methods.


658
01:09:10.511 --> 01:09:16.699
Inderjit Dhillon: So that's the end of this lecture? Any questions?


659
01:09:17.749 --> 01:09:22.668
Inderjit Dhillon: Oh, there's something on the chat. Sorry I didn't see that. Oh, yes, thank you. Okay.


660
01:09:25.239 --> 01:09:27.459
Inderjit Dhillon: Any other questions.


661
01:09:34.359 --> 01:09:39.018
Inderjit Dhillon: Okay? So if there are no more questions, and we conclude, for the day.


662
01:09:39.169 --> 01:09:41.929
Inderjit Dhillon: we will meet again on Wednesday.


663
01:09:42.569 --> 01:09:48.419
Inderjit Dhillon: I'll talk about logistic regression, among other things, and remember, your homework is due.


664
01:09:48.559 --> 01:09:51.449
Inderjit Dhillon: It is due on Friday.


665
01:09:52.219 --> 01:09:53.568
Inderjit Dhillon: Okay, thank you.


666
01:09:53.899 --> 01:09:54.539
Hormoz Shahrzad: Thank you.


667
01:09:55.419 --> 01:09:55.641
Letizia Fazzini: Thank you.



---- END OF LECTURE -------- START OF LECTURE 9 ----
WEBVTT

1
00:00:00.253 --> 00:00:02.172
Abhiram Maddukuri: I had a question about the homework.


2
00:00:02.953 --> 00:00:04.123
Inderjit Dhillon: Oh, yeah. Go ahead.


3
00:00:04.893 --> 00:00:07.573
Abhiram Maddukuri: Yeah. So question, one C on the homework


4
00:00:07.813 --> 00:00:19.182
Abhiram Maddukuri: like asked to center the matrix. So it says to like, 1st center the whole matrix by the whole mean, and then center the row to mean 0, and then do the same for the columns.


5
00:00:19.343 --> 00:00:22.462
Abhiram Maddukuri: But if you center the columns after the rows.


6
00:00:22.783 --> 00:00:28.892
Abhiram Maddukuri: The rows won't necessarily have a mean of 0. Right? So I was wondering if, like that's like expected or.


7
00:00:30.373 --> 00:00:31.563
Inderjit Dhillon: Ronelish.


8
00:00:37.853 --> 00:00:40.783
Nilesh Gupta: Yeah, so.


9
00:00:40.783 --> 00:00:46.773
Inderjit Dhillon: The question did, do you all sorry? You know. I guess we didn't get around to it, but I think you asked the question question on


10
00:00:46.993 --> 00:00:49.033
Inderjit Dhillon: the discussion platform. Also, right?


11
00:00:49.033 --> 00:00:49.873
Abhiram Maddukuri: Yeah, yeah.


12
00:00:50.113 --> 00:00:51.633
Inderjit Dhillon: Oh, sorry we didn't get to that.


13
00:00:51.633 --> 00:00:52.233
Inderjit Dhillon: No worries.


14
00:00:52.233 --> 00:00:53.192
Inderjit Dhillon: Go ahead! And Alicia.


15
00:00:54.013 --> 00:00:55.735
Nilesh Gupta: Yeah, I was just saying like,


16
00:00:56.223 --> 00:01:06.452
Nilesh Gupta: it is true. And for this it's it's it's like, your rows will no longer be like centered around 0. But like, that's still okay.


17
00:01:07.203 --> 00:01:08.493
Nilesh Gupta: like, you're located.


18
00:01:08.873 --> 00:01:10.343
Abhiram Maddukuri: Alright. Got it? Thank you. Yeah.


19
00:01:11.863 --> 00:01:15.102
Inderjit Dhillon: Yeah. So sometimes, when you do these operations, you know things.


20
00:01:15.773 --> 00:01:20.483
Inderjit Dhillon: A normalization that you may have done can get undone. But there are certain cases where


21
00:01:20.833 --> 00:01:34.333
Inderjit Dhillon: you know not in this particular case, but like, for example, if you normalize them, to have l. 2 norm. One, and you kept on repeating, you can actually iterate so that both will eventually have a column normal flow and column normal.


22
00:01:41.003 --> 00:01:45.642
Inderjit Dhillon: Okay, so let's start today's lecture. So today is the.


23
00:01:45.753 --> 00:01:54.202
Inderjit Dhillon: you know, second lecture on classification. So today we'll talk about some regression approaches to classification.


24
00:01:54.433 --> 00:02:06.763
Inderjit Dhillon: So classification last time, what we did was we did some probabilistic modeling, where we assume that


25
00:02:06.873 --> 00:02:30.902
Inderjit Dhillon: each class had a certain distribution, for example, a multivariate normal distribution. And then we saw that in some cases, we got a linear classifier or linear decision surface. And in some other cases we got a quadratic decision surface. So today, let's kind of think back a little bit back to regression and see if you know what would be. Regression approaches


26
00:02:31.073 --> 00:02:36.293
Inderjit Dhillon: to classification, and later on, as we will see.


27
00:02:36.413 --> 00:02:46.793
Inderjit Dhillon: you know what I'm going to talk about today actually ends up being used partially in current deep learning architectures.


28
00:02:48.833 --> 00:02:58.602
Inderjit Dhillon: So again, a problem is, you have. Xi, Yi, this is the training data you have. Xi belongs to Rd.


29
00:02:59.433 --> 00:03:03.122
Inderjit Dhillon: You have now. Why, I remember, are


30
00:03:03.363 --> 00:03:08.743
Inderjit Dhillon: we are doing classification. Let's consider the K class classification proper. So that means, instead of.


31
00:03:08.913 --> 00:03:17.692
Inderjit Dhillon: you know, last time we considered K equal to class one and class 2. But today, let's be a little bit more general in the beginning, right? Where we


32
00:03:18.743 --> 00:03:29.833
Inderjit Dhillon: consider a general K class classification problem. And of course, when K equal to 2, then you will get the 2 class classification problem. So we're going to actually say that Ryi


33
00:03:30.573 --> 00:03:38.353
Inderjit Dhillon: is belongs to Rk, and this is going to be the K class classification problem.


34
00:03:43.823 --> 00:03:44.783
Inderjit Dhillon: Okay?


35
00:03:45.953 --> 00:03:50.003
Inderjit Dhillon: And then, of course, you know, we have. I equals one


36
00:03:50.123 --> 00:03:55.802
Inderjit Dhillon: to N capital N being the size of the training data. So just as we had before


37
00:03:56.273 --> 00:04:01.932
Inderjit Dhillon: remember just writing it out. I have my training points. Suppose let's suppose I have X


38
00:04:02.313 --> 00:04:07.872
Inderjit Dhillon: first, st component, second component, and there are d components.


39
00:04:08.713 --> 00:04:15.613
Inderjit Dhillon: Okay, and similarly, we have y, which is now, K,


40
00:04:15.743 --> 00:04:20.702
Inderjit Dhillon: okay, so remember that this size is d, it's d-dimensional.


41
00:04:20.863 --> 00:04:27.042
Inderjit Dhillon: And this is K dimensional. And what we will say is that Y


42
00:04:28.083 --> 00:04:34.833
Inderjit Dhillon: is an indicator, vector, right? Which basically means that it has just 1 1.


43
00:04:35.733 --> 00:04:39.503
Inderjit Dhillon: Okay, in the jet position.


44
00:04:43.423 --> 00:04:49.513
Inderjit Dhillon: Okay? And that basically. So suppose this is Yi, and this means that


45
00:04:49.713 --> 00:04:52.822
Inderjit Dhillon: it is a way of encoding


46
00:04:53.413 --> 00:04:57.163
Inderjit Dhillon: that Xi belongs to class. J,


47
00:04:58.513 --> 00:05:04.813
Inderjit Dhillon: okay, so you can think of it as the Jf position. You can also think of this as the Jf column


48
00:05:04.923 --> 00:05:09.003
Inderjit Dhillon: or raw of the identity matrix.


49
00:05:12.483 --> 00:05:16.162
Inderjit Dhillon: So that's how we are kind of encoding the classification problem.


50
00:05:16.473 --> 00:05:21.802
Inderjit Dhillon: And then what we want to do is just as we did before a linear method


51
00:05:22.013 --> 00:05:32.292
Inderjit Dhillon: for regression. And but let's look at a linear method for classification. Okay, so my linear. My coefficients are going to be w's so w naught.


52
00:05:32.593 --> 00:05:39.692
Inderjit Dhillon: plus w. 1 x one plus W. 2 x 2,


53
00:05:40.253 --> 00:05:50.743
Inderjit Dhillon: and I have d plus one coefficients. WDXD, yeah. So this is the linear fit.


54
00:05:51.683 --> 00:05:56.723
Inderjit Dhillon: And I can think of. I'll denote this as y given X,


55
00:05:57.493 --> 00:06:00.463
Inderjit Dhillon: okay, y of function of X,


56
00:06:00.803 --> 00:06:04.323
Inderjit Dhillon: and generally these kinds of in the classification setting.


57
00:06:04.443 --> 00:06:12.232
Inderjit Dhillon: These kinds of Y's are called linear discriminants, because what they are trying to do is discriminate between


58
00:06:12.393 --> 00:06:15.172
Inderjit Dhillon: one from one class to the other.


59
00:06:15.733 --> 00:06:19.763
Inderjit Dhillon: So each each class.


60
00:06:20.093 --> 00:06:31.463
Inderjit Dhillon: we'll actually have a different linear or discriminator. Sorry, discriminate discriminates. This could should be discriminate.


61
00:06:32.683 --> 00:06:45.643
Inderjit Dhillon: Okay, so I'll have wj, this is a, vector, okay, transpose X plus Wj, not


62
00:06:46.873 --> 00:06:51.423
Inderjit Dhillon: okay. And this is a linear discriminant for the Jf class.


63
00:06:52.953 --> 00:06:55.552
Inderjit Dhillon: Okay, so let's let's write it out again.


64
00:06:55.823 --> 00:07:01.853
Inderjit Dhillon: Suppose I, take all the I'm given training data set.


65
00:07:02.143 --> 00:07:06.492
Inderjit Dhillon: I'll have training examples for Class c, 1


66
00:07:06.803 --> 00:07:13.842
Inderjit Dhillon: training data points that belong to class C, 2, and so on. So that's for the moment in the training data set


67
00:07:14.023 --> 00:07:17.453
Inderjit Dhillon: list, all the X's that belong to Class one.


68
00:07:18.273 --> 00:07:25.743
Inderjit Dhillon: then class 2, then class 3, and so on. Right? So so I'm going to arrange my training data


69
00:07:26.443 --> 00:07:35.172
Inderjit Dhillon: in this matrix, right? Just very similar to what we had done for the linear regression problem.


70
00:07:35.363 --> 00:07:41.853
Inderjit Dhillon: And now I have x, 1 x, 1, 1 x 1, 2


71
00:07:42.623 --> 00:07:45.352
Inderjit Dhillon: x 1 d. So I basically listed


72
00:07:45.533 --> 00:07:50.863
Inderjit Dhillon: the 1st training data point as the row of this matrix.


73
00:07:51.403 --> 00:07:57.042
Inderjit Dhillon: And I'm going to do the corresponding linear discriminant right?


74
00:07:57.223 --> 00:08:02.292
Inderjit Dhillon: And that is going to be W. 1 0 w.


75
00:08:04.453 --> 00:08:09.703
Inderjit Dhillon: Oh, and then this this part over here.


76
00:08:10.193 --> 00:08:13.193
Inderjit Dhillon: just if if I want to keep this notation.


77
00:08:14.223 --> 00:08:19.663
Inderjit Dhillon: you know, I'll just say that this is w 1 bar


78
00:08:19.913 --> 00:08:22.753
Inderjit Dhillon: we get. This is a size D,


79
00:08:23.533 --> 00:08:27.722
Inderjit Dhillon: and this is of size. D, and this is, of course, one.


80
00:08:28.563 --> 00:08:34.062
Inderjit Dhillon: Okay, and there's going to be. And and I'm going to arrange all the training data for c 1


81
00:08:34.333 --> 00:08:35.472
Inderjit Dhillon: over here.


82
00:08:35.763 --> 00:08:38.713
Inderjit Dhillon: Okay, so what that means is


83
00:08:39.093 --> 00:08:44.782
Inderjit Dhillon: that when I take this and I multiply with the corresponding linear discriminant.


84
00:08:45.393 --> 00:08:50.873
Inderjit Dhillon: Okay, I'll have other linear discriminants. W. 2 0 w. 2 bar.


85
00:08:51.983 --> 00:08:57.542
Inderjit Dhillon: and so on. So I'll have. I have K classes. So I have K, linear discriminants.


86
00:08:58.053 --> 00:09:06.423
Inderjit Dhillon: K, 0, wk, okay?


87
00:09:06.713 --> 00:09:09.912
Inderjit Dhillon: And what I want to happen is that the first, st


88
00:09:10.203 --> 00:09:14.802
Inderjit Dhillon: when I take this times, the 1st linear discriminant.


89
00:09:15.213 --> 00:09:26.363
Inderjit Dhillon: I want to get one, but for all the other discriminants right, I will get zeros


90
00:09:31.083 --> 00:09:32.413
Inderjit Dhillon: blue locate.


91
00:09:34.213 --> 00:09:35.573
Inderjit Dhillon: This is K.


92
00:09:36.863 --> 00:09:41.063
Inderjit Dhillon: This is K, so that means that you know.


93
00:09:42.138 --> 00:09:52.073
Inderjit Dhillon: X. This is in this language, w. 1 transpose x plus w. 1 naught.


94
00:09:52.233 --> 00:09:55.793
Inderjit Dhillon: I want that to give a 1,


95
00:09:56.103 --> 00:10:02.452
Inderjit Dhillon: but for the other discriminants. I want this to give a 0. So this now becomes the corresponding


96
00:10:02.603 --> 00:10:06.162
Inderjit Dhillon: yi, which is like this one over here.


97
00:10:06.643 --> 00:10:13.663
Inderjit Dhillon: Okay, similarly, the second data point belongs to one, and


98
00:10:15.233 --> 00:10:20.353
Inderjit Dhillon: my 1st few points belong to classy one.


99
00:10:20.683 --> 00:10:25.112
Inderjit Dhillon: So I have this pattern, and if this belongs to C. 2,


100
00:10:25.233 --> 00:10:28.913
Inderjit Dhillon: what I will have is 0 1 0 0


101
00:10:29.173 --> 00:10:31.863
Inderjit Dhillon: 0 1 0 0, and so on.


102
00:10:34.893 --> 00:10:40.053
Inderjit Dhillon: And then when I have a class, see K over here.


103
00:10:40.603 --> 00:10:51.462
Inderjit Dhillon: right? So class. Ck, you know, there'll be some last point, which is Xn, one xn, 2 x. Md.


104
00:10:52.313 --> 00:10:58.553
Inderjit Dhillon: okay. And in between there'll be something which is one XYXI, 1,


105
00:10:59.603 --> 00:11:06.172
Inderjit Dhillon: XI, 2, through XIT, okay, and this will be


106
00:11:07.953 --> 00:11:12.492
Inderjit Dhillon: 0 0 and a 1 in the kids place.


107
00:11:13.953 --> 00:11:18.493
Inderjit Dhillon: Okay, so what I want is that this


108
00:11:18.793 --> 00:11:26.253
Inderjit Dhillon: is approximately equal to this again, we don't want to overfit, but we want to do. Do want to fit to the training data.


109
00:11:26.413 --> 00:11:34.822
Inderjit Dhillon: Let me call this matrix as X. This is the matrix of training data, very similar or almost identical to what we did in linear regression.


110
00:11:35.073 --> 00:11:40.273
Inderjit Dhillon: This, now is the matrix of coefficients.


111
00:11:40.433 --> 00:11:43.062
Inderjit Dhillon: And you'll see that basically, there is


112
00:11:43.273 --> 00:11:48.223
Inderjit Dhillon: a vector for every class, which is the linear discriminant.


113
00:11:48.713 --> 00:11:52.182
Inderjit Dhillon: Okay, so this is X times W,


114
00:11:52.593 --> 00:11:55.523
Inderjit Dhillon: and this matrix I can call as y.


115
00:11:57.213 --> 00:12:01.782
Inderjit Dhillon: Okay. So what I did I want is, I want to find coefficients. W,


116
00:12:02.253 --> 00:12:07.723
Inderjit Dhillon: so that it gives me Xw is nearly equal to y.


117
00:12:08.973 --> 00:12:16.563
Inderjit Dhillon: Okay, now, we've seen this before. And you might say, Okay, I've also said in the beginning that you know I'm doing kind of a regression approach.


118
00:12:16.783 --> 00:12:19.893
Inderjit Dhillon: and then you might ask, oh, you know, can I?


119
00:12:20.043 --> 00:12:24.292
Inderjit Dhillon: What do I mean by Xw is nearly equal to Y


120
00:12:24.763 --> 00:12:29.343
Inderjit Dhillon: right? So one can potentially, I'm not saying this is actually going to work well.


121
00:12:29.453 --> 00:12:34.252
Inderjit Dhillon: But one could potentially do the following right is to think about


122
00:12:35.523 --> 00:12:46.202
Inderjit Dhillon: trying to find W. Such that this is minimized, which is the square arrow.


123
00:12:49.303 --> 00:12:53.262
Inderjit Dhillon: And if you think about it, if you remember, the solution to this is.


124
00:12:53.563 --> 00:13:01.053
Inderjit Dhillon: X transpose X. Times, W. Star, I'm skipping all the steps in between. I'm more taking the gradient.


125
00:13:01.303 --> 00:13:05.123
Inderjit Dhillon: and so on is equal to X, transpose y.


126
00:13:05.283 --> 00:13:12.643
Inderjit Dhillon: So note that the difference from linear regression was that in linear regression, when we looked at only one target.


127
00:13:12.943 --> 00:13:17.452
Inderjit Dhillon: right? We only had one single column of voice


128
00:13:18.093 --> 00:13:21.033
Inderjit Dhillon: and one single column for W's.


129
00:13:21.463 --> 00:13:27.642
Inderjit Dhillon: But here, what we are saying is, we have K classes. So we'll actually have okay, different columns


130
00:13:29.443 --> 00:13:35.163
Inderjit Dhillon: of Y, and similarly, we'll have one coefficient for each class.


131
00:13:35.983 --> 00:13:36.733
Inderjit Dhillon: Okay?


132
00:13:38.383 --> 00:13:45.433
Inderjit Dhillon: And then, if WX. Transpose, X is invertible, then I will get the solution.


133
00:13:50.233 --> 00:13:53.863
Inderjit Dhillon: So that could be like a 1st regression approach


134
00:13:53.993 --> 00:14:02.343
Inderjit Dhillon: that you might be that you might use for classification. Okay, and remember that XW. Star


135
00:14:02.783 --> 00:14:05.892
Inderjit Dhillon: will be the prediction on the training data.


136
00:14:14.683 --> 00:14:22.742
Inderjit Dhillon: Okay? So now just think about it. I want a new XA new X little X


137
00:14:23.623 --> 00:14:30.573
Inderjit Dhillon: times. W. Star would be the prediction. What will it give me? It'll give me a K dimensional vector.


138
00:14:32.093 --> 00:14:34.403
Inderjit Dhillon: right? K dimensional. Vector.


139
00:14:37.773 --> 00:14:40.703
Inderjit Dhillon: remember that W is d, by K,


140
00:14:41.663 --> 00:14:46.952
Inderjit Dhillon: okay, W belongs to RD, by K,


141
00:14:47.863 --> 00:14:51.873
Inderjit Dhillon: so when I take X transpose, which is.


142
00:14:51.873 --> 00:14:52.973
Hormoz Shahrzad: Oh, I'm sorry!


143
00:14:53.293 --> 00:14:54.142
Inderjit Dhillon: Sorry go ahead.


144
00:14:54.143 --> 00:14:56.973
Hormoz Shahrzad: D, or d, plus one by k.


145
00:14:58.212 --> 00:15:06.202
Inderjit Dhillon: Yes. Good point I did include d right? So there's 1 over here. So good. Good point. Thank you.


146
00:15:06.863 --> 00:15:09.012
Inderjit Dhillon: This is d plus one by K,


147
00:15:14.993 --> 00:15:22.093
Inderjit Dhillon: okay, and remember that X is, or, oh.


148
00:15:23.673 --> 00:15:27.392
Inderjit Dhillon: so remember, I'm doing Rd, plus one, because it's of this form.


149
00:15:29.113 --> 00:15:37.022
Inderjit Dhillon: Okay, so d features. But the 1st one corresponds to the bias term.


150
00:15:40.043 --> 00:15:42.763
Inderjit Dhillon: It seems that looks like a 1. So


151
00:15:47.063 --> 00:15:54.062
Inderjit Dhillon: okay, and what you would like is, you would actually like this X transpose W. Star.


152
00:15:54.323 --> 00:15:57.093
Inderjit Dhillon: you know, like there are key numbers.


153
00:15:58.413 --> 00:16:10.613
Inderjit Dhillon: You want it to be one if it belongs to the class. So suppose this belongs to. You know, this is something which is close to something on the the training data. And it says, this is point 9 8,


154
00:16:11.323 --> 00:16:17.532
Inderjit Dhillon: right? Maybe you want these to be kind of probabilities, right? Maybe this is point 0 1.


155
00:16:17.743 --> 00:16:20.632
Inderjit Dhillon: This is point 0 2, and so on.


156
00:16:22.144 --> 00:16:27.593
Inderjit Dhillon: That's what you would like, right? And this would be the Jth position


157
00:16:31.943 --> 00:16:41.813
Inderjit Dhillon: if Xi belongs to Cj, okay, however, the above


158
00:16:42.793 --> 00:16:47.492
Inderjit Dhillon: right. And when I say the above, I am talking about this because this is


159
00:16:47.733 --> 00:16:50.843
Inderjit Dhillon: given by a least squares problem


160
00:16:50.993 --> 00:16:56.093
Inderjit Dhillon: in the least squares problem. Really right? When I do.


161
00:16:56.483 --> 00:16:59.183
Inderjit Dhillon: XW. Star.


162
00:16:59.473 --> 00:17:05.723
Inderjit Dhillon: there is no guarantee that these values will actually be probabilities or so on right? In fact, they can actually be negative


163
00:17:06.583 --> 00:17:07.493
Inderjit Dhillon: right?


164
00:17:08.283 --> 00:17:11.742
Inderjit Dhillon: So this, the above W star


165
00:17:15.123 --> 00:17:22.782
Inderjit Dhillon: can lead 2 x. Transpose W. Star being of the form.


166
00:17:23.723 --> 00:17:26.523
Inderjit Dhillon: you know some negative number over here.


167
00:17:28.593 --> 00:17:31.233
Inderjit Dhillon: The plan positive numbers here.


168
00:17:32.693 --> 00:17:34.913
Inderjit Dhillon: Nothing related to probabilities.


169
00:17:36.073 --> 00:17:40.293
Inderjit Dhillon: Okay? And clearly, okay. And clearly.


170
00:17:45.893 --> 00:17:51.653
Inderjit Dhillon: this is not a good approximation.


171
00:17:57.283 --> 00:18:01.253
Inderjit Dhillon: 2 and indicator vectors.


172
00:18:05.113 --> 00:18:09.542
Inderjit Dhillon: Okay, which is kind of of this form, 0 0 1 0, 0.


173
00:18:10.263 --> 00:18:13.613
Inderjit Dhillon: Okay, often, this is also called a 1 hot factor.


174
00:18:15.713 --> 00:18:22.903
Inderjit Dhillon: Okay, so just least squares just these squares


175
00:18:26.183 --> 00:18:27.953
Inderjit Dhillon: for classification


176
00:18:32.433 --> 00:18:34.383
Inderjit Dhillon: as obvious products.


177
00:18:40.913 --> 00:18:44.503
Inderjit Dhillon: Okay, it's it is kind of like an obvious method to try.


178
00:18:44.803 --> 00:18:53.983
Inderjit Dhillon: But when you look at it, you realize that you know the values that you get really can't be interpreted as probability. So it's really not a great method.


179
00:18:54.683 --> 00:18:58.232
Inderjit Dhillon: Okay? But we are just introducing it. And later on you will see


180
00:18:58.383 --> 00:19:03.042
Inderjit Dhillon: that actually, when we come to deep learning, then


181
00:19:04.053 --> 00:19:07.492
Inderjit Dhillon: you can actually have different nonlinear functions


182
00:19:07.753 --> 00:19:11.332
Inderjit Dhillon: that can take these real numbers and then map them


183
00:19:11.803 --> 00:19:21.353
Inderjit Dhillon: onto, let's say, a probability distribution. There's something called softmax, which does exactly that. It'll basically take a vector and it'll map into


184
00:19:22.561 --> 00:19:32.032
Inderjit Dhillon: numbers between 0 and one that can be interpreted as probabilities. And the sum of those numbers will add up to one. So we'll come to that.


185
00:19:32.233 --> 00:19:38.972
Inderjit Dhillon: Okay. But classically, you know, the above that I've done is, you know, something to look at.


186
00:19:39.123 --> 00:19:45.172
Inderjit Dhillon: but then also quickly, disregard and say, Okay, this is not really a very good method.


187
00:19:46.683 --> 00:19:51.042
Inderjit Dhillon: but I just wanted to kind of introduce that, because, you know, it's an obvious thing to try


188
00:19:54.663 --> 00:19:56.203
Inderjit Dhillon: any question.


189
00:20:00.533 --> 00:20:08.993
Inderjit Dhillon: So we've so far actually introduced 2 methods right? One through probabilistic modeling and one over here.


190
00:20:09.413 --> 00:20:13.842
Inderjit Dhillon: And actually, neither is that great a method. Okay, it's not really what


191
00:20:14.243 --> 00:20:19.563
Inderjit Dhillon: was used in practice even before the days of deep learning.


192
00:20:19.783 --> 00:20:24.053
Inderjit Dhillon: So let me now talk about one very, very popular method


193
00:20:25.763 --> 00:20:32.212
Inderjit Dhillon: which, before the days of deep learning, everybody used to do this logistic regression I'll talk about.


194
00:20:32.373 --> 00:20:39.312
Inderjit Dhillon: And then also, I'll talk about support vector machines. So these were the most popular methods of doing classification.


195
00:20:41.053 --> 00:20:44.183
Inderjit Dhillon: So logistic regression.


196
00:20:45.353 --> 00:20:47.812
Inderjit Dhillon: And it's actually very good to understand this.


197
00:20:48.983 --> 00:20:52.553
Inderjit Dhillon: even with the current. You know, deep learning methodologies.


198
00:20:54.223 --> 00:20:57.083
Inderjit Dhillon: Okay, so let's do a little review.


199
00:20:57.993 --> 00:21:10.613
Inderjit Dhillon: Okay, last time, when we did probabilistic modeling, right? What had we done? We had modeled each class


200
00:21:12.283 --> 00:21:13.623
Inderjit Dhillon: as a Gaussian


201
00:21:18.433 --> 00:21:19.513
Inderjit Dhillon: ribs.


202
00:21:19.943 --> 00:21:21.543
Inderjit Dhillon: Covariant. Sigma.


203
00:21:25.893 --> 00:21:42.402
Inderjit Dhillon: Okay? And if I I'm not going to derive it, and so on. But the rule that we had got was something like this, that we were interested in the posterior, which is P. Of ci, given X. Suppose this is the K class problem.


204
00:21:42.953 --> 00:21:49.043
Inderjit Dhillon: and we. I was trying to distinguish it from a 2 class problem from the Ci and Cj.


205
00:21:49.243 --> 00:21:57.913
Inderjit Dhillon: Then this log of the posterior was the log of the prior


206
00:21:58.873 --> 00:22:06.063
Inderjit Dhillon: probabilities, the ratio. Okay? And then there was this linear term, or


207
00:22:07.203 --> 00:22:08.393
Inderjit Dhillon: Oh.


208
00:22:08.713 --> 00:22:16.492
Inderjit Dhillon: so so remember, this is the I'm assuming that the covariance is identical. So the quadratic term goes away and you get


209
00:22:16.933 --> 00:22:21.362
Inderjit Dhillon: mi plus mj, you can look back at the notes from last time.


210
00:22:21.493 --> 00:22:26.013
Inderjit Dhillon: And this was what we've got MI, minus mj.


211
00:22:26.473 --> 00:22:33.153
Inderjit Dhillon: plus x transpose sigma inverse m, 1 minus m. 2. So this is the linear term.


212
00:22:33.503 --> 00:22:40.813
Inderjit Dhillon: right? So this part you can think of, as you know, W. Naught.


213
00:22:41.533 --> 00:22:49.613
Inderjit Dhillon: And then this is the linear term, and I can think of it as w. 1 trans.


214
00:22:49.893 --> 00:22:51.612
Inderjit Dhillon: W. Transpose x.


215
00:22:55.843 --> 00:23:02.152
Hormoz Shahrzad: So the last parenthesis is also mi minus mj, right? Not.


216
00:23:02.153 --> 00:23:04.523
Inderjit Dhillon: Thank you. Thank you for keeping me honest.


217
00:23:05.483 --> 00:23:10.863
Inderjit Dhillon: Canada's MI, minus. Mjs.


218
00:23:13.913 --> 00:23:14.813
Inderjit Dhillon: okay.


219
00:23:16.463 --> 00:23:17.613
Inderjit Dhillon: So


220
00:23:19.653 --> 00:23:28.352
Inderjit Dhillon: you see from here that you know, even though we did this probabilistic modeling, we finally got a decision. So decision rule of this form.


221
00:23:29.973 --> 00:23:40.342
Inderjit Dhillon: The problem with, like I told you last time is when we try to model each class as a Gaussian with covariance. Sigma is at 1st you have to actually


222
00:23:43.313 --> 00:23:46.062
Inderjit Dhillon: try to estimate Sigma for each class.


223
00:23:47.013 --> 00:23:54.723
Inderjit Dhillon: and that actually becomes complicated because Sigma has, you know, a number of parameters that is quadratic in the dimensionality.


224
00:23:55.283 --> 00:24:00.209
Inderjit Dhillon: Right? So while this approach, you know, allows you to


225
00:24:01.603 --> 00:24:12.612
Inderjit Dhillon: get to W. Naught without really doing much competition. Right? I mean, you have to do Sigma inverse. But once you estimate Sigma Sigma inverse, basically you get the formula.


226
00:24:13.313 --> 00:24:15.352
Inderjit Dhillon: but the formula is of this form.


227
00:24:15.843 --> 00:24:27.223
Inderjit Dhillon: And if you remember where this came from, this came from modeling the data within each class using a probabilistic. So P. Of x given C day.


228
00:24:27.923 --> 00:24:32.122
Inderjit Dhillon: So the flow is that, hey? Look.


229
00:24:32.293 --> 00:24:44.442
Inderjit Dhillon: let me just bypass this modeling because you know that probabilistic modeling. And, you know, figuring out the covariance matrix and the parameters of the distribution is a little harder.


230
00:24:44.623 --> 00:24:51.432
Inderjit Dhillon: Let me just say that I'm going to model the log priors.


231
00:24:51.563 --> 00:24:58.102
Inderjit Dhillon: the log the the log ratios of the probabilities with a linear function.


232
00:24:58.693 --> 00:25:03.112
Inderjit Dhillon: So what logistic regression says is, suppose I have a K class problem.


233
00:25:07.893 --> 00:25:15.903
Inderjit Dhillon: Then I'm gonna look at log off BC, 1. Given X,


234
00:25:16.153 --> 00:25:23.532
Inderjit Dhillon: and for all these probabilities I will normalize them with or take the ratio. With respect to the last class.


235
00:25:26.693 --> 00:25:33.692
Inderjit Dhillon: and I'll say that this log odds is w naught


236
00:25:34.083 --> 00:25:40.603
Inderjit Dhillon: plus let me just call it w. 1. Tilde, transpose X, because I don't want to be carrying this around.


237
00:25:40.853 --> 00:25:44.413
Inderjit Dhillon: And this is w, 1 transpose X,


238
00:25:45.013 --> 00:25:48.492
Inderjit Dhillon: okay, where? You know access.


239
00:25:48.923 --> 00:25:54.443
Inderjit Dhillon: You know, this is X bar, and X has, you know, the bias term one in its 1st coordinate.


240
00:25:57.963 --> 00:26:01.122
Inderjit Dhillon: and then I have log of B.


241
00:26:01.603 --> 00:26:09.063
Inderjit Dhillon: See 2 given XBCK. Given X


242
00:26:10.083 --> 00:26:13.533
Inderjit Dhillon: is equal to W, 2. Transpose X,


243
00:26:14.873 --> 00:26:20.513
Inderjit Dhillon: okay, and what we are gonna do is we'll need to find the W's right. That's what we need to find.


244
00:26:20.853 --> 00:26:25.003
Inderjit Dhillon: And similarly, I can go all the way. So I have K minus one


245
00:26:26.343 --> 00:26:32.313
Inderjit Dhillon: pck, minus one. Given X, divided by ck, k, 1 x


246
00:26:32.623 --> 00:26:35.903
Inderjit Dhillon: is equal to WK, minus one transfer.


247
00:26:37.383 --> 00:26:46.922
Inderjit Dhillon: Okay, so this is now my 1st situation, second equations and K, minus, 1st equation.


248
00:26:50.633 --> 00:26:51.393
Inderjit Dhillon: Okay?


249
00:26:51.913 --> 00:26:57.551
Inderjit Dhillon: And just for simplicity, because, you know, I don't want to carry this around. I'm gonna do a bunch of


250
00:26:58.023 --> 00:27:04.603
Inderjit Dhillon: algebraic manipulations. Let me write this right be.


251
00:27:04.783 --> 00:27:08.783
Inderjit Dhillon: See? I given X as pi.


252
00:27:11.563 --> 00:27:18.902
Inderjit Dhillon: Okay? And if I do that right, then I get that. You know, my 1st equation is log


253
00:27:19.563 --> 00:27:23.473
Inderjit Dhillon: b, 1 divided by p. 2


254
00:27:24.033 --> 00:27:30.472
Inderjit Dhillon: is equal to w, 1 transpose X, which means that


255
00:27:31.043 --> 00:27:40.223
Inderjit Dhillon: p. 1. Given p. Sorry, divided by p. 2, is equal to E to the power. w. 1 transpose X.


256
00:27:40.683 --> 00:27:43.553
Inderjit Dhillon: So this is first, st okay?


257
00:27:44.453 --> 00:27:48.622
Inderjit Dhillon: And similarly, I'll get. Oh, did I do? p. 1 divided by p. 2.


258
00:27:48.623 --> 00:27:53.193
Hormoz Shahrzad: That's a pk, yes, perfect. I actually got this before you.


259
00:27:53.643 --> 00:27:54.553
Hormoz Shahrzad: Yeah, yeah.


260
00:27:54.888 --> 00:27:59.913
Inderjit Dhillon: Okay, so I just realized when I saw it that yes, this should be pk.


261
00:28:01.523 --> 00:28:06.122
Inderjit Dhillon: okay, because we are dealing with a general K class problem. Okay, so similarly, we'll have


262
00:28:06.473 --> 00:28:13.343
Inderjit Dhillon: p. 2. Given, divided by Pk is equal to E to the power. W. 2. Transpose X


263
00:28:14.913 --> 00:28:18.753
Inderjit Dhillon: and all the way till PK. Minus one


264
00:28:19.513 --> 00:28:25.503
Inderjit Dhillon: divided by Pk is E to the power. Wk, minus one transpose. X,


265
00:28:31.433 --> 00:28:37.413
Inderjit Dhillon: okay. And remember, what we want to try to find is, the W's.


266
00:28:38.003 --> 00:28:39.872
Inderjit Dhillon: The W's that.


267
00:28:40.373 --> 00:28:46.331
Inderjit Dhillon: you know. Give us good predictions. You know they fit the training data, plus they are good on


268
00:28:47.223 --> 00:28:48.343
Inderjit Dhillon: new point.


269
00:28:48.483 --> 00:28:54.643
Inderjit Dhillon: Okay? So let's do again some manipulate algebra. So what we are going to do is


270
00:28:54.823 --> 00:28:57.593
Inderjit Dhillon: we'll see that it's actually gonna be easy to add.


271
00:28:58.253 --> 00:29:02.563
Hormoz Shahrzad: Ask, where where did W. 0 went?


272
00:29:03.513 --> 00:29:07.192
Inderjit Dhillon: Oh, so remember that I did this thing where I.


273
00:29:07.193 --> 00:29:08.142
Hormoz Shahrzad: Oh, yeah. Yeah.


274
00:29:08.783 --> 00:29:14.533
Inderjit Dhillon: I, basically, you know, put W. Naught, and w. 1 Tilde into w. 1.


275
00:29:14.533 --> 00:29:15.642
Hormoz Shahrzad: Right, right.


276
00:29:15.883 --> 00:29:22.012
Inderjit Dhillon: Right. So the x is d plus one dimension. So each wi.


277
00:29:22.583 --> 00:29:23.573
Hormoz Shahrzad: As the.


278
00:29:23.573 --> 00:29:32.562
Inderjit Dhillon: Has is our okay. And similarly, X belongs to our d plus one. It's just taking the. The.


279
00:29:32.983 --> 00:29:38.083
Inderjit Dhillon: The 1st feature or 1st component of x is one.


280
00:29:38.083 --> 00:29:39.472
Hormoz Shahrzad: The bias. Thank you.


281
00:29:39.473 --> 00:29:41.513
Inderjit Dhillon: And it accounts for the bias. Exactly.


282
00:29:42.353 --> 00:29:43.133
Inderjit Dhillon: Okay.


283
00:29:44.213 --> 00:29:49.713
Inderjit Dhillon: So if I now add up all these equations, one plus 2. And the reason I'm doing. It is


284
00:29:50.243 --> 00:29:54.153
Inderjit Dhillon: because if you see on the left hand side, right over here.


285
00:29:54.893 --> 00:30:04.403
Inderjit Dhillon: I have. p. 1, p. 2, pk. Minus one. And remember, we are modeling these as probabilities, so the sum of all the pi's will be one.


286
00:30:05.563 --> 00:30:08.193
Inderjit Dhillon: So if I add all these together.


287
00:30:08.423 --> 00:30:10.663
Inderjit Dhillon: since they have the same denominator.


288
00:30:11.023 --> 00:30:17.103
Inderjit Dhillon: I will get p. 1, plus p. 2, through pk, minus one, and then, of course, on the right hand side, I'll get something.


289
00:30:17.483 --> 00:30:22.502
Inderjit Dhillon: But then, since the sum of the probabilities ends up, adds up to be one.


290
00:30:22.633 --> 00:30:26.863
Inderjit Dhillon: I will end up getting this to be one minus Pk


291
00:30:27.183 --> 00:30:30.383
Inderjit Dhillon: divided by pk, okay, so let's let's do that.


292
00:30:30.643 --> 00:30:35.342
Inderjit Dhillon: Okay? Because that'll help us simplify things. So this implies that I'll get


293
00:30:36.083 --> 00:30:41.762
Inderjit Dhillon: p. 1 divided by Pk, plus p. 2, divided by Pk.


294
00:30:43.063 --> 00:30:48.903
Inderjit Dhillon: plus pk, minus one divided by Pk is equal to


295
00:30:49.553 --> 00:30:54.723
Inderjit Dhillon: E to the power. w. 1 transpose X. That's from here.


296
00:30:55.863 --> 00:31:02.002
Inderjit Dhillon: plus E to the power. W. 2. Transpose X, but from here.


297
00:31:02.183 --> 00:31:03.013
Inderjit Dhillon: Okay?


298
00:31:03.313 --> 00:31:14.233
Inderjit Dhillon: And then I'll get E to the power. WK, minus one. Transpose X, and this will be


299
00:31:14.383 --> 00:31:16.913
Inderjit Dhillon: p, 1, plus p. 2.


300
00:31:18.143 --> 00:31:24.923
Inderjit Dhillon: That's pk, minus one divided by Pk is equal to


301
00:31:25.773 --> 00:31:29.842
Inderjit Dhillon: summation of I equals one to K, minus one


302
00:31:30.303 --> 00:31:34.042
Inderjit Dhillon: E to the power. WI transpose. X,


303
00:31:36.593 --> 00:31:41.948
Inderjit Dhillon: okay. But, by the way, what I'm actually teaching you is like a sometimes the favorite


304
00:31:43.325 --> 00:31:47.333
Inderjit Dhillon: interview questions. If you interview, for


305
00:31:47.623 --> 00:31:51.902
Inderjit Dhillon: you know, machine learning position in in industry many times.


306
00:31:52.703 --> 00:32:01.452
Inderjit Dhillon: Okay, so if I look at the numerator, I have p. 1


307
00:32:01.913 --> 00:32:08.242
Inderjit Dhillon: to Pk, minus one, I'm adding them right? And okay, I


308
00:32:08.633 --> 00:32:10.823
Inderjit Dhillon: see there's something in the chat.


309
00:32:12.723 --> 00:32:16.032
Inderjit Dhillon: Oh, unless you should have asked this question before.


310
00:32:18.133 --> 00:32:26.162
Inderjit Dhillon: is there a specific set of lambda values that we should be training. Why don't we come to this later? Nilesh? Can you just remind me? Because I think this is about the homework?


311
00:32:31.733 --> 00:32:40.213
Inderjit Dhillon: Okay? So I get, this is one minus pk, divided by Pk is equal to


312
00:32:41.243 --> 00:32:47.743
Inderjit Dhillon: summation of, I equals one through k, minus one E to the power wi transpose. X,


313
00:32:48.793 --> 00:32:54.982
Inderjit Dhillon: okay. So I can take Pk to the other side. I can write it as one minus pk.


314
00:32:55.893 --> 00:33:00.773
Inderjit Dhillon: dk, times summation. I equals one k. Minus one


315
00:33:06.062 --> 00:33:09.252
Inderjit Dhillon: e to the power wi transpose X


316
00:33:09.743 --> 00:33:15.243
Inderjit Dhillon: one. Sorry I'm taking maybe more time than I should. But it's okay.


317
00:33:19.053 --> 00:33:22.792
Inderjit Dhillon: Just keep me honest. Make sure that I'm not making any mistakes.


318
00:33:22.903 --> 00:33:30.973
Inderjit Dhillon: Okay? So this means that pk is equal to one divided by one plus.


319
00:33:37.933 --> 00:33:38.843
Inderjit Dhillon: Okay.


320
00:33:40.763 --> 00:33:42.312
Inderjit Dhillon: And what is pi?


321
00:33:46.243 --> 00:33:48.602
Inderjit Dhillon: Let's see. Oh, here it is, right.


322
00:33:49.203 --> 00:33:53.323
Inderjit Dhillon: Pi divided by pk, is this right?


323
00:33:53.433 --> 00:33:57.722
Inderjit Dhillon: So I have pi divided by Pk


324
00:33:58.343 --> 00:34:02.923
Inderjit Dhillon: is E to the power wi transpose X.


325
00:34:05.863 --> 00:34:06.803
Inderjit Dhillon: Okay.


326
00:34:07.363 --> 00:34:17.712
Inderjit Dhillon: So since Pk has this form, by the way, I don't want to confuse you between I and I. So remember, this is a summation sign. So I'll just put it as J,


327
00:34:25.233 --> 00:34:29.563
Inderjit Dhillon: yeah, this means that pi is equal to


328
00:34:30.173 --> 00:34:34.163
Inderjit Dhillon: EWI transpose X, divided by


329
00:34:34.723 --> 00:34:40.183
Inderjit Dhillon: one plus summation of J equals one to K, minus one


330
00:34:40.543 --> 00:34:43.483
Inderjit Dhillon: E to the power. Wj transpose. X,


331
00:34:44.553 --> 00:34:51.563
Inderjit Dhillon: okay. And this is, for I is equal to 1, 2 through K minus one.


332
00:34:52.763 --> 00:34:58.563
Inderjit Dhillon: and you can confirm that the sum of all the pis will then be equal to one


333
00:35:01.163 --> 00:35:02.773
Inderjit Dhillon: any questions so far.


334
00:35:07.483 --> 00:35:10.919
Inderjit Dhillon: Okay, so remember, I'm just kind of really done


335
00:35:12.383 --> 00:35:23.082
Inderjit Dhillon: algebra right now to find out the form for Pk, and the form for Bi.


336
00:35:23.633 --> 00:35:26.802
Inderjit Dhillon: p. 1, p. 2, through K, minus one.


337
00:35:27.183 --> 00:35:30.383
Inderjit Dhillon: Okay. But remember that these are the parameters.


338
00:35:30.773 --> 00:35:37.013
Inderjit Dhillon: w. 1 w. 2 W. 3 WK. Minus one.


339
00:35:37.443 --> 00:35:41.893
Inderjit Dhillon: They are all parameters.


340
00:35:44.243 --> 00:35:51.693
Inderjit Dhillon: Okay? And then the question that we have is, how do we find them?


341
00:35:57.393 --> 00:35:58.243
Inderjit Dhillon: Okay?


342
00:35:59.003 --> 00:36:00.123
Inderjit Dhillon: So


343
00:36:00.593 --> 00:36:11.673
Inderjit Dhillon: the parameters are usually going to be found by something called maximum likelihood estimation. Okay, so we are going to look at now, the details.


344
00:36:12.153 --> 00:36:14.462
Inderjit Dhillon: Okay? So the parameters


345
00:36:18.093 --> 00:36:20.072
Inderjit Dhillon: in logistic regression.


346
00:36:25.323 --> 00:36:31.672
Inderjit Dhillon: Okay? And these are Wi, and remember how big are these


347
00:36:35.433 --> 00:36:41.583
Inderjit Dhillon: d plus one and how many. I equals one through.


348
00:36:43.583 --> 00:36:47.883
Inderjit Dhillon: Okay, okay?


349
00:36:48.223 --> 00:37:01.323
Inderjit Dhillon: And they are going to be and usually usually fit fit by maximum likelihood.


350
00:37:08.143 --> 00:37:18.913
Inderjit Dhillon: Okay? And we'll see. So this is kind of very similar to the again, the oh.


351
00:37:19.513 --> 00:37:25.633
Inderjit Dhillon: least regression case, you know, we could have actually shown that this world, if we had done probabilistic modeling.


352
00:37:26.280 --> 00:37:31.573
Inderjit Dhillon: We could have shown that the least squares problem arises from maximum likelihood.


353
00:37:31.743 --> 00:37:41.102
Inderjit Dhillon: Okay, so that's what we are going to use over here. Okay, so to simplify, to derive it, let me consider the simple case of a 2 class partner.


354
00:37:52.213 --> 00:37:54.473
Inderjit Dhillon: Okay? So in that case, I have


355
00:37:56.283 --> 00:38:04.313
Inderjit Dhillon: Oh, just y 1 and y 2, and I'll actually use a different encoding for wise. Okay, so


356
00:38:04.433 --> 00:38:07.503
Inderjit Dhillon: let's consider a do class problem. I have


357
00:38:08.353 --> 00:38:12.503
Inderjit Dhillon: need to figure out. PC, one given X, right?


358
00:38:12.683 --> 00:38:19.833
Inderjit Dhillon: And remember, that's what we were calling, p. 1. But


359
00:38:20.003 --> 00:38:23.322
Inderjit Dhillon: and and I have p. 2. So I have. p. 1 and p. 2.


360
00:38:23.583 --> 00:38:27.553
Inderjit Dhillon: But p. 2 is one minus p. 1. So let me just call this as P.


361
00:38:28.093 --> 00:38:33.213
Inderjit Dhillon: So I don't have to carry around this index. And P of C, 2, given X


362
00:38:33.333 --> 00:38:40.912
Inderjit Dhillon: is one minus B, okay, because obviously, blc, one. Given x


363
00:38:41.023 --> 00:38:45.422
Inderjit Dhillon: plus p of c, 2, given X, add up to one.


364
00:38:46.033 --> 00:38:49.593
Inderjit Dhillon: Okay, and do remember that. You know, we are modeling it.


365
00:38:50.103 --> 00:38:58.882
Inderjit Dhillon: So P is some. So it's actually a function of W,


366
00:39:00.233 --> 00:39:09.943
Inderjit Dhillon: okay, W, are my parameters. Okay? So I have training data. Xi, yi, here's training data.


367
00:39:20.243 --> 00:39:24.913
Inderjit Dhillon: Okay, I assume kind of it is, you know, as before, it's kind of Iid.


368
00:39:26.533 --> 00:39:31.053
Inderjit Dhillon: So here is the following encoding, I'm going to use this time. Okay.


369
00:39:31.193 --> 00:39:35.592
Inderjit Dhillon: is that let YI equals one.


370
00:39:36.153 --> 00:39:48.883
Inderjit Dhillon: When Xi belongs to c, 1 YI equals 0 when Xi belongs to C, 2,


371
00:39:49.933 --> 00:39:54.323
Inderjit Dhillon: okay, instead of using kind of indicator vectors.


372
00:39:54.443 --> 00:39:55.473
Inderjit Dhillon: That's what I'm doing.


373
00:39:55.643 --> 00:39:58.222
Inderjit Dhillon: So I'm going to try to figure out one. W,


374
00:39:58.943 --> 00:40:02.173
Inderjit Dhillon: okay, that either gives one or 0.


375
00:40:03.443 --> 00:40:06.742
Inderjit Dhillon: Okay, and what I want is that when


376
00:40:07.163 --> 00:40:13.663
Inderjit Dhillon: Xi belongs to class c. 1, i want the prediction to be close to one


377
00:40:14.103 --> 00:40:18.863
Inderjit Dhillon: when Xi belongs to C. 2 xi. Belongs to the training data and C 2.


378
00:40:18.973 --> 00:40:21.103
Inderjit Dhillon: Then I want it to belong to the


379
00:40:21.693 --> 00:40:24.323
Inderjit Dhillon: Yi to be close to 0.


380
00:40:24.493 --> 00:40:30.473
Inderjit Dhillon: Okay, so I can look at kind of the data likelihood.


381
00:40:36.693 --> 00:40:39.502
Inderjit Dhillon: Okay? And I'm basically going to try to maximize that.


382
00:40:39.663 --> 00:40:44.063
Inderjit Dhillon: Okay? And I can write that as let me just 1st write. It


383
00:40:44.903 --> 00:40:48.222
Inderjit Dhillon: is the product from I equal to one to N.


384
00:40:49.053 --> 00:40:51.533
Inderjit Dhillon: P. To the power. YI.


385
00:40:51.853 --> 00:40:57.852
Inderjit Dhillon: Times one minus P to the power one minus YI.


386
00:40:58.413 --> 00:41:04.822
Inderjit Dhillon: Okay? So so just to sort of understand what's going on. Remember, you have. XI. Yi.


387
00:41:05.623 --> 00:41:13.143
Inderjit Dhillon: If Xi belongs to Ci, you want your corresponding Yi equals one.


388
00:41:13.993 --> 00:41:21.292
Inderjit Dhillon: Okay, so what is this evaluate to what is this evaluate to when Xi belongs to? Ci.


389
00:41:22.593 --> 00:41:26.772
Inderjit Dhillon: right? So I have. P of YI.


390
00:41:28.213 --> 00:41:31.422
Inderjit Dhillon: One minus p times one minus yi.


391
00:41:32.733 --> 00:41:42.873
Inderjit Dhillon: So if YI equal to one, I get this equal to p raised to the power one


392
00:41:43.553 --> 00:41:45.563
Inderjit Dhillon: times one minus p.


393
00:41:46.153 --> 00:41:48.503
Inderjit Dhillon: Raised to the power one minus one.


394
00:41:49.323 --> 00:41:55.683
Inderjit Dhillon: So it's p times one minus p to the power 0. So that's equal to P


395
00:41:56.883 --> 00:41:59.333
Inderjit Dhillon: again, that's what you want, right.


396
00:42:02.153 --> 00:42:06.093
Inderjit Dhillon: And similarly, you'll see that when YI equal to 0,


397
00:42:06.443 --> 00:42:09.703
Inderjit Dhillon: then this will evaluate to one minus. P.


398
00:42:10.563 --> 00:42:17.663
Inderjit Dhillon: So the data likelihood is given by this is taking the product.


399
00:42:18.143 --> 00:42:19.023
Inderjit Dhillon: Okay?


400
00:42:19.153 --> 00:42:25.452
Inderjit Dhillon: But you know, we have exponentials. So it's better to look at log likelihood


401
00:42:29.203 --> 00:42:31.893
Inderjit Dhillon: lock like 3 hood.


402
00:42:32.853 --> 00:42:35.433
Inderjit Dhillon: Okay? Because log is a monotonic function.


403
00:42:36.983 --> 00:42:42.802
Inderjit Dhillon: Okay? So my blog likelihood is, I'm basically taking log


404
00:42:43.353 --> 00:43:00.423
Inderjit Dhillon: of this expression. And remember, this is a function of W, right? Don't forget that. Why this prediction is going to be, or P is going to be, a function of W, and that is what we need to find out. So we are going to try to maximize the log likelihood.


405
00:43:00.793 --> 00:43:03.633
Inderjit Dhillon: And by doing that we are going to try to.


406
00:43:03.963 --> 00:43:07.612
Inderjit Dhillon: we are going to be able to find the corresponding W,


407
00:43:10.513 --> 00:43:17.793
Inderjit Dhillon: okay, so if I take log of this quantity, right, I will get summation.


408
00:43:18.593 --> 00:43:29.053
Inderjit Dhillon: The product turns into a summation and I have log off BYI,


409
00:43:29.523 --> 00:43:36.233
Inderjit Dhillon: times one minus b 1 minus y, this is equal to


410
00:43:38.253 --> 00:43:52.983
Inderjit Dhillon: log of B of YI plus log of one minus b 1 minus. YR, okay.


411
00:43:54.383 --> 00:43:56.933
Inderjit Dhillon: this is, yeah, this is fine.


412
00:43:57.083 --> 00:44:06.283
Inderjit Dhillon: And then this is equal to I equals one to NYI log off B


413
00:44:06.873 --> 00:44:12.843
Inderjit Dhillon: plus one minus YI log of one minus B,


414
00:44:14.533 --> 00:44:21.742
Inderjit Dhillon: okay. So when I was saying that this is A, you know, like a favorite interview question sometimes might some someone might just say, Hey.


415
00:44:21.943 --> 00:44:26.382
Inderjit Dhillon: go to the whiteboard, and can you derive the log likelihood for logistic version?


416
00:44:28.693 --> 00:44:30.523
Inderjit Dhillon: Okay, so this is what you would do.


417
00:44:30.633 --> 00:44:35.913
Inderjit Dhillon: And then, of course, how would you find the parameters right? So you want to try to maximize


418
00:44:37.873 --> 00:44:41.153
Inderjit Dhillon: the likelihood of the data or the log likelihood?


419
00:44:45.883 --> 00:44:49.632
Inderjit Dhillon: Okay, so remember that this is a function of


420
00:44:49.803 --> 00:44:52.613
Inderjit Dhillon: W, right? Because P is a function of W.


421
00:44:53.453 --> 00:44:59.293
Inderjit Dhillon: So my likelihood log likelihood is here.


422
00:44:59.633 --> 00:45:03.243
Inderjit Dhillon: And I'm going to try to maximize overall that.


423
00:45:04.063 --> 00:45:08.472
Inderjit Dhillon: Yeah. So maximum over W.


424
00:45:09.063 --> 00:45:16.982
Inderjit Dhillon: And it is summation of, I equals one through. And I'm just copying what's written above


425
00:45:17.683 --> 00:45:28.223
Inderjit Dhillon: YI log of b plus one minus YI log off one minus. P,


426
00:45:30.143 --> 00:45:34.672
Inderjit Dhillon: okay. And this is all within the summation side.


427
00:45:34.923 --> 00:45:38.482
Inderjit Dhillon: So this is what I want to try to maximize.


428
00:45:39.723 --> 00:45:42.072
Inderjit Dhillon: And remember, I have a W over here.


429
00:45:43.113 --> 00:45:48.663
Inderjit Dhillon: And this is because P is a function of, don't it?


430
00:45:49.183 --> 00:45:53.413
Inderjit Dhillon: Okay? So why don't we now write it out? What the function, what we had.


431
00:45:53.603 --> 00:45:54.973
Inderjit Dhillon: If you go back.


432
00:45:58.233 --> 00:45:59.932
Inderjit Dhillon: Okay, we had.


433
00:46:00.403 --> 00:46:01.642
Inderjit Dhillon: BI,


434
00:46:02.683 --> 00:46:13.103
Inderjit Dhillon: okay. So in this case, we have just p 1. So P is E to the power. W transpose X divided by one plus. W transpose. X,


435
00:46:14.493 --> 00:46:17.072
Inderjit Dhillon: okay? Because we are only considering a 2 class problem.


436
00:46:18.203 --> 00:46:22.493
Inderjit Dhillon: Okay? And we've encoded in such a way that we just want one W,


437
00:46:23.853 --> 00:46:25.493
Inderjit Dhillon: because it's a 2 class problem.


438
00:46:25.873 --> 00:46:38.873
Inderjit Dhillon: So P of W is E to the power. W transpose. Xi, okay.


439
00:46:41.353 --> 00:46:46.163
Inderjit Dhillon: one plus E to the power. W transpose xi.


440
00:46:53.843 --> 00:47:02.062
Inderjit Dhillon: and one minus b is one minus E to the power. W transpose Xi.


441
00:47:02.603 --> 00:47:08.192
Inderjit Dhillon: Divided by one plus E to the power. W transpose xi, and that's equal to


442
00:47:08.933 --> 00:47:14.652
Inderjit Dhillon: one divided by one plus E to the power. W transpose. Xi.


443
00:47:15.053 --> 00:47:18.303
Inderjit Dhillon: So this means that log of P.


444
00:47:20.733 --> 00:47:31.053
Inderjit Dhillon: Okay. If I take the logarithm of this quantity, I get W transpose Xi minus log off


445
00:47:31.833 --> 00:47:35.193
Inderjit Dhillon: one plus E to the power. W transpose X,


446
00:47:38.323 --> 00:47:46.892
Inderjit Dhillon: and then, if I take log of one minus b, that becomes minus log of


447
00:47:47.683 --> 00:47:51.202
Inderjit Dhillon: one plus E to the power. W transpose X.


448
00:48:09.843 --> 00:48:11.383
Inderjit Dhillon: Any questions.


449
00:48:25.183 --> 00:48:25.993
Inderjit Dhillon: Okay?


450
00:48:26.883 --> 00:48:30.693
Inderjit Dhillon: And then again, I want to maximize.


451
00:48:32.973 --> 00:48:36.893
Inderjit Dhillon: Okay, and this will turn out to be like a concave function.


452
00:48:37.513 --> 00:48:44.983
Inderjit Dhillon: Right? So I can set the gradient equal to 0.


453
00:48:45.203 --> 00:48:48.532
Inderjit Dhillon: And it's a convey concave function. So if I try to


454
00:48:48.643 --> 00:48:54.352
Inderjit Dhillon: maximize it, I need to set the gradient to be equal to 0.


455
00:48:57.363 --> 00:49:00.772
Inderjit Dhillon: Okay, now, if you think about the gradient of this


456
00:49:02.193 --> 00:49:05.772
Inderjit Dhillon: gradient of LW, remember, LW. Is here.


457
00:49:06.313 --> 00:49:10.592
Inderjit Dhillon: So it'll basically involve gradient of W. Log of P


458
00:49:11.403 --> 00:49:14.933
Inderjit Dhillon: and gradient of W. Log of one minus. P.


459
00:49:16.283 --> 00:49:24.403
Inderjit Dhillon: Okay, so this, the gradient will involve


460
00:49:28.323 --> 00:49:34.362
Inderjit Dhillon: log of P and the gradient of log of


461
00:49:34.693 --> 00:49:38.253
Inderjit Dhillon: one minus p. Right? There's 1 log of one minus P.


462
00:49:38.853 --> 00:49:41.253
Inderjit Dhillon: So let's 1st kind of look at that.


463
00:49:41.923 --> 00:49:45.933
Inderjit Dhillon: So log of P is given here.


464
00:49:46.803 --> 00:49:52.803
Inderjit Dhillon: and if I differentiate this with respect to W. Remember, I'll get just Xi over here.


465
00:49:53.113 --> 00:49:57.813
Inderjit Dhillon: And then this is log. So I'll get one divided by this quantity.


466
00:49:58.043 --> 00:50:05.893
Inderjit Dhillon: And then I have. Xi, okay, so it'll be of this form.


467
00:50:09.453 --> 00:50:24.813
Inderjit Dhillon: Okay? So if I have this, okay, so just look at over here, and the gradient is Xi minus


468
00:50:26.413 --> 00:50:33.393
Inderjit Dhillon: log. I take differentiate it. I get one plus E to the power. W transpose. Xi.


469
00:50:35.343 --> 00:50:36.313
Inderjit Dhillon: Okay.


470
00:50:36.443 --> 00:50:47.863
Inderjit Dhillon: Times, well, I have E to the power. W transpose. Xi times Xi.


471
00:50:50.543 --> 00:50:55.082
Inderjit Dhillon: Okay. If I take the gradient of this with respect to W.


472
00:50:56.113 --> 00:51:02.193
Inderjit Dhillon: Okay. And I can write this as Xi times


473
00:51:02.953 --> 00:51:12.322
Inderjit Dhillon: one minus E to the power W transpose. Xi. Divided by one plus e transpose.


474
00:51:17.753 --> 00:51:23.033
Inderjit Dhillon: And then I also need to. If I need to take the gradient of the log likelihood.


475
00:51:23.323 --> 00:51:27.392
Inderjit Dhillon: I'll need to take the gradient of log of one minus p.


476
00:51:27.823 --> 00:51:30.543
Inderjit Dhillon: Right? And so if I look at


477
00:51:30.753 --> 00:51:37.143
Inderjit Dhillon: log of one minus P, it's given over here, it's actually a little simpler, right? It doesn't have this


478
00:51:37.433 --> 00:51:40.642
Inderjit Dhillon: term. And it has the identical term over here.


479
00:51:41.083 --> 00:51:45.073
Inderjit Dhillon: Okay, so it basically just becomes


480
00:51:52.203 --> 00:51:55.522
Inderjit Dhillon: the second dorm over here, right this term.


481
00:51:56.383 --> 00:52:07.913
Inderjit Dhillon: Okay, so it becomes minus E to the power. W transpose Xi divided by one plus 5 times.


482
00:52:13.273 --> 00:52:16.013
Inderjit Dhillon: Okay. So now let's put these things together.


483
00:52:17.083 --> 00:52:22.903
Inderjit Dhillon: Okay, so remember, this is the likelihood log likelihood.


484
00:52:23.643 --> 00:52:30.763
Inderjit Dhillon: Okay. So if I now look at gradient WL. Of W.


485
00:52:32.863 --> 00:52:40.292
Inderjit Dhillon: That is equal to well, I have the summation. I equals one through N.


486
00:52:41.153 --> 00:52:44.013
Inderjit Dhillon: I have YI.


487
00:52:44.743 --> 00:52:53.733
Inderjit Dhillon: Gradient. With respect to W. Of log of p plus one minus YI.


488
00:52:54.433 --> 00:52:55.783
Inderjit Dhillon: Gradient.


489
00:52:56.203 --> 00:53:00.642
Inderjit Dhillon: With respect to W. Of log of one minus P.


490
00:53:04.533 --> 00:53:05.463
Inderjit Dhillon: Okay?


491
00:53:05.913 --> 00:53:14.583
Inderjit Dhillon: And then let me just copy things from above. I equals one to NYII have. This


492
00:53:17.213 --> 00:53:18.903
Inderjit Dhillon: is equal to this.


493
00:53:19.713 --> 00:53:27.283
Inderjit Dhillon: So I copy that lighted here is Xi


494
00:53:27.443 --> 00:53:30.622
Inderjit Dhillon: times one minus e to the power


495
00:53:31.243 --> 00:53:35.913
Inderjit Dhillon: W transpose xi. Divided by one plus.


496
00:53:40.643 --> 00:53:47.203
Inderjit Dhillon: and then I have the remaining term, which is plus one minus yi.


497
00:53:51.593 --> 00:54:00.072
Inderjit Dhillon: and then I have gradient of log, one minus p, that I get from here.


498
00:54:01.913 --> 00:54:04.192
Inderjit Dhillon: and so I substitute that over here.


499
00:54:04.773 --> 00:54:07.553
Inderjit Dhillon: So I have this.


500
00:54:08.753 --> 00:54:18.862
Inderjit Dhillon: I have this, and I substitute that over here, so it becomes minus e to the power


501
00:54:19.603 --> 00:54:26.913
Inderjit Dhillon: W transpose Xi. Divided by one plus E to the power. W transpose. Xi.


502
00:54:31.533 --> 00:54:35.322
Inderjit Dhillon: Okay, let's see if we can expand it out.


503
00:54:36.313 --> 00:54:38.473
Inderjit Dhillon: Y equals one through M.


504
00:54:40.393 --> 00:54:46.343
Inderjit Dhillon: Okay. Well, oh, I did miss something right? I did miss this. Xi.


505
00:54:47.683 --> 00:54:52.653
Inderjit Dhillon: Okay, remember that there's this Xi. I noted it down here. I forgot it over here.


506
00:54:53.363 --> 00:54:55.463
Inderjit Dhillon: So it is. Xi.


507
00:54:56.953 --> 00:55:01.893
Inderjit Dhillon: So let me take the Xi common. So remember, Xi is a vector.


508
00:55:02.543 --> 00:55:05.433
Inderjit Dhillon: Okay, and the rest of the quantities are scalars.


509
00:55:06.523 --> 00:55:09.193
Inderjit Dhillon: Right? So I have YI


510
00:55:10.333 --> 00:55:16.283
Inderjit Dhillon: let me expand this out. Yi times one minus yi times this


511
00:55:28.143 --> 00:55:31.813
Inderjit Dhillon: one plus e to the power W transpose. Xi.


512
00:55:33.833 --> 00:55:43.073
Inderjit Dhillon: And then, when I do this, I get minus of this quantity minus of


513
00:55:43.903 --> 00:55:49.492
Inderjit Dhillon: E to the power W transpose. Xi. So just keep me honest. I


514
00:55:50.143 --> 00:55:52.762
Inderjit Dhillon: very possible that I might make a mistake.


515
00:55:53.843 --> 00:55:57.883
Inderjit Dhillon: and then I have this minus yi and minus here.


516
00:55:58.113 --> 00:56:00.833
Inderjit Dhillon: So it gives me plus YI.


517
00:56:01.373 --> 00:56:04.623
Inderjit Dhillon: E. To the power W transpose Xi.


518
00:56:05.543 --> 00:56:09.472
Inderjit Dhillon: Divided by one plus E to the part W. Transpose. Xi.


519
00:56:12.803 --> 00:56:15.242
Inderjit Dhillon: No. But look at this term this term.


520
00:56:16.343 --> 00:56:20.003
Inderjit Dhillon: It's identical to this, except it's opposite in sign.


521
00:56:20.613 --> 00:56:22.543
Inderjit Dhillon: so I can cancel this out.


522
00:56:23.613 --> 00:56:25.693
Inderjit Dhillon: So I get that


523
00:56:28.723 --> 00:56:42.273
Inderjit Dhillon: gradient of the log. Likelihood is somewhat simplified, and it is xi times YI. Minus


524
00:56:43.023 --> 00:56:46.333
Inderjit Dhillon: e. To the power W transpose xi.


525
00:56:47.273 --> 00:56:50.213
Inderjit Dhillon: One plus e. To the power W. Transpose x.


526
00:56:53.753 --> 00:56:57.303
Inderjit Dhillon: Oh, and look, remember what this is. Have we seen this before?


527
00:56:58.343 --> 00:57:04.123
Inderjit Dhillon: E. To the power W. Transpose. Xi. Divided by one plus E to the power W. Transpose. Xi.


528
00:57:04.603 --> 00:57:06.012
Inderjit Dhillon: Seems like it, right.


529
00:57:09.173 --> 00:57:09.963
Hormoz Shahrzad: It was.


530
00:57:13.243 --> 00:57:15.103
Inderjit Dhillon: Here. Right? It's this.


531
00:57:15.993 --> 00:57:20.663
Inderjit Dhillon: Oh, it's the probability of see one. You have an X.


532
00:57:39.943 --> 00:57:40.733
Inderjit Dhillon: Okay?


533
00:57:41.263 --> 00:57:43.823
Inderjit Dhillon: So you can see right? For example, if


534
00:57:45.263 --> 00:57:47.023
Inderjit Dhillon: Now XI.


535
00:57:48.243 --> 00:57:51.822
Inderjit Dhillon: If Xi belongs to the 1st class


536
00:57:54.823 --> 00:58:00.552
Inderjit Dhillon: class c. 1, then YI is going to be one.


537
00:58:02.323 --> 00:58:06.893
Inderjit Dhillon: and this is P of c, 1 given X.


538
00:58:07.453 --> 00:58:08.813
Inderjit Dhillon: This is the prediction.


539
00:58:11.523 --> 00:58:14.043
Inderjit Dhillon: If I'm close to one, it'll match.


540
00:58:14.933 --> 00:58:19.083
Inderjit Dhillon: If I'm far away from one right, it will not match.


541
00:58:20.803 --> 00:58:24.193
Inderjit Dhillon: So then I basically will change


542
00:58:24.413 --> 00:58:27.742
Inderjit Dhillon: my w depending upon the gradient


543
00:58:28.123 --> 00:58:30.992
Inderjit Dhillon: right? And I want this to be actually 0, right?


544
00:58:31.543 --> 00:58:41.863
Inderjit Dhillon: So to solve, we'll find W. That maximizes


545
00:58:43.653 --> 00:58:51.302
Inderjit Dhillon: L of W. And I'm not going to go through the all the details. But you can show that law L. Of W. Is a concave function.


546
00:58:51.583 --> 00:58:58.002
Inderjit Dhillon: So if I set this to be equal to 0. Okay.


547
00:58:59.423 --> 00:59:02.702
Inderjit Dhillon: that gives the maximum likelihood solution.


548
00:59:05.773 --> 00:59:08.412
Inderjit Dhillon: So I, what I need to do is


549
00:59:11.553 --> 00:59:15.152
Inderjit Dhillon: summation of I equals one to N.


550
00:59:15.813 --> 00:59:21.983
Inderjit Dhillon: Xi, yi, minus e to the power W. Transpose xi.


551
00:59:22.463 --> 00:59:26.173
Inderjit Dhillon: One plus E to the power W. Transpose Xi.


552
00:59:26.783 --> 00:59:28.103
Inderjit Dhillon: Equals 0.


553
00:59:33.513 --> 00:59:36.153
Inderjit Dhillon: But one other thing that you should notice is.


554
00:59:36.843 --> 00:59:46.722
Inderjit Dhillon: you know, remember that Xi is dimension. How much is Rd plus one right.


555
00:59:46.723 --> 00:59:47.823
Hormoz Shahrzad: That's fun. Yeah.


556
00:59:52.173 --> 00:59:56.953
Inderjit Dhillon: And W belongs to Rd plus one.


557
00:59:58.053 --> 01:00:00.053
Inderjit Dhillon: So how many equations are these?


558
01:00:05.223 --> 01:00:07.323
Inderjit Dhillon: These are d plus one equations, right.


559
01:00:07.323 --> 01:00:07.883
Hormoz Shahrzad: Yep.


560
01:00:09.503 --> 01:00:12.583
Inderjit Dhillon: And how many unknowns do I have d plus one?


561
01:00:14.673 --> 01:00:21.953
Inderjit Dhillon: Okay, so I have basically d plus one parameters.


562
01:00:34.023 --> 01:00:38.773
Inderjit Dhillon: And I have d plus one any questions.


563
01:00:40.553 --> 01:00:46.193
Inderjit Dhillon: Sense Xi belongs to RD plus one


564
01:00:49.703 --> 01:01:01.619
Inderjit Dhillon: in linear regression. What happened? Do you remember? We had also kind of Xi right? We had this.


565
01:01:03.943 --> 01:01:07.489
Inderjit Dhillon: if you if you really looked at the the


566
01:01:08.163 --> 01:01:11.713
Inderjit Dhillon: linear Regression case also, right? With least squares regression.


567
01:01:11.893 --> 01:01:19.313
Inderjit Dhillon: you would have noticed that there was basically an Xi times, y, minus X transpose W.


568
01:01:19.683 --> 01:01:22.053
Inderjit Dhillon: There was none of this E to the power stuff.


569
01:01:25.863 --> 01:01:32.483
Inderjit Dhillon: So it was basically a linear equation in a W. And it actually had this form. That was Xi.


570
01:01:32.593 --> 01:01:38.512
Inderjit Dhillon: Times like the in a little way. This is kind of the residual


571
01:01:39.233 --> 01:01:44.573
Inderjit Dhillon: or the difference from the actual value to the predicted value.


572
01:01:45.023 --> 01:01:51.842
Inderjit Dhillon: So you're basically essentially getting a very, very similar thing over here, except computationally, it is much more different


573
01:01:52.903 --> 01:01:58.753
Inderjit Dhillon: right over here. You have this exponential or a ratio of exponentials.


574
01:01:59.233 --> 01:02:05.232
Inderjit Dhillon: So the problem over here is that these are nonlinear equations.


575
01:02:13.133 --> 01:02:15.963
Inderjit Dhillon: So in the least squares case


576
01:02:17.203 --> 01:02:20.363
Inderjit Dhillon: where we did not have this exponential.


577
01:02:21.333 --> 01:02:25.363
Inderjit Dhillon: We could actually just formulate this as a system of linear equations


578
01:02:27.123 --> 01:02:29.172
Inderjit Dhillon: by setting the gradient equal to 0.


579
01:02:30.703 --> 01:02:35.713
Inderjit Dhillon: Here it is a system of nonlinear equations system of linear equations.


580
01:02:35.833 --> 01:02:44.453
Inderjit Dhillon: We actually had a closed form solution given by the normal equations, and you could express it in the inverse in terms of the inverse of X transpose X.


581
01:02:45.133 --> 01:02:45.993
Inderjit Dhillon: Okay.


582
01:02:46.913 --> 01:02:49.413
Inderjit Dhillon: But here there is no closed form solution.


583
01:03:05.163 --> 01:03:09.773
Inderjit Dhillon: And this is the most common thing that happens in machine learning. Right?


584
01:03:09.883 --> 01:03:15.752
Inderjit Dhillon: Generally, there are no closed form solutions, least squares. Linear regression is just a very special case


585
01:03:15.933 --> 01:03:18.013
Inderjit Dhillon: where you can get a closed form solution.


586
01:03:19.693 --> 01:03:23.462
Inderjit Dhillon: Okay, so there is no closed form solution. So how do we go about solving it?


587
01:03:28.783 --> 01:03:33.642
Inderjit Dhillon: How do we solve for double


588
01:03:35.983 --> 01:03:38.032
Inderjit Dhillon: alright? So we need to solve this.


589
01:03:44.523 --> 01:03:45.403
Inderjit Dhillon: Okay?


590
01:03:45.703 --> 01:03:49.833
Inderjit Dhillon: So we now really need to use some optimization methods.


591
01:03:52.553 --> 01:03:56.423
Inderjit Dhillon: So one thing I also want to specify stress is that, yes.


592
01:03:56.968 --> 01:04:02.672
Inderjit Dhillon: when we do least squares. When we do linear regression, we get a closed form solution.


593
01:04:03.843 --> 01:04:18.642
Inderjit Dhillon: There exists a closed form solution, and you can use that to solve for linear regression. But even there you could actually have used some iterative optimization methods to solve it. Here. It is a necessity. Okay, so we need to use


594
01:04:20.293 --> 01:04:36.113
Inderjit Dhillon: optimization methods to solve for leaves parameters.


595
01:04:38.323 --> 01:04:39.203
Inderjit Dhillon: Okay?


596
01:04:40.153 --> 01:04:45.042
Inderjit Dhillon: And what are these optimization methods? Well, you know classically, you have


597
01:04:45.593 --> 01:04:49.563
Inderjit Dhillon: things that you might have heard about gradient descent.


598
01:04:49.923 --> 01:04:52.772
Inderjit Dhillon: We will talk about these methods more in class.


599
01:04:53.633 --> 01:04:56.383
Inderjit Dhillon: Okay? Or we can have gradient ascent.


600
01:04:57.603 --> 01:05:00.302
Inderjit Dhillon: Okay? Well, gradient descent. When we are


601
01:05:05.053 --> 01:05:07.722
Inderjit Dhillon: when we are solving a minimization problem.


602
01:05:08.093 --> 01:05:11.412
Inderjit Dhillon: And we have a convex function, gradient descent.


603
01:05:11.873 --> 01:05:15.242
Inderjit Dhillon: For example, here we have a maximization problem.


604
01:05:15.593 --> 01:05:23.953
Inderjit Dhillon: But the function is concave. Right? Or you could have solved. You know, the negative of this


605
01:05:24.733 --> 01:05:31.183
Inderjit Dhillon: right. And you would have gotten you basically would try to be minimizing the negative of this log like pivot.


606
01:05:31.613 --> 01:05:34.063
Inderjit Dhillon: And then you would look for a minimum.


607
01:05:35.153 --> 01:05:39.822
Inderjit Dhillon: Okay, if you look at like optimization books, there are more sophisticated methods.


608
01:05:40.383 --> 01:05:44.763
Inderjit Dhillon: Second order methods. For example, Newton's method.


609
01:05:45.763 --> 01:05:46.703
Inderjit Dhillon: Okay?


610
01:05:49.173 --> 01:05:54.373
Inderjit Dhillon: And these are various ways to solve it. Right? And for


611
01:05:54.743 --> 01:06:08.112
Inderjit Dhillon: specific cases, you can actually develop specific solutions. So for logistic regression you have, you know, some. You can actually solve it using an iterative, least squares solver.


612
01:06:08.513 --> 01:06:14.762
Inderjit Dhillon: Okay, you can actually also use Newton's method over here and adapt it so that it is fast.


613
01:06:15.653 --> 01:06:24.562
Inderjit Dhillon: One thing that happens is that in modern machine learning problems, N can be very large.


614
01:06:26.313 --> 01:06:31.032
Inderjit Dhillon: Okay, so typical methods like gradient descent.


615
01:06:31.313 --> 01:06:36.432
Inderjit Dhillon: What they will do is they will have some Wi or Wj


616
01:06:36.993 --> 01:06:42.512
Inderjit Dhillon: and they will basically go along the direction of the negative of the gradient


617
01:06:42.713 --> 01:06:45.082
Inderjit Dhillon: and have an update rule of the form.


618
01:06:45.363 --> 01:06:55.913
Inderjit Dhillon: You know, Wj, plus, one is Wj, minus, some step size times the gradient of


619
01:06:57.683 --> 01:06:59.683
Inderjit Dhillon: okay. So this is gradient descent.


620
01:07:05.623 --> 01:07:14.013
Inderjit Dhillon: Okay? And Newton's method is, you know, Wj, plus one is Wj, minus eta.


621
01:07:14.173 --> 01:07:16.783
Inderjit Dhillon: And then they have something called the Hessian.


622
01:07:17.693 --> 01:07:24.523
Inderjit Dhillon: Okay, off this in worse times oops.


623
01:07:31.523 --> 01:07:37.763
Inderjit Dhillon: Okay, let's forget about all these. But one common problem over here is


624
01:07:38.092 --> 01:07:40.162
Inderjit Dhillon: that both of them require the gradient.


625
01:07:42.103 --> 01:07:46.472
Inderjit Dhillon: And what is the gradient to evaluate the entire gradient.


626
01:07:47.753 --> 01:07:49.563
Inderjit Dhillon: It's actually quite expensive, right?


627
01:07:49.813 --> 01:07:52.953
Inderjit Dhillon: If capital N is very large.


628
01:07:55.113 --> 01:08:00.053
Inderjit Dhillon: So the drawback with gradient descent and methods like that is


629
01:08:00.243 --> 01:08:04.423
Inderjit Dhillon: that they at each step requires order and computation.


630
01:08:08.153 --> 01:08:09.353
Inderjit Dhillon: The drawback


631
01:08:12.773 --> 01:08:14.543
Inderjit Dhillon: of these methods.


632
01:08:18.123 --> 01:08:24.142
Inderjit Dhillon: And when I said these methods, I mean gradient descent.


633
01:08:26.793 --> 01:08:40.583
Inderjit Dhillon: Newtons is that each step requires essentially the entire training data


634
01:08:41.833 --> 01:08:47.142
Inderjit Dhillon: order and computation. And you make 0 progress until you do this.


635
01:08:48.033 --> 01:08:50.343
Inderjit Dhillon: Okay, so this is not feasible


636
01:08:54.093 --> 01:08:56.932
Inderjit Dhillon: when N is very large.


637
01:09:01.703 --> 01:09:02.663
Inderjit Dhillon: Okay?


638
01:09:02.893 --> 01:09:11.353
Inderjit Dhillon: So as a result, people use methods like stochastic, brilliant.


639
01:09:12.903 --> 01:09:13.873
Inderjit Dhillon: Listen.


640
01:09:18.013 --> 01:09:27.166
Inderjit Dhillon: okay. And sometimes this is called Std, and what Std does, basically, it gets


641
01:09:28.723 --> 01:09:33.063
Inderjit Dhillon: an estimate of the gradient by


642
01:09:33.423 --> 01:09:46.373
Inderjit Dhillon: just looking at a few points, or maybe just even 1 point. Classical, stochastic, gradient descent method is just looking at one training data. Or you can start looking at a subset of the training data like what's called a mini batch.


643
01:09:47.403 --> 01:09:53.623
Inderjit Dhillon: Okay? So you don't. Basically the competition then either just becomes order. D,


644
01:09:53.793 --> 01:09:58.613
Inderjit Dhillon: where d is a dimension or B times d, where B is the bad size.


645
01:09:59.363 --> 01:10:02.273
Inderjit Dhillon: Okay? And typically, these are the methods that are used.


646
01:10:02.383 --> 01:10:06.022
Inderjit Dhillon: And these are also the methods that now power the


647
01:10:06.867 --> 01:10:08.843
Inderjit Dhillon: deep, neural, neural neural networks.


648
01:10:09.503 --> 01:10:19.783
Inderjit Dhillon: Okay, so we will look in more detail at optimization methods, especially things like stochastic gradient descend.


649
01:10:19.933 --> 01:10:21.552
Inderjit Dhillon: or other


650
01:10:21.983 --> 01:10:39.353
Inderjit Dhillon: methods that you might have heard of like atom, and so on, that in addition, use some momentum and some orders of a second order method, because they do basically curve per coordinate scaling that can be thought of as a diagonal approximation.


651
01:10:40.121 --> 01:10:42.393
Inderjit Dhillon: To a to offer Newton's method.


652
01:10:42.593 --> 01:10:53.412
Inderjit Dhillon: Okay? Now, you know, classically, if you solve it, using, you know, either gradient descent or stochastic gradient descent. You also, as in linear regression.


653
01:10:53.943 --> 01:10:56.162
Inderjit Dhillon: You may want to add regularization.


654
01:10:56.353 --> 01:11:04.863
Inderjit Dhillon: Okay, so you may do to norm or 1 9.


655
01:11:05.143 --> 01:11:14.492
Inderjit Dhillon: Okay. So in practice, this, some sort of regularization should be used.


656
01:11:19.583 --> 01:11:24.563
Inderjit Dhillon: Okay, so that kind of concludes today's lecture, where you know, we


657
01:11:27.333 --> 01:11:34.902
Inderjit Dhillon: looked at logistic regression. And we said, if we are just going to directly find the parameters forget about modeling the covariance matrix.


658
01:11:35.243 --> 01:11:38.513
Inderjit Dhillon: But by maximizing the data likelihood.


659
01:11:38.923 --> 01:11:44.893
Inderjit Dhillon: I'm going to try and get the parameters. And we saw by doing some analysis


660
01:11:45.133 --> 01:11:49.443
Inderjit Dhillon: that that leads to a system of nonlinear equations


661
01:11:50.033 --> 01:11:58.673
Inderjit Dhillon: d plus one nonlinear equations in d plus one unknowns. And then you can basically try to solve it using an optimization method.


662
01:12:00.583 --> 01:12:03.123
Inderjit Dhillon: Okay, so any questions.


663
01:12:05.475 --> 01:12:07.123
Vishal Thyagarajan: I have a question.


664
01:12:07.123 --> 01:12:08.142
Inderjit Dhillon: Yes, we shall go ahead.


665
01:12:08.143 --> 01:12:16.020
Vishal Thyagarajan: So I have 2 questions. So firstly, will gradient descent be guaranteed to find


666
01:12:16.903 --> 01:12:24.312
Vishal Thyagarajan: will will it be guaranteed to find the global maximum, since, like you said, like the log function was convex. Right? So.


667
01:12:24.543 --> 01:12:35.123
Inderjit Dhillon: Sorry the it's not. I mean, the log function is monotonic. But in this case you can show that the log likelihood for a logistic regression is a concave function.


668
01:12:36.443 --> 01:12:37.453
Vishal Thyagarajan: So when we.


669
01:12:37.453 --> 01:12:45.792
Inderjit Dhillon: Take the, you can take the second derivative, and you can show that that has that property, that the Hessian is negative, definite.


670
01:12:46.843 --> 01:12:53.552
Vishal Thyagarajan: So because it's concave. If we use gradient descent like is, there is like guaranteed to find the maximum.


671
01:12:53.773 --> 01:12:58.482
Inderjit Dhillon: Yes, so so sorry. Just to keep the notation correct.


672
01:12:58.643 --> 01:13:04.673
Inderjit Dhillon: If you have a concave function and you want to find its maximum, you use gradient ascent.


673
01:13:05.543 --> 01:13:10.143
Inderjit Dhillon: and that'll under certain conditions, get you the global maximum.


674
01:13:10.313 --> 01:13:14.243
Inderjit Dhillon: You could equivalently take the negative of the log likelihood.


675
01:13:14.843 --> 01:13:25.703
Inderjit Dhillon: and you can then show that that function is convex, and you can try to find the minimum of that. And gradient descent will find you the minimum global minimum.


676
01:13:26.093 --> 01:13:26.793
Vishal Thyagarajan: Right.


677
01:13:27.143 --> 01:13:32.402
Vishal Thyagarajan: And then one more question. At the beginning we assume that all of the


678
01:13:32.613 --> 01:13:35.422
Vishal Thyagarajan: we assume that the public, like the weights, represent


679
01:13:36.256 --> 01:13:40.243
Vishal Thyagarajan: like, like certain terms in like a multivariate caution.


680
01:13:40.623 --> 01:13:44.998
Vishal Thyagarajan: So when we, when we plug those weights back into the Gaussian


681
01:13:45.823 --> 01:13:52.112
Vishal Thyagarajan: like, will will it still, and we like find the covariance like W. Will those 2 terms be equal to each other? So like, if we find, like.


682
01:13:52.113 --> 01:13:54.772
Inderjit Dhillon: No, no, so sorry, so just to make it clear.


683
01:13:55.103 --> 01:13:57.493
Inderjit Dhillon: All I was showing over here is


684
01:13:57.833 --> 01:14:02.913
Inderjit Dhillon: that when we had modeled each class as a Gaussian with covariant Sigma.


685
01:14:03.313 --> 01:14:08.662
Inderjit Dhillon: Then we actually had gotten that the log odds of the probabilities


686
01:14:09.053 --> 01:14:15.453
Inderjit Dhillon: were of this form, W naught plus w transpose X. This is when the covariances of the 2 classes are equal.


687
01:14:16.793 --> 01:14:21.813
Inderjit Dhillon: All wh. What that was saying is that that was just motivating us


688
01:14:22.123 --> 01:14:26.423
Inderjit Dhillon: to model the log probabilities as a linear function.


689
01:14:27.863 --> 01:14:28.813
Inderjit Dhillon: So it was.


690
01:14:28.813 --> 01:14:37.593
Inderjit Dhillon: And then logistic regression tries to find these parameters right through maximizing the data likelihood


691
01:14:37.943 --> 01:14:48.622
Inderjit Dhillon: and does not try to estimate the covariance of each. So so in logistic regression there is no assumption that the class has a Gaussian distribution.


692
01:14:50.103 --> 01:14:50.893
Vishal Thyagarajan: Right. Got it?


693
01:14:50.893 --> 01:14:53.742
Inderjit Dhillon: Okay. So when you assume that the Gaussian has.


694
01:14:53.993 --> 01:15:00.693
Inderjit Dhillon: if you do this modeling as a Gaussian with covariance, Sigma, you basically 1st need to estimate Sigma


695
01:15:00.993 --> 01:15:06.632
Inderjit Dhillon: once you estimate Sigma, the answer is there for WW. And W. Naught.


696
01:15:06.823 --> 01:15:14.270
Inderjit Dhillon: But it turns out it's actually not a very good method, because in practice a lot of problems.


697
01:15:15.073 --> 01:15:18.942
Inderjit Dhillon: you know, you don't need to model it as a Gaussian.


698
01:15:19.433 --> 01:15:23.573
Inderjit Dhillon: And you're also trying to estimate deep square parameters. So it's actually


699
01:15:23.703 --> 01:15:25.953
Inderjit Dhillon: not a good method, this one.


700
01:15:26.673 --> 01:15:33.723
Inderjit Dhillon: But logistic regression is generally one of the best methods.


701
01:15:34.073 --> 01:15:35.512
Inderjit Dhillon: Pretty deep learning curve.


702
01:15:36.763 --> 01:15:47.892
Vishal Thyagarajan: Got it. So all this does is like it's just motivation, not actually, like assuming that, like log of the probabilities divided by like each like each other, is a Gaussian right?


703
01:15:47.893 --> 01:15:51.101
Inderjit Dhillon: Yeah, it's not assuming that these are costumes.


704
01:15:51.663 --> 01:16:00.462
Inderjit Dhillon: Yes. So by maximizing the data, likelihood, we are basically doing. That's what logistic regression does. And these are called log odds.


705
01:16:01.483 --> 01:16:02.752
Vishal Thyagarajan: I got it. Thank you.


706
01:16:03.263 --> 01:16:09.573
Inderjit Dhillon: Okay, great. Any other questions, Nilesh, are you around.


707
01:16:10.403 --> 01:16:14.423
Nilesh Gupta: Yeah. And I was going to say, like, we need to get back to Anises question.


708
01:16:14.423 --> 01:16:15.713
Inderjit Dhillon: Exactly. Yes.


709
01:16:16.563 --> 01:16:17.423
Inderjit Dhillon: So you want to.


710
01:16:17.423 --> 01:16:20.038
Nilesh Gupta: Yes. Okay. So anas


711
01:16:20.993 --> 01:16:31.132
Nilesh Gupta: like. Whenever in general, like a good thumb of rule, rule of thumb is that like, you want to test your hyper parameters on a logarithmic scale.


712
01:16:31.133 --> 01:16:52.103
Nilesh Gupta: That is, you try different values. And typically like between 10 par minus 5 to something like 10 power, one or 10 part 2 is what you want to try for most of these hyperparameters, and if you find some interesting regions where there is a lot of activity in the final numbers that you are getting there like you might want to refine it even more.


713
01:16:52.533 --> 01:16:54.802
Nilesh Gupta: So hopefully that answers it.


714
01:16:56.333 --> 01:16:57.361
Inderjit Dhillon: Anas, does that answer your question?


---- END OF LECTURE -------- START OF LECTURE 10 ----
WEBVTT

1
00:00:00.422 --> 00:00:07.561
Inderjit Dhillon: we'll start actually working with deep learning. And so today we have a lecture.


2
00:00:07.792 --> 00:00:11.181
Inderjit Dhillon: The 1st lecture on introduction to deep learning.


3
00:00:11.738 --> 00:00:16.251
Inderjit Dhillon: I'm afraid last week or end of last week I hurt my


4
00:00:16.382 --> 00:00:26.031
Inderjit Dhillon: back actually, quite badly. So today it will be the Ta. Nilesh, who will be teaching this lecture. So, Nilesh.


5
00:00:26.202 --> 00:00:27.322
Inderjit Dhillon: you can go ahead.


6
00:00:29.142 --> 00:00:30.392
Nilesh Gupta: Thanks. Najit.


7
00:00:30.782 --> 00:00:36.332
Nilesh Gupta: So yeah, today's lecture is on introduction to deep learning.


8
00:00:36.622 --> 00:00:39.859
Nilesh Gupta: Here we'll talk a little bit about


9
00:00:40.512 --> 00:00:44.451
Nilesh Gupta: deep learning what it is, why do we want to do it.


10
00:00:44.692 --> 00:00:55.451
Nilesh Gupta: and what are its applications? Why is it so exciting? And then we'll look into what are the key ingredients that make this deep learning framework.


11
00:00:56.452 --> 00:01:00.101
Nilesh Gupta: So yeah, let's begin.


12
00:01:00.682 --> 00:01:04.032
Nilesh Gupta: So what is deep learning.


13
00:01:04.392 --> 00:01:11.431
Nilesh Gupta: deep learning is a subfield of machine learning that leverages deep neural networks to model data.


14
00:01:11.672 --> 00:01:14.856
Nilesh Gupta: So if if we if we draw


15
00:01:15.652 --> 00:01:21.632
Nilesh Gupta: a rough vein diagram so like, if AI is the bigger, no


16
00:01:21.842 --> 00:01:30.801
Nilesh Gupta: circle like which which constitutes of, like the the goal of trying to mimic human behavior like trying to mimic human intelligence.


17
00:01:30.972 --> 00:01:36.342
Nilesh Gupta: So machine learning is a sub field of AI which uses


18
00:01:36.452 --> 00:01:42.531
Nilesh Gupta: statistical techniques to sort of do. AI


19
00:01:54.715 --> 00:02:08.442
Nilesh Gupta: deep learning is a smaller circle inside machine learning, which specifically leverages neural networks to achieve the


20
00:02:08.812 --> 00:02:13.621
Nilesh Gupta: goal of AI, which is trying to mimic intelligence artificially.


21
00:02:13.952 --> 00:02:15.082
Nilesh Gupta: So


22
00:02:16.422 --> 00:02:23.052
Nilesh Gupta: why do we want to do it like? Why? Why not traditional machine learning that we have seen so far


23
00:02:23.252 --> 00:02:46.111
Nilesh Gupta: because it is a very powerful technique which allows us to automatically learn representations from data eliminating the need for manual feature engineering that is like with classical machine learning. There are often times like you need specific practitioners in the field to design the features for the


24
00:02:46.482 --> 00:02:49.931
Nilesh Gupta: task that you are solving. For to get an effective method.


25
00:02:50.042 --> 00:03:05.441
Nilesh Gupta: What deep learning promises, not like sort of promises to do, is it? It eliminates that need, and it can learn from large amount of data by itself, without any need to design


26
00:03:06.215 --> 00:03:11.821
Nilesh Gupta: features, or like special representations to enable machine learning


27
00:03:13.572 --> 00:03:37.482
Nilesh Gupta: so deep learning. Neural networks as a concept have existed before. Like they were. They were 1st introduced in some something like 19 fifties, but they were not successful at that time in this current, like since last decade they have gained a lot of traction. And this has happened


28
00:03:37.632 --> 00:03:49.452
Nilesh Gupta: because of these 3 components. First, st is the hardware, that is, Gpus and Tpus happen, which are a very powerful hardware to accelerate these


29
00:03:49.572 --> 00:03:52.722
Nilesh Gupta: matrix operations.


30
00:03:53.702 --> 00:03:59.251
Nilesh Gupta: And then the like Internet also happened which gave us a lot of data to train on.


31
00:03:59.402 --> 00:04:06.892
Nilesh Gupta: So these unsupervised methods which which don't need any


32
00:04:07.242 --> 00:04:10.341
Nilesh Gupta: any annotation to to train on


33
00:04:10.472 --> 00:04:16.622
Nilesh Gupta: they they became possible because, like the data was so much available that, like you


34
00:04:17.182 --> 00:04:21.702
Nilesh Gupta: you were able to train on such a vast amount of data.


35
00:04:21.872 --> 00:04:31.452
Nilesh Gupta: and the model was able to learn properly. And then the 3rd is the 3rd pillar is the frameworks. These it is


36
00:04:32.403 --> 00:04:48.961
Nilesh Gupta: sub frameworks, like pytorch tensorflow jax they enabled, or they provided, a very seamless Apis to work on and to to share code on that open source community could build upon, and


37
00:04:49.182 --> 00:05:00.032
Nilesh Gupta: a lot of the the details of the deep learning could be hidden inside these frameworks, and people can work on abstractions, simpler abstractions.


38
00:05:00.702 --> 00:05:09.352
Nilesh Gupta: So these 3 pillar pillars kind of enabled deep learning in modern era and sorry.


39
00:05:15.782 --> 00:05:30.512
Nilesh Gupta: So let's come to some of its applications. So essentially, any intelligent task can be. Deep learning can be applied to it could be object recognition that is, given an image or a video like you want to identify an object.


40
00:05:30.762 --> 00:05:36.502
Nilesh Gupta: It could be self-driving. It could be playing games something like chess, Alphago


41
00:05:37.296 --> 00:05:44.252
Nilesh Gupta: go. And any kind of games like recently AI agent has also solved among us.


42
00:05:44.372 --> 00:05:45.901
Nilesh Gupta: which is great to see


43
00:05:46.362 --> 00:05:58.031
Nilesh Gupta: and conversation agent that we see as chat bots like they are ubiquitous these days. And then we have media generation, something like, imagine Dali.


44
00:05:58.232 --> 00:06:19.561
Nilesh Gupta: and much more that followed which which can, like given a text from, they can generate image or a full length video even these days, and at the end, like maybe like it, the any like the artificial general intelligence that that probably like, at least that's the goal that we want to solve using deep learning.


45
00:06:20.272 --> 00:06:28.142
Nilesh Gupta: So all all of these applications are exciting, and they they are worth working for.


46
00:06:28.602 --> 00:06:29.772
Nilesh Gupta: So


47
00:06:29.941 --> 00:06:46.861
Nilesh Gupta: let's look at like some of the key components of deep learning. So there are 3 kind of broad components of a deep learning framework. The 1st is the model, which is typically a neural network that maps your input data to output prediction.


48
00:06:48.232 --> 00:06:52.372
Nilesh Gupta: Then, second, you have a loss. It's it's a.


49
00:06:52.582 --> 00:07:00.041
Nilesh Gupta: It's so. Loss is a scalar quantity that defines how well a model fits our data.


50
00:07:00.342 --> 00:07:10.842
Nilesh Gupta: So it is typically defined using a loss function which consumes your model and your data. And it gives out this scalar quantity which we call loss.


51
00:07:11.702 --> 00:07:12.872
Nilesh Gupta: So


52
00:07:13.332 --> 00:07:26.092
Nilesh Gupta: the 3rd is the optimization. It's a process, typically some variant of gradient descent that adjusts the model parameters to minimize the loss. So it's essentially the learning process


53
00:07:26.252 --> 00:07:33.682
Nilesh Gupta: that you have defined your task, and you have defined your mechanism to solve the task. Now, how do you learn?


54
00:07:33.902 --> 00:07:38.012
Nilesh Gupta: The learning process itself is what we call optimization?


55
00:07:38.192 --> 00:07:47.731
Nilesh Gupta: So these 3 are the key components. And in the rest of the class, like, we will dive deeper into all of these 3 components one by one.


56
00:07:48.772 --> 00:07:54.122
Nilesh Gupta: So before we go ahead, any questions so far.


57
00:08:02.782 --> 00:08:11.772
Nilesh Gupta: Okay, so let's come to the model, which is essentially neural networks in the case of deep learning.


58
00:08:12.082 --> 00:08:37.362
Nilesh Gupta: So a brief history like I said earlier, the early work on perceptrons in 19 fifties, laid the foundation for neural networks. But back then, because of limited compute limited data, these methods were not that successful, and eventually, like the excitement around these methods like did die down. But in 19 nineties, like, they saw a resurgence.


59
00:08:37.542 --> 00:09:00.072
Nilesh Gupta: and with more data available, and specifically in 2012, with Imagenet and Gpus getting available, they saw a big rise in popularity. And that's the main. It's been the main like workhorse behind all kind of AI. Since then.


60
00:09:00.312 --> 00:09:22.601
Nilesh Gupta: so roughly, it's modeled after the human brain's neurons. That is like in the brain. Like we see these interconnected neurons where a neuron is a basic computational unit. It takes some inputs and produces some activation based on what kind of inputs is seeing. So essentially like it's neural networks themselves are also somewhat inspired.


61
00:09:24.102 --> 00:09:37.212
Nilesh Gupta: but it's it's they do work quite differently than actual neurons. So like it's, it's like, it's only fair to say that they are inspired, but they are not exactly trying to replicate what brain neurons are doing.


62
00:09:37.612 --> 00:09:38.772
Nilesh Gupta: So


63
00:09:39.972 --> 00:09:54.151
Nilesh Gupta: so let's look at what a neuron looks like in a neural, deep, neural network that deep learning uses. So it's it's as I said, like, it's the building block for our neural network.


64
00:09:56.027 --> 00:10:04.671
Nilesh Gupta: it's a basic computational unit. So there are 4 components to it. One is like it, the inputs it receives multiple scalar inputs.


65
00:10:04.862 --> 00:10:09.022
Nilesh Gupta: And then there are weights for each of the input, it receives


66
00:10:09.352 --> 00:10:16.372
Nilesh Gupta: the weight essentially measures how much that input is important for activating this neuron


67
00:10:16.712 --> 00:10:21.661
Nilesh Gupta: and the summation. The so like the summation part is just it's it's a


68
00:10:21.892 --> 00:10:27.191
Nilesh Gupta: it's doing all the sum of adds up all the weighted inputs that it is seeing.


69
00:10:27.292 --> 00:10:32.521
Nilesh Gupta: And then there is a nonlinearity which is a filter or activation function


70
00:10:32.682 --> 00:10:49.551
Nilesh Gupta: that, given given the weighted sum like it decides whether to like, shoot up or like stay below 0. So it's it's acts kind of like a filter function if you want to loosely draw analogy. So


71
00:10:49.992 --> 00:10:53.991
Nilesh Gupta: let's quickly look at like a like a basic neuron.


72
00:10:55.712 --> 00:11:09.441
Nilesh Gupta: here, like, I'm looking at a linear neuron, this doesn't have a non linearity. So it just is doing a weighted sum of its inputs. So here, as I like, we have our input.


73
00:11:09.662 --> 00:11:10.542
Nilesh Gupta: Sorry.


74
00:11:14.892 --> 00:11:17.182
Nilesh Gupta: Our input consists of


75
00:11:20.242 --> 00:11:22.522
Nilesh Gupta: this is our input, which is


76
00:11:22.812 --> 00:11:30.631
Nilesh Gupta: or 2 dimensional. Input 1st is x 1, which is your X coordinate, let's say, and your x 2, which is, let's say, your y coordinate.


77
00:11:31.392 --> 00:11:32.312
Nilesh Gupta: And


78
00:11:32.732 --> 00:11:46.241
Nilesh Gupta: here, like all throughout the figures, the kind of notation is like blue represents a positive quantity, and orange represents a negative quantity. So keep that and like, if if I if you are


79
00:11:47.028 --> 00:11:57.751
Nilesh Gupta: like, sometimes like when when I'm like denoting data points like or like class classification. So they represent like 2 different classes. So


80
00:11:58.152 --> 00:12:01.752
Nilesh Gupta: so like here, like as we can see,


81
00:12:05.122 --> 00:12:10.711
Nilesh Gupta: we have this neuron, which which, essentially like is this module.


82
00:12:10.952 --> 00:12:21.871
Nilesh Gupta: It's it's taking 2 inputs x 1 x 2, it's multiplying x, 1 with w, 1 x, 2 with w, 2, summing them up and giving this output


83
00:12:22.072 --> 00:12:22.832
Nilesh Gupta: sorry.


84
00:12:28.122 --> 00:12:31.140
Nilesh Gupta: which is Z that we denote. It's


85
00:12:31.702 --> 00:12:35.112
Nilesh Gupta: w. 1 times x 1 plus w 2 times x 2.


86
00:12:35.922 --> 00:13:03.732
Nilesh Gupta: So that's how a neuron looks like. And on the right, like, I have shown the decision boundary that this neuron creates so essentially depending on what your w. 1 and W. 2. This decision boundary will look something different. But right now, for like for some w. 1 and W. 2, the decision boundary looks like this. And it is always going to be a linear kind of a decision boundary.


87
00:13:04.952 --> 00:13:09.602
Nilesh Gupta: Because, like, it's, it's a linear function, the like, what a neuron is modeling.


88
00:13:10.342 --> 00:13:17.901
Nilesh Gupta: And yeah, here you can see. See that like for this particular data, this decision boundary, like perfectly


89
00:13:18.567 --> 00:13:21.062
Nilesh Gupta: Fits the data that we have.


90
00:13:21.842 --> 00:13:26.901
Nilesh Gupta: So that's what a linear neuron looks like.


91
00:13:27.002 --> 00:13:34.982
Nilesh Gupta: Now, let's add a nonlinearity in this neuron. So essentially like, we still have the same. This is our input


92
00:13:35.702 --> 00:13:41.522
Nilesh Gupta: but our neuron now has this additional component, which is our nonlinearity


93
00:13:45.782 --> 00:13:54.371
Nilesh Gupta: in this case. In the diagram, I'm showing you a rectified linear unit which is, in short called relu


94
00:13:54.966 --> 00:14:07.182
Nilesh Gupta: it's a simple function which we'll see later. But like it's essentially like for all the negative input like, it stays at 0. And for the all, the positive input like


95
00:14:10.462 --> 00:14:15.681
Nilesh Gupta: it linearly gives you the identity function.


96
00:14:16.752 --> 00:14:23.152
Nilesh Gupta: So this is our x-axis. This is value of X,


97
00:14:27.762 --> 00:14:33.292
Nilesh Gupta: so it's a very simple nonlinearity that is like it, just kind of


98
00:14:33.462 --> 00:14:47.662
Nilesh Gupta: partitions. Your, the input that it is seeing. So if it's negative, it remains 0. If it's positive or like non negative. It just gives you the input itself.


99
00:14:48.042 --> 00:14:57.492
Nilesh Gupta: So it's a very simple nonlinearity. But like this itself is very powerful, like, if you will see later that if you, if you start stacking up


100
00:14:57.592 --> 00:15:04.682
Nilesh Gupta: neurons with such nonlinearities like you can just draw arbitrary. You can fit any arbitrary function.


101
00:15:05.102 --> 00:15:10.221
Nilesh Gupta: So I hope so far the the


102
00:15:11.022 --> 00:15:15.882
Nilesh Gupta: the things that I presented are clear. I'll take some time


103
00:15:16.072 --> 00:15:21.272
Nilesh Gupta: to see like, if there any questions or anything confusing with the notations.


104
00:15:29.272 --> 00:15:30.042
Nilesh Gupta: Okay?


105
00:15:31.312 --> 00:15:36.161
Nilesh Gupta: So so there are. Let's let's take a


106
00:15:36.412 --> 00:15:46.181
Nilesh Gupta: brief look at some of the nonlinearities that are popular in deep learning and kind of constitute the basic nonlinearities.


107
00:15:47.812 --> 00:15:52.631
Nilesh Gupta: So 1st is value, as I said, like, it's, it's a function, G of X,


108
00:15:52.922 --> 00:16:03.511
Nilesh Gupta: which gives 0 if x is less than 0, and it is X, if X is greater than 0.


109
00:16:05.652 --> 00:16:10.012
Nilesh Gupta: So it's a simple function. It's kind of looks like this.


110
00:16:12.742 --> 00:16:21.312
Nilesh Gupta: and yeah, but but it's still quite powerful. And like some variations of it, is still used in everything that you


111
00:16:21.602 --> 00:16:28.662
Nilesh Gupta: so like all the top deep learning products that you see, including chat, gpt, and stuff.


112
00:16:29.182 --> 00:16:33.302
Nilesh Gupta: So this is a relu nonlinearity.


113
00:16:33.842 --> 00:16:39.682
Nilesh Gupta: And then sigmoid, as we saw in the last lecture. It's it's it's


114
00:16:43.962 --> 00:16:49.052
Nilesh Gupta: it's the function that it represents is this one plus e per minus x.


115
00:16:49.382 --> 00:16:51.365
Nilesh Gupta: So essentially like it's, it's


116
00:16:51.942 --> 00:16:56.302
Nilesh Gupta: it's graph. Looks something like this like, for


117
00:16:57.232 --> 00:16:59.541
Nilesh Gupta: maybe I should use different color


118
00:17:01.312 --> 00:17:04.511
Nilesh Gupta: for all the negative values like it remains at 0


119
00:17:05.707 --> 00:17:09.612
Nilesh Gupta: not all the negative values, but like, if if you're


120
00:17:09.772 --> 00:17:17.631
Nilesh Gupta: x is very negative, like it kind of stays close to 0 and then add


121
00:17:17.762 --> 00:17:22.951
Nilesh Gupta: x equal to 0. It gets to like point 5, and then at


122
00:17:23.222 --> 00:17:29.002
Nilesh Gupta: slowly it started increasing, and then one


123
00:17:30.660 --> 00:17:35.912
Nilesh Gupta: it, it, it gets capped at the value of one like as you keep increasing your x.


124
00:17:37.272 --> 00:17:42.381
Nilesh Gupta: so it kinds of like acts as a gated gating function that


125
00:17:43.119 --> 00:18:00.571
Nilesh Gupta: if if your X is very negative, like, it is 0, if your X is very positive, or like even moderately positive, it it gets to one, and in between, like it, it gradually rises to 0 to one.


126
00:18:00.982 --> 00:18:03.251
Nilesh Gupta: So that's how the curve looks like.


127
00:18:03.896 --> 00:18:13.222
Nilesh Gupta: This we have already seen in our previous lecture 1010 H. Is also


128
00:18:13.632 --> 00:18:16.472
Nilesh Gupta: like it's. It's the inverse tan function.


129
00:18:17.382 --> 00:18:29.102
Nilesh Gupta: and it's essentially like the same as sigmoid. Just that like it's a scaled and shifted version of sigmoid that it's instead of like being capped between 0 and one. It is between minus one and one.


130
00:18:29.292 --> 00:18:42.392
Nilesh Gupta: So it's almost identical to sigmoid, just that. It's 1 minus one.


131
00:18:42.692 --> 00:18:48.632
Nilesh Gupta: So it's it's kind of like 2 times wider than sigmoid.


132
00:18:48.892 --> 00:18:56.812
Nilesh Gupta: and then like. But and the other thing is like at 0. It it is at 0 so


133
00:18:59.512 --> 00:19:01.101
Nilesh Gupta: so very similar to sigmoid.


134
00:19:02.982 --> 00:19:13.132
Nilesh Gupta: Then we have softmax. So softmax is something different than these 3 nonlinearities. So in the sense that, like these nonlinearities are act on a scalar.


135
00:19:13.692 --> 00:19:21.511
Nilesh Gupta: That is this X that so far I've been giving you. It's it's it's a real number. But softmax acts on a. Vector


136
00:19:22.312 --> 00:19:24.421
Nilesh Gupta: so here let me.


137
00:19:29.662 --> 00:19:33.932
Nilesh Gupta: they know the vector with different notation.


138
00:19:35.322 --> 00:19:43.202
Nilesh Gupta: So here, my X is a scalar, a vector of real numbers.


139
00:19:43.622 --> 00:19:47.571
Nilesh Gupta: So what it does is like it.


140
00:19:48.202 --> 00:19:52.472
Nilesh Gupta: It so it it takes a vector and gives you a, vector so it it


141
00:19:52.592 --> 00:19:55.851
Nilesh Gupta: it G is my softmax is


142
00:19:56.352 --> 00:20:01.682
Nilesh Gupta: r to the n, to r, to the n, and the


143
00:20:02.662 --> 00:20:12.762
Nilesh Gupta: Ith element of this vector is going to be E, bar XI divided by sum of if are.


144
00:20:13.192 --> 00:20:23.696
Nilesh Gupta: although XJ is where J is from one to n, so it's


145
00:20:30.882 --> 00:20:34.902
Nilesh Gupta: so it's it's typically used to convert or


146
00:20:35.532 --> 00:21:00.662
Nilesh Gupta: draw vector that that is unbounded, that is like it's it can all the values inside, the vector can be anything between minus infinity to infinity. And then like, it takes that vector and converts it into a probability distribution. That is like, if you see that, like one good property, that this has is like summation of all the


147
00:21:02.422 --> 00:21:07.732
Nilesh Gupta: Gx. I. From i. 1 to N is one


148
00:21:07.872 --> 00:21:13.602
Nilesh Gupta: that it satisfies the probability constraint that, and more on top of that


149
00:21:14.152 --> 00:21:18.521
Nilesh Gupta: G of XI is greater than


150
00:21:21.892 --> 00:21:29.492
Nilesh Gupta: is greater than 0. So, and these are the 2 things that we need for a


151
00:21:30.450 --> 00:21:34.281
Nilesh Gupta: vector of scalars to represent a probability distribution.


152
00:21:36.272 --> 00:21:37.552
Nilesh Gupta: So


153
00:21:37.872 --> 00:21:59.372
Nilesh Gupta: we'll see this often at the output layer of a neural network. We'll talk about like what output layer of a neural network is. But whenever we want to convert some raw vector, quantities to a probability distribution vector of probabilities. Then, like, we use this softmax function.


154
00:21:59.872 --> 00:22:06.592
Nilesh Gupta: And yeah, this is also something very popular now that we'll see later.


155
00:22:08.792 --> 00:22:09.692
Nilesh Gupta: Okay?


156
00:22:09.872 --> 00:22:13.542
Nilesh Gupta: So that concludes the


157
00:22:15.292 --> 00:22:22.711
Nilesh Gupta: brief discussion on the nonlinearities. Let's see how these neurons can be interconnected to form a deep neural network.


158
00:22:22.912 --> 00:22:32.291
Nilesh Gupta: So a deep neural network, like essentially is a layered arrangement of neurons that that forms a neural network.


159
00:22:32.502 --> 00:22:44.052
Nilesh Gupta: So, for example, in this like each of like, we have 2 hidden layers, this and this.


160
00:22:44.362 --> 00:22:48.731
Nilesh Gupta: And then we have the the input layer as before.


161
00:22:48.892 --> 00:22:58.722
Nilesh Gupta: And then, like, here is our output layer like which so essentially like, there's a neuron setting here as well, which which gives out like just one single scalar.


162
00:22:59.172 --> 00:23:03.302
Nilesh Gupta: So here we we have.


163
00:23:05.501 --> 00:23:15.562
Nilesh Gupta: So all of remember, like all of these neurons, are, some like, represent just this quantity, just that, like they are arranged in a stacked up manner. Now.


164
00:23:16.272 --> 00:23:24.031
Nilesh Gupta: so let's look at like, how how do we? How like? How does a


165
00:23:24.412 --> 00:23:32.231
Nilesh Gupta: how does this whole neural network acts as a function like, how does it take an input and gives you the output that you want?


166
00:23:33.042 --> 00:23:40.952
Nilesh Gupta: So let me try to write it down the some notations first.st


167
00:23:41.752 --> 00:23:43.052
Nilesh Gupta: So.


168
00:23:44.292 --> 00:23:45.252
Nilesh Gupta: So


169
00:23:51.592 --> 00:23:57.091
Nilesh Gupta: remember that, like each of the edges, represents a weight. So the


170
00:23:58.433 --> 00:24:21.721
Nilesh Gupta: as I said, like the notation that here I'm using is blue color represents a negative quantity. Orange, blue color represents a positive quantity. Orange represents a negative quantity, and the thickness represents the weight of that quantity. So here like, if you see like, these are the weights of your neural network. So this let me denote as


171
00:24:21.892 --> 00:24:22.702
Nilesh Gupta: sorry.


172
00:24:36.132 --> 00:24:37.112
Nilesh Gupta: So


173
00:24:40.242 --> 00:24:49.212
Nilesh Gupta: this is the wait for the 1st neuron and the 1st input. So I'll represent w 1 1.


174
00:24:50.012 --> 00:24:58.641
Nilesh Gupta: And this is the weight for 1st neuron and second input, sorry.


175
00:24:59.672 --> 00:25:05.401
Nilesh Gupta: So I'll represent as 2 comma one. And similarly, like, we can have like a weight


176
00:25:05.672 --> 00:25:08.371
Nilesh Gupta: for each of the edges that we see here.


177
00:25:09.442 --> 00:25:10.692
Nilesh Gupta: So


178
00:25:11.742 --> 00:25:19.172
Nilesh Gupta: here, like the notation that we'll be using is like, like, because this is for our second hidden layer.


179
00:25:19.912 --> 00:25:20.842
Nilesh Gupta: So


180
00:25:24.142 --> 00:25:27.512
Nilesh Gupta: so this takes the 1st input


181
00:25:27.622 --> 00:25:33.162
Nilesh Gupta: and it's for the 1st hidden year neuron of the layer. 2.


182
00:25:33.962 --> 00:25:39.662
Nilesh Gupta: See? Now be 2.


183
00:26:11.702 --> 00:26:16.042
Nilesh Gupta: So if you look at this neuron, which I'm going to call


184
00:26:16.722 --> 00:26:24.552
Nilesh Gupta: n. 1, 1, because, like this represents like which layer it belongs to layer one.


185
00:26:24.842 --> 00:26:33.711
Nilesh Gupta: This represents, like, what is the index of this neuron in that layer.


186
00:26:37.072 --> 00:26:42.781
Nilesh Gupta: So this n. 1, 1 has 2 parameters, which is


187
00:26:43.532 --> 00:26:50.852
Nilesh Gupta: w. 1, 1 com. One and tap blue, 2.


188
00:26:55.972 --> 00:27:10.462
Nilesh Gupta: Similarly like, if if we take this hidden neuron, this I'll call, and 1, 2 presence layer, 2 presence


189
00:27:12.392 --> 00:27:15.371
Nilesh Gupta: index of this neuron.


190
00:27:19.912 --> 00:27:26.151
Nilesh Gupta: So this has 3 parameters, which is the blue.


191
00:27:26.812 --> 00:27:37.172
Nilesh Gupta: 2, 1 com. 1, 2, 2, group, comma, one tablet, 3.


192
00:27:55.542 --> 00:27:56.562
Nilesh Gupta: So


193
00:27:59.012 --> 00:28:19.991
Nilesh Gupta: as you can see, like, like all of these neurons, comes with their own parameters. So like, if you go on to write, the whole equations like it will become quite messy. So how? How we typically represent our neural network is in terms of matrices. So each layer


194
00:28:21.660 --> 00:28:26.502
Nilesh Gupta: gets can be represented as a matrix of


195
00:28:27.240 --> 00:28:35.312
Nilesh Gupta: size. In this case, 2 cross 3, because it acts, it has like 2, it takes 2 inputs.


196
00:28:35.512 --> 00:28:38.101
Nilesh Gupta: and it produces like 3 outputs.


197
00:28:40.692 --> 00:28:53.442
Nilesh Gupta: So so like we can represent hidden layer, one h. 1 as w, 1, which is


198
00:28:55.502 --> 00:29:00.792
Nilesh Gupta: a matrix of size, 2 cross, 3, and then


199
00:29:01.282 --> 00:29:11.892
Nilesh Gupta: we can represent hidden layer 2 as W. 2 with chess, matrix of size.


200
00:29:16.182 --> 00:29:17.562
Nilesh Gupta: G. Cross.


201
00:29:18.552 --> 00:29:22.772
Nilesh Gupta: Sorry it's case 2.


202
00:29:23.542 --> 00:29:29.702
Nilesh Gupta: And then we have the output layer, as I said, like, imagine a neuron setting here, this


203
00:29:29.992 --> 00:29:31.452
Nilesh Gupta: in this case.


204
00:29:35.782 --> 00:29:42.521
Nilesh Gupta: So its shape is going to be 2 cross one, because it's taking 2 inputs producing one output.


205
00:29:43.642 --> 00:29:49.311
Nilesh Gupta: So. And one thing that I missed is there is also like


206
00:29:49.552 --> 00:29:52.342
Nilesh Gupta: like the inputs can also be considered as a layer.


207
00:29:52.502 --> 00:30:05.711
Nilesh Gupta: So in this case, the input is just one data point. But typically in deep learning, like we deal with multiple multiple inputs at once. So like we create a batch of inputs.


208
00:30:05.812 --> 00:30:15.242
Nilesh Gupta: So in this case, let's say, the batch are are like in the batch, like, we have only have one data point. So like, our input, is.


209
00:30:18.952 --> 00:30:20.941
Nilesh Gupta: Input is of shape


210
00:30:29.862 --> 00:30:36.312
Nilesh Gupta: 2 cross one, where this one represents, like the batch size.


211
00:30:36.942 --> 00:30:39.962
Nilesh Gupta: this particular case. But it could be anything.


212
00:30:41.622 --> 00:30:46.835
Nilesh Gupta: So this is our X. And now, if we go on writing our


213
00:30:47.662 --> 00:30:52.132
Nilesh Gupta: like, what? What function this this whole neural network represents.


214
00:30:52.362 --> 00:30:54.712
Nilesh Gupta: So we can try that.


215
00:30:55.362 --> 00:30:56.962
Nilesh Gupta: So the


216
00:31:12.502 --> 00:31:15.112
Nilesh Gupta: in right Z.


217
00:31:17.122 --> 00:31:22.062
Nilesh Gupta: One, which is just the weighted sum after hidden layer one.


218
00:31:22.212 --> 00:31:28.482
Nilesh Gupta: It's that's going to be so let's see the shapes


219
00:31:30.292 --> 00:31:34.292
Nilesh Gupta: and so we want to have like 3 outputs. And


220
00:31:38.312 --> 00:31:44.262
Nilesh Gupta: we have 2 inputs. So it's going to be,


221
00:31:47.702 --> 00:31:54.001
Nilesh Gupta: if someone wants to help me like in what? What's it's it's going to be like W. Times X. But like.


222
00:31:54.142 --> 00:31:55.472
Nilesh Gupta: does anyone.


223
00:31:55.612 --> 00:32:02.731
Nilesh Gupta: Is this right? Like just writing 11 matrix multiplication with the x.


224
00:32:10.762 --> 00:32:13.252
Nilesh Gupta: No, right like the shapes don't match.


225
00:32:15.642 --> 00:32:17.662
Nilesh Gupta: Sorry someone was saying something.


226
00:32:18.551 --> 00:32:20.731
Hormoz Shahrzad: Yeah, so should it be. Eggs.


227
00:32:21.151 --> 00:32:23.311
Hormoz Shahrzad: Transpose times. W.


228
00:32:24.747 --> 00:32:34.821
Nilesh Gupta: X transpose times. W also works it depends on like what kind of notation you want to use, but like it does work it will like if you, if you, if we


229
00:32:36.042 --> 00:32:37.341
Nilesh Gupta: I do.


230
00:32:38.611 --> 00:32:43.732
Nilesh Gupta: X, transpose times W. This will give me something


231
00:32:44.152 --> 00:32:51.141
Nilesh Gupta: of the shape. One cross 3. Where, like I've shifted the batch access on the 1st one.


232
00:32:51.872 --> 00:32:52.432
Hormoz Shahrzad: Right.


233
00:32:53.022 --> 00:32:55.411
Nilesh Gupta: And here, like this is my output axis.


234
00:32:55.522 --> 00:32:59.062
Nilesh Gupta: or I could have written. Instead of this, I could have


235
00:33:00.062 --> 00:33:06.642
Nilesh Gupta: done the other way, like without shifting the batch access, which is W.


236
00:33:07.702 --> 00:33:12.462
Nilesh Gupta: One transpose X, which will give me


237
00:33:14.912 --> 00:33:20.482
Nilesh Gupta: 3 cross one where the Batch accessor remains consistent.


238
00:33:20.942 --> 00:33:27.332
Nilesh Gupta: So for the sake of this lecture, I'll use this notation, which is like, I'll use W transpose X


239
00:33:27.522 --> 00:33:32.262
Nilesh Gupta: to denote the linear combination.


240
00:33:32.957 --> 00:33:37.812
Nilesh Gupta: But yeah, you could have done X. Transpose. W. It just depends on what notation you want to use.


241
00:33:38.192 --> 00:33:57.272
Nilesh Gupta: So this is my so. But remember, like this is not the final output of my hidden layer one. This is just the weighted sums I now need to apply the activation. So let's assume that, like in this neural network, we are using sigmoid activation which we represent by Sigma.


242
00:33:57.492 --> 00:33:59.002
Nilesh Gupta: So our


243
00:34:02.782 --> 00:34:05.401
Nilesh Gupta: maybe, let me use a different color.


244
00:34:06.152 --> 00:34:13.842
Nilesh Gupta: So after playing the activation, we'll get sigmoid of z 1.


245
00:34:14.052 --> 00:34:19.961
Nilesh Gupta: And this is an element wise operation. So it doesn't really change the shape of your


246
00:34:20.242 --> 00:34:24.732
Nilesh Gupta: output. So it still gives you a 3 cross one entity.


247
00:34:26.042 --> 00:34:35.822
Nilesh Gupta: So. And now this even, is what this, this whole output that we will be seeing from the collective 3 neurons in the hidden layer. One.


248
00:34:36.982 --> 00:34:46.412
Nilesh Gupta: So okay, so now that we have seen, like 1st layer in action, we can now see second layer.


249
00:34:46.852 --> 00:34:51.982
Nilesh Gupta: So the weighted SIM of the second layer is going to be something like W.


250
00:34:52.982 --> 00:34:53.822
Nilesh Gupta: 2.


251
00:34:54.122 --> 00:34:55.592
Nilesh Gupta: Transpose.


252
00:34:55.912 --> 00:34:58.981
Nilesh Gupta: And now remember, like this hidden layer 2


253
00:34:59.242 --> 00:35:04.191
Nilesh Gupta: is acting such as like the output of hidden layer. One is input for hidden layer 2.


254
00:35:04.482 --> 00:35:09.642
Nilesh Gupta: So now we'll multiply this with a of one.


255
00:35:11.922 --> 00:35:14.752
Nilesh Gupta: And what's the shape that I'll get?


256
00:35:15.202 --> 00:35:20.932
Nilesh Gupta: I'll get the shape since it's W. 2. Transpose I'll get 2 cross.


257
00:35:21.332 --> 00:35:22.022
Nilesh Gupta: What?


258
00:35:23.252 --> 00:35:26.742
Nilesh Gupta: So remember, this one is still my batch access.


259
00:35:27.002 --> 00:35:31.872
Nilesh Gupta: and this 2 is the output that I'm seeing from hidden layer 2.


260
00:35:32.922 --> 00:35:35.892
Nilesh Gupta: And finally, like I can oh.


261
00:35:36.052 --> 00:35:39.472
Nilesh Gupta: like similar to hidden layer one. I can do


262
00:35:39.662 --> 00:35:43.271
Nilesh Gupta: a 2 sigmoid of Z. 2,


263
00:35:49.392 --> 00:35:58.452
Nilesh Gupta: and then let's assume that there is another neuron, which which is off size 2 plus one.


264
00:35:59.702 --> 00:36:11.432
Nilesh Gupta: So my Z. 3 is going to be W. 3. Transpose E. 2.


265
00:36:15.512 --> 00:36:19.162
Nilesh Gupta: Assuming this is all this also has the


266
00:36:19.952 --> 00:36:24.161
Nilesh Gupta: sigmoid activation. This is going to be sigmoid of C.


267
00:36:25.022 --> 00:36:25.722
Nilesh Gupta: 3.


268
00:36:29.472 --> 00:36:31.912
Nilesh Gupta: And what's the shape of this?


269
00:36:32.542 --> 00:36:37.102
Nilesh Gupta: It's the one cross, one batch.


270
00:36:44.232 --> 00:36:54.511
Nilesh Gupta: So that's the final output that my neural network is giving me like for a batch of data points which have 2 dimensions as the features


271
00:36:55.187 --> 00:36:59.461
Nilesh Gupta: and in this case there is there are only like the batch. Size is one.


272
00:36:59.672 --> 00:37:07.322
Nilesh Gupta: So that's why I'm getting a 1 cross one. But if I had n data points in my batch, I would have got a r 1 cross n


273
00:37:08.178 --> 00:37:12.101
Nilesh Gupta: quantity at the end. And a 3 is my final output.


274
00:37:12.522 --> 00:37:16.641
Nilesh Gupta: And that's how typically a neural network works.


275
00:37:17.302 --> 00:37:27.519
Nilesh Gupta: So I hope this is clear. And specifically, the notations are clear. I did like.


276
00:37:28.589 --> 00:37:38.072
Nilesh Gupta: it's not written that cleanly, as I I imagine. But yeah, if there are any questions here like, please let me know, because we'll be using this notation.


277
00:37:38.562 --> 00:37:46.371
Nilesh Gupta: And then this way of thinking about neural networks later in the class, and actually like later throughout the whole course as well.


278
00:37:57.192 --> 00:38:04.767
Nilesh Gupta: Okay, so let's come to the question that like, why do we even need this nonlinearity like,


279
00:38:05.322 --> 00:38:14.031
Nilesh Gupta: essentially like, what we said initially, was that a neuron like takes some inputs. And then it's it did.


280
00:38:15.707 --> 00:38:21.401
Nilesh Gupta: It needs to accumulate those inputs into one signal. So why is the summation? Not enough?


281
00:38:22.122 --> 00:38:23.382
Nilesh Gupta: So


282
00:38:24.196 --> 00:38:35.172
Nilesh Gupta: I'm I'm sure, like some of you already know the answer, but still like, if you if others who don't like they wanna make a guess, or the people who know like they wanna


283
00:38:35.452 --> 00:38:41.961
Nilesh Gupta: say, like. What's the intuition behind having a nonlinearity inside a neuron?


284
00:38:47.602 --> 00:38:51.152
Vishal Thyagarajan: So we can like like model, more functions.


285
00:38:52.474 --> 00:38:58.221
Nilesh Gupta: But why not like, if I just had like in this neural network? If I just had linear neurons


286
00:38:58.422 --> 00:39:01.212
Nilesh Gupta: like, why is that not enough?


287
00:39:01.732 --> 00:39:08.341
Vishal Thyagarajan: Because we might as well just use like linear regression like we didn't like. There's no point in using a neural network. If it's just going to be linear right?


288
00:39:08.962 --> 00:39:10.882
Nilesh Gupta: Yeah. So


289
00:39:11.602 --> 00:39:21.331
Nilesh Gupta: so that's that's true. That like, if you have a bunch of linear neurons and you connect it in any way that you want, you will still end up getting a linear function.


290
00:39:21.992 --> 00:39:22.512
Nilesh Gupta: And


291
00:39:23.202 --> 00:39:31.582
Nilesh Gupta: I mean this. All of these diagrams that I've been using are from this playground or tensorflow. So let's quickly take a look at


292
00:39:33.324 --> 00:39:46.352
Nilesh Gupta: like, what for? For this dummy example that we had like if we had, like bunch of linear neurons connected like in a fairly complex setup in a neural network. And if we did training over it like


293
00:39:47.962 --> 00:39:57.122
Nilesh Gupta: anything that we do like, we'll still get a linear separator of our data, and particularly for this kind of data, which is a concentric circles like, we won't be able to separate it.


294
00:39:57.542 --> 00:39:58.772
Nilesh Gupta: So


295
00:39:59.922 --> 00:40:10.202
Nilesh Gupta: you can try to write the math, and, like you'll see that, like you will at the end. The neural network will be early like it will just represent A. W. Times X operation.


296
00:40:10.532 --> 00:40:15.562
Nilesh Gupta: So so let's see, like what


297
00:40:16.632 --> 00:40:27.281
Nilesh Gupta: go back. Let's see. Like, if if we had a relu activation here and then we did like training our model like you can see like it. It can


298
00:40:27.582 --> 00:40:55.281
Nilesh Gupta: give you like. Some, like the hidden neurons here can give you a part like a kind of a partition of your whole space, and then they can be combined in a way by the later neurons, to give you a much more complex boundary, for example, like it's like this is creating like a parabolic shape from the right side. This is a parabolic shape from the left side, and then these things can be combined to give you concentric circles.


299
00:40:55.682 --> 00:40:56.672
Nilesh Gupta: So


300
00:40:58.252 --> 00:41:07.492
Nilesh Gupta: so so, if you see like, even with a very simple nonlinearity such as value like you were you were able to approximate it, even if you have a sigmoid activation.


301
00:41:07.762 --> 00:41:13.762
Nilesh Gupta: alright like that, would still give you a similar result. Just that like it will.


302
00:41:16.482 --> 00:41:18.512
Nilesh Gupta: In this case it's not


303
00:41:28.852 --> 00:41:49.462
Nilesh Gupta: with enough learning like it does give you. So as you can see that even with activations like, they can be finicky at sometimes. So like depending on the type of problem you're solving, like, you'll have to choose the right kind of activations. But like having a nonlinearity really helps in modeling complex data.


304
00:41:49.882 --> 00:41:50.902
Nilesh Gupta: So


305
00:41:51.582 --> 00:42:00.331
Nilesh Gupta: coming back to our lecture, so nonlinearity gives us a superpower that, like combination of nonlinear neurons is much more expressive.


306
00:42:00.542 --> 00:42:04.582
Nilesh Gupta: And it is oh.


307
00:42:05.082 --> 00:42:19.781
Nilesh Gupta: it is actually like, so powerful that, like it can be shown that a neural network with at least one hidden layer, and which has a sufficiently large number of neurons and an appropriate activation function. It can approximate any continuous function.


308
00:42:20.484 --> 00:42:24.071
Nilesh Gupta: That to any desired degree of accuracy.


309
00:42:24.272 --> 00:42:25.392
Nilesh Gupta: So


310
00:42:27.912 --> 00:42:46.762
Nilesh Gupta: I'll give a slight intuition about like this. This theorem so essentially like, let's say that, like we have this function, this blue curve that we want to approximate. So how we can approximate it like we can construct a neural network which has like one hidden layer, and it has enough number of neurons in it.


311
00:42:46.982 --> 00:43:04.401
Nilesh Gupta: And it can like approximate this function to an arbitrary degree to an arbitrary accuracy. And how we can construct such a function. The key idea is that like 2 neurons can be constructed to give a pulse function. That is like, if you have, for example, like relu activation.


312
00:43:05.272 --> 00:43:20.301
Nilesh Gupta: then you can like have one neuron which gives you the one side of your step function, the other neuron, which gives you the other side of your step function, and you can combine with the weight one and minus one. To give you this step function


313
00:43:20.442 --> 00:43:39.401
Nilesh Gupta: and this width you can control depending on like, the what values that you choose. For even so, like typically like, there's a bias also, like in your neurons, which for brevity, I didn't show earlier, but depending on the biases that you choose. Like you can shift these


314
00:43:39.502 --> 00:43:51.132
Nilesh Gupta: activations like left or right, and you can give. Get a very arbitrary smaller with t 1 step for pulse function as well.


315
00:43:51.442 --> 00:44:06.331
Nilesh Gupta: and then, like all of these pairs of neurons, can be just summed up at the output layer to give you a bunch of rectangles like which approximate it. So if you keep like reducing the width of your pulse functions.


316
00:44:06.472 --> 00:44:11.672
Nilesh Gupta: then, like you can like, just get to an arbitrary degree of approximation for this continuous function.


317
00:44:12.112 --> 00:44:13.122
Nilesh Gupta: So


318
00:44:13.592 --> 00:44:26.521
Nilesh Gupta: yeah, feel free to read more about it. But like, it's, it's a pretty neat construction and pretty powerful way to see that like, with just like simple nonlinearities like, we are able to get some things this complex.


319
00:44:27.962 --> 00:44:35.372
Nilesh Gupta: So yeah, I realize, like, I'm running a bit short on time. So I'll speed up a little bit.


320
00:44:35.512 --> 00:44:40.101
Nilesh Gupta: So now, but still like any questions so far.


321
00:44:47.442 --> 00:44:48.192
Nilesh Gupta: Okay.


322
00:44:52.082 --> 00:44:54.341
Nilesh Gupta: so coming to loss functions.


323
00:44:55.052 --> 00:45:17.881
Nilesh Gupta: So the art like, remember, like how we started was deep. Learning has 3 key components. One is the model, then is the loss function. And then is your optimization process. So we have seen the model so far, which is a deep neural network, and how we do computation with it. Now let's come to the loss function. So loss is essentially a measure of how good our model is for an input.


324
00:45:17.982 --> 00:45:25.291
Nilesh Gupta: so usually, it's quantified in a negative way that, like, we measure like the cost of an incorrect prediction


325
00:45:25.512 --> 00:45:31.262
Nilesh Gupta: so like, if if your loss is 0, then your model, if your loss is less, then your model is better.


326
00:45:31.522 --> 00:45:44.551
Nilesh Gupta: So it's it's simply a notation, or like a convention kind of a thing. You could have defined it the other way as well, but like it makes things like mathematically, like, easier to justify sometimes.


327
00:45:44.702 --> 00:45:46.082
Nilesh Gupta: So


328
00:45:46.362 --> 00:45:53.132
Nilesh Gupta: it's it's so again, like, it's a scalar measure of how good our model is for an input


329
00:45:53.472 --> 00:46:03.912
Nilesh Gupta: so typically in deep learning, we measure the empirical loss, which is that we measure the total loss averaged over an entire data sets. So let's say, like we have, like N data points.


330
00:46:05.852 --> 00:46:13.681
Nilesh Gupta: Then, like our typically, our loss is going to be one by N. Summation over, I to N,


331
00:46:14.152 --> 00:46:17.641
Nilesh Gupta: and the loss function which takes your


332
00:46:18.532 --> 00:46:22.122
Nilesh Gupta: model parameters, which I'll represent by Theta


333
00:46:23.082 --> 00:46:27.831
Nilesh Gupta: and your Ith data point, which I'll represent by Xi.


334
00:46:29.022 --> 00:46:39.632
Nilesh Gupta: and then you sum over your all of your entire data set. And then you take the average. So this is what we call empirical loss or empirical risk.


335
00:46:47.932 --> 00:46:59.452
Nilesh Gupta: So we have seen loss functions like in in so far in the lecture, the 1st one that we started was a regression loss function, which was mean, squared error. So here, like


336
00:46:59.922 --> 00:47:14.622
Nilesh Gupta: we, we are given like our Yi is a scalar that we want to approximate. We make a scalar prediction which is our Yi hat for the I data point, and we take its the square difference


337
00:47:15.182 --> 00:47:21.501
Nilesh Gupta: as our loss function. So our like small L is going to be just


338
00:47:22.792 --> 00:47:30.482
Nilesh Gupta: the predicted scalar minus the actual, true scalar go ahead.


339
00:47:31.462 --> 00:47:36.142
Nilesh Gupta: So this is this, we have seen this already in regression.


340
00:47:36.322 --> 00:47:42.162
Nilesh Gupta: Then, in the last lecture we saw binary cross entropy for doing logistic regression.


341
00:47:47.942 --> 00:47:58.611
Nilesh Gupta: And this is essentially like we had a 2 vehicle problem classification problem that is like, we want to identify whether this class is present or not so. Our yi is


342
00:47:59.822 --> 00:48:09.762
Nilesh Gupta: 0 or one, and then like why I had, is the probability of class being present


343
00:48:11.812 --> 00:48:13.652
Nilesh Gupta: which is given by your model.


344
00:48:16.722 --> 00:48:19.702
Nilesh Gupta: and then, like our losses. So


345
00:48:19.932 --> 00:48:25.722
Nilesh Gupta: this quantity, actually, the negative of this quantity which I've taken here outside.


346
00:48:25.932 --> 00:48:31.331
Nilesh Gupta: So the negative of this quantity is our loss for that data point.


347
00:48:31.462 --> 00:48:34.331
Nilesh Gupta: And this we call the Binary cross entropy.


348
00:48:34.452 --> 00:48:42.222
Nilesh Gupta: And remember, like this works for a kind of a two-way classification problem. That is, you want to identify whether something is present or not.


349
00:48:43.042 --> 00:49:09.301
Nilesh Gupta: So like a generalization of this binary cross entropy is the cross entropy function, which, if you like, really look deep, like carefully like it. If if you have it, it looks very similar to the binary cross entropy. It's just a general version of it. So this is for like, if we have 2 like instead of 2 way classification, we have C way classification problem, that is, we have, like C classes


350
00:49:09.502 --> 00:49:11.172
Nilesh Gupta: to classify into.


351
00:49:15.032 --> 00:49:28.551
Nilesh Gupta: Then we can write our loss like in this way, and if if you, if C equal to 2, you can see that like it will actually be will be the same equation as the binary cross entropy.


352
00:49:28.812 --> 00:49:42.912
Nilesh Gupta: So the this loss is typically like, remember, like here, like our YI, hat is a vector


353
00:49:45.502 --> 00:49:52.061
Nilesh Gupta: of size. C, and this YIJ. Hat, needs to be a probability distribution.


354
00:49:56.342 --> 00:49:57.312
Nilesh Gupta: So


355
00:49:57.622 --> 00:50:04.761
Nilesh Gupta: like typically in deep learning, as I said, like. Whenever you like, you encounter a probability distribution, you model it using softmax.


356
00:50:05.252 --> 00:50:09.302
Nilesh Gupta: So this why I had is


357
00:50:10.032 --> 00:50:14.892
Nilesh Gupta: often, or like almost all the times, like a result of a softmax operation.


358
00:50:15.042 --> 00:50:22.602
Nilesh Gupta: So it's it's cross. Entropy is like very much like if you in deep learning, you'll see like it disappeared with your Softmax operation.


359
00:50:23.612 --> 00:50:27.342
Nilesh Gupta: But this is the loss function, like, if you have a Cva classification problem.


360
00:50:28.000 --> 00:50:47.732
Nilesh Gupta: Let's let's take some. Take a look at some examples and see. So for the 1st example, we have this input of an image of a cat or like it, it may not be a cat, and that's our task that is there a cat in the image? So what kind of loss function will use in this case?


361
00:50:57.722 --> 00:51:01.362
Nilesh Gupta: I know it's simple, but still feel free to speak up.


362
00:51:03.312 --> 00:51:05.782
Hanaa Siddiqui: Would you use binary cross entropy.


363
00:51:05.782 --> 00:51:10.132
Nilesh Gupta: Yeah. So because it's a 2 way classification problem that we wanna


364
00:51:10.282 --> 00:51:18.851
Nilesh Gupta: identify. If a cat is present or not like the right or the appropriate loss function, for this task is going to be Bce.


365
00:51:18.992 --> 00:51:20.802
Nilesh Gupta: which is binary cross entropy.


366
00:51:21.362 --> 00:51:28.441
Nilesh Gupta: So let's take another example, like, let's say, like, we want to determine the age of the cat. What loss function like should we use now?


367
00:51:44.892 --> 00:51:46.002
Hanaa Siddiqui: Squared error.


368
00:51:46.602 --> 00:51:51.991
Nilesh Gupta: Yep, because we wanna like our output is now just a scalar quantity.


369
00:51:53.189 --> 00:52:00.231
Nilesh Gupta: Which? Which? So we we can use something like mean squared error like to do regression over it.


370
00:52:02.502 --> 00:52:05.972
Nilesh Gupta: Also, like it can potentially be solved using


371
00:52:06.092 --> 00:52:15.640
Nilesh Gupta: cross entropy as well like, we can just model every age, category, or every age that exists as a separate class and do it. But


372
00:52:16.627 --> 00:52:20.431
Nilesh Gupta: in this particular case, I don't think it's the right formulation.


373
00:52:21.018 --> 00:52:34.412
Nilesh Gupta: Yeah. But I just wanted to say that that kind of that can also be used. But a mean, squared error is the more appropriate option in this case.


374
00:52:34.762 --> 00:52:36.570
Nilesh Gupta: and then like it could be


375
00:52:37.956 --> 00:52:52.351
Nilesh Gupta: that, like we we, it's not just a 1-way 2 way classification. That means there is a cat or not. Let's say we wanted to find a cat versus a dog versus a bird. So this becomes like now a 3 way classification.


376
00:52:52.592 --> 00:53:02.082
Nilesh Gupta: So for this cross entropy, specifically cross entropy paired with softmax is appropriate.


377
00:53:02.442 --> 00:53:06.632
Nilesh Gupta: Oh, loss function for this task.


378
00:53:07.262 --> 00:53:13.971
Nilesh Gupta: So okay, any questions so far like, because now we'll be moving to the next component.


379
00:53:21.602 --> 00:53:41.922
Nilesh Gupta: So coming to the next component, which is optimization. So remember this, here, we'll be talking about optimization on a very broad level. We'll have, like a specific lectures dedicated to optimization and the better versions of optimization we can do. But here I just wanted to introduce optimization in the context of deep learning.


380
00:53:42.142 --> 00:53:45.862
Nilesh Gupta: And how does it like look like on a broad level?


381
00:53:47.272 --> 00:53:54.771
Nilesh Gupta: So the overview of optimization is the goal is that we want to minimize the loss by adjusting the model parameters.


382
00:53:54.932 --> 00:54:05.061
Nilesh Gupta: So let's say, like our loss, surface looks something like this. In this particular case. We only have 2 parameters w. 1 and W. 2,


383
00:54:05.192 --> 00:54:16.862
Nilesh Gupta: and for each choice of w. 1 and W. 2 like we can. We can like have a loss for w. 1 comma. W. 2. And let's say we have some data. D,


384
00:54:16.962 --> 00:54:20.281
Nilesh Gupta: so this this loss will be a scalar quantity.


385
00:54:21.872 --> 00:54:22.532
Nilesh Gupta: Sorry.


386
00:54:28.632 --> 00:54:31.692
Nilesh Gupta: And this is what is represented on this axis


387
00:54:32.552 --> 00:54:39.572
Nilesh Gupta: for a particular choice of loss of w. 1 and W. 2, we can get some loss


388
00:54:39.832 --> 00:54:44.141
Nilesh Gupta: here, so if you draw the whole lost surface. It will look something like this.


389
00:54:44.442 --> 00:54:47.072
Nilesh Gupta: And oh.


390
00:54:47.192 --> 00:55:02.602
Nilesh Gupta: the goal of deep learning or goal of the whole Ml. Process is to get your some model parameters which minimize this loss, which which gives you the global minimum of this loss.


391
00:55:02.772 --> 00:55:31.441
Nilesh Gupta: So like one very brute force thing that you can think of is like you can just try bunch of random w. 1 and W. 2, or maybe a grid of w. 1 and W. 2, and then evaluate your loss on each of those choices, and then you can choose the one which gives you the least loss. But of course, like even in this case, like which, where we only have 2 parameters like the search space is pretty big, because, like it's continuous.


392
00:55:31.742 --> 00:55:32.782
Nilesh Gupta: So


393
00:55:33.652 --> 00:55:58.842
Nilesh Gupta: even then, like this brute force approach might not like is going to be infeasible, and will not give you the right results that you're looking for. But typically like in deep learning, like, we easily have billions of parameters these days, or like even touching trillion of parameters. So in that case, like it just becomes like infeasible to even think of something like brute force.


394
00:55:58.962 --> 00:56:20.462
Nilesh Gupta: So we we use like the most popular optimization process that is being used in deep learning is gradient descent, which is like we iteratively update parameters in the opposite direction of the gradient. So we we start from a random initialization of our w. 1 and W. 2. Let's say this point is here.


395
00:56:20.622 --> 00:56:27.211
Nilesh Gupta: and then we compute the loss, compute it the gradient with respect to w. 1 and W. 2,


396
00:56:27.532 --> 00:56:40.281
Nilesh Gupta: and take the step like a scaled step in the opposite direction of the gradient, so the scaled opposite direction will look something like this here. It's denoted in the picture, but maybe it's not very clear to see


397
00:56:40.842 --> 00:56:42.022
Nilesh Gupta: so


398
00:56:42.332 --> 00:56:53.221
Nilesh Gupta: and we keep taking these steps again and again. And this will eventually like for some lost surfaces, it is guaranteed to arrive at the global minimum. But


399
00:56:53.672 --> 00:56:58.982
Nilesh Gupta: even though, like, it's like, quite surprising. And it's it's 1 of the


400
00:57:01.282 --> 00:57:16.782
Nilesh Gupta: probably unknown parts of deep learning is that, like even like of like, currently in deep learning loss surfaces don't look this smooth or this neat like. They are very jagged lot of filled with a lot of valleys and


401
00:57:19.512 --> 00:57:32.332
Nilesh Gupta: plain areas. But still, like this, gradient descent works like or like some version of gradient descent works which is just the simple process of iteratively updating parameters in the opposite direction of the gradient.


402
00:57:33.572 --> 00:57:53.302
Nilesh Gupta: So that's that's the kind of the overview of gradient descent. If we try to look at in terms of an algorithm. The pseudo code would look something like this that you initialize your parameters, your W weights randomly or like from from like some distribution.


403
00:57:53.412 --> 00:57:55.002
Nilesh Gupta: and


404
00:57:55.192 --> 00:58:22.632
Nilesh Gupta: actually like how you initialize your parameters also play a role in modern deep learning. So this will like in one of the homeworks will take a look like it will you. You'll experience that like with different initializations, you'll get different results. But in this lecture. I'm not going to cover it, but you initialize your parameters, for now let's just assume, like you initializing randomly from a normal distribution.


405
00:58:22.972 --> 00:58:33.871
Nilesh Gupta: and then for e. For like, until convergence for each iteration like you want to compute the loss. L. You want to compute the gradients of the loss with respect to each of your parameters that you have.


406
00:58:34.242 --> 00:58:42.341
Nilesh Gupta: This will give you your gradient, vector and you update your parameters by a scaled negative


407
00:58:42.542 --> 00:58:45.901
Nilesh Gupta: of your gradient. So this eta.


408
00:58:46.212 --> 00:58:51.572
Nilesh Gupta: This eta is our scale, which is typically the learning rate.


409
00:58:52.982 --> 00:59:00.572
Nilesh Gupta: And this negative dl, by Dw, is our gradient on the negative gradient.


410
00:59:00.922 --> 00:59:04.321
Nilesh Gupta: So that's the pseudocode for gradient descent.


411
00:59:04.482 --> 00:59:05.347
Nilesh Gupta: But


412
00:59:06.342 --> 00:59:27.742
Nilesh Gupta: okay, so I mean, like, the algorithm looks simple. And all of these steps, like initializing parameters like we know, like we can do call Np, dot Rand, computing the loss. We saw that, like you can do like how to do a forward pass with neural network. And then, like you just need to plug your loss. Calculation, equation there.


413
00:59:28.172 --> 00:59:45.872
Nilesh Gupta: we this also like looks simpler like once we have deal by DW. Quantity, then like, this is just like one equation which can be just written in one line. But this the computing the gradients of the loss with respect to W. This is the one of the hard parts of this algorithm.


414
00:59:46.212 --> 00:59:49.742
Nilesh Gupta: and that we achieve


415
00:59:50.722 --> 01:00:06.652
Nilesh Gupta: like in case of neural networks, or in case of like any kind of functions like continuous functions. We can do it now like we, we need a efficient way to compute the gradients for all parameters, and we do it using by via back propagation.


416
01:00:06.932 --> 01:00:08.221
Nilesh Gupta: And it is


417
01:00:08.512 --> 01:00:16.702
Nilesh Gupta: back. Propagation is essentially like the chain rule to like propagate errors starting from the output to the input


418
01:00:17.062 --> 01:00:18.052
Nilesh Gupta: so


419
01:00:20.032 --> 01:00:35.002
Nilesh Gupta: like just a refresher on the chain rule. So like chain rule, like for a parameter, theta that affects the loss L through an intermediate variable. Z. We can write del L. By del Theta, as del L. By del z times del z by del theta.


420
01:00:35.262 --> 01:00:36.492
Nilesh Gupta: So


421
01:00:37.476 --> 01:00:54.462
Nilesh Gupta: that's your chain rule, and like back provision typically involves like a forward pass and a backward pass forward pass is the like, the the what we already saw that, like in neural networks like how to compute the final output of your function. So and so forward pass just means, like you in


422
01:00:54.895 --> 01:01:16.952
Nilesh Gupta: along with computing the just the final output. You also compute all the intermediate activations as well, because that will be needing in the backward pass. So you compute all intermediate activations, Zl and Al that we saw earlier, and then, like in the backward pass, starting at the output layer, you compute the gradient and propagate it backwards.


423
01:01:17.822 --> 01:01:19.930
Nilesh Gupta: So I mean, it's it's


424
01:01:20.838 --> 01:01:30.021
Nilesh Gupta: I know, like, here, I'm just seeing it. But let's let's take a look at a sample task, and how how to do back propagation on that task.


425
01:01:30.602 --> 01:01:38.739
Nilesh Gupta: So let's let's take this sample task that we saw earlier that, like, it's, it's a concentric


426
01:01:40.009 --> 01:01:52.112
Nilesh Gupta: classification problem like you have, like your orange data points on your outer ring. And then you have, like the blue data points in the in, in the inside. And you want to classify orange from blue.


427
01:01:52.622 --> 01:01:56.762
Nilesh Gupta: So maybe like something you we want to get like a boundary like this.


428
01:01:57.682 --> 01:02:04.372
Nilesh Gupta: So here, like, let's say, like we are. Because, like, it's a two-way classification problem, we'll be using the binary cross entropy.


429
01:02:06.388 --> 01:02:09.952
Nilesh Gupta: loss function. Sorry there should be a negative here.


430
01:02:12.262 --> 01:02:18.321
Nilesh Gupta: So and the model that we'll be using for this sample task is going to be a


431
01:02:19.122 --> 01:02:23.261
Nilesh Gupta: like a neural network with one hidden layer.


432
01:02:23.502 --> 01:02:26.262
Nilesh Gupta: It's it's there's only one hidden layer. h 1.


433
01:02:26.622 --> 01:02:28.882
Nilesh Gupta: And then this is my output layer.


434
01:02:29.592 --> 01:02:31.611
Nilesh Gupta: And this is my input layer.


435
01:02:32.232 --> 01:02:41.002
Nilesh Gupta: And we'll be using sigmoid activation. And there are like 3 hidden neurons in my activation


436
01:02:41.132 --> 01:02:44.352
Nilesh Gupta: and then my hidden layer at layer one.


437
01:02:44.822 --> 01:02:46.002
Nilesh Gupta: So


438
01:02:46.262 --> 01:02:59.532
Nilesh Gupta: the optimization process, as I showed earlier, like it, will be the gradient descent method that will compute the loss. Compute gradients, update parameters. But as we saw like, this is the hard part. So that's what we'll be diving a bit deeper into.


439
01:02:59.972 --> 01:03:04.331
Nilesh Gupta: So so the forward pass. Let me


440
01:03:04.732 --> 01:03:09.441
Nilesh Gupta: right again, like we have already seen this, but for X


441
01:03:10.002 --> 01:03:17.732
Nilesh Gupta: we can define as a matrix, which is of 2 cross. One has 2 features and one data point in this case.


442
01:03:18.152 --> 01:03:27.002
Nilesh Gupta: and then RH. 1, which is parameterized by w. 1 is going to be all


443
01:03:27.132 --> 01:03:39.532
Nilesh Gupta: 3 cross 2 matrix, and then my output layer is going to be one W. 2, which is


444
01:03:40.872 --> 01:03:52.791
Nilesh Gupta: one plus 3, and my forward pass is going to look like my z. 1 is w. 1. Transpose


445
01:03:53.312 --> 01:03:54.242
Nilesh Gupta: X.


446
01:03:57.392 --> 01:04:09.732
Nilesh Gupta: My, a 1 is going to be sigmoid of transpose X, or let me. Just try.


447
01:04:10.232 --> 01:04:11.281
Nilesh Gupta: Let's even


448
01:04:15.562 --> 01:04:28.342
Nilesh Gupta: my Z. 2 is going to be W. 2. Transpose e 1 and my T. 2,


449
01:04:28.642 --> 01:04:35.092
Nilesh Gupta: which is also my final prediction from my neural network, so I'll call it my y hat as well


450
01:04:35.212 --> 01:04:39.352
Nilesh Gupta: is going to be sigmoid of Z. 2.


451
01:04:42.302 --> 01:04:46.182
Nilesh Gupta: My loss function is going to be


452
01:04:47.068 --> 01:04:53.272
Nilesh Gupta: here, because there's only one data point. So I'm just writing it for that. It's going to be y


453
01:04:53.382 --> 01:05:05.292
Nilesh Gupta: times log of y hat plus one minus y times log of one minus y hat.


454
01:05:12.152 --> 01:05:15.271
Nilesh Gupta: Negative. Of this, any questions. So far.


455
01:05:17.442 --> 01:05:26.401
Nilesh Gupta: so this we have kind of already seen, like we have seen a more complicated version of it right now. There's only one hidden layer. So it's simpler to write.


456
01:05:26.812 --> 01:05:33.112
Nilesh Gupta: So that's what my forward pass is going to look like. And remember that, like I'm oh.


457
01:05:33.832 --> 01:05:42.542
Nilesh Gupta: along the way I'm I'm storing my z. 1, my a. 1, my Z. 2, my a 2, because that I'll be needing and my backward pass.


458
01:05:45.112 --> 01:05:55.872
Nilesh Gupta: So okay, so let's see like how the backward pass looks like. And for this particular demonstration I'll we want to compute our del L.


459
01:05:55.992 --> 01:06:05.792
Nilesh Gupta: By Dell, w. 1, 1 comma. 2. That is, for this particular weight. W.


460
01:06:06.442 --> 01:06:07.732
Nilesh Gupta: Comment to.


461
01:06:09.222 --> 01:06:16.501
Nilesh Gupta: We want to compute the gradient with respect of the del L. By del this weight.


462
01:06:18.282 --> 01:06:24.521
Nilesh Gupta: So, as I said, like, let's let's try to draw our chain like we have our X,


463
01:06:26.152 --> 01:06:31.121
Nilesh Gupta: that using w. 1 v. 7,


464
01:06:37.541 --> 01:06:40.262
Nilesh Gupta: using w. 1, we got to z 1.


465
01:06:40.882 --> 01:06:52.872
Nilesh Gupta: Using sigmoid. We got to a 1, then, using W. 2, but Z. 2.


466
01:06:54.842 --> 01:07:03.251
Nilesh Gupta: Then, using 6 points, we got to our Y hat, which is also a 2.


467
01:07:04.592 --> 01:07:09.431
Nilesh Gupta: Then, like applying our loss function here, like we, the BC.


468
01:07:09.782 --> 01:07:13.471
Nilesh Gupta: From loss function, we ended up being at L,


469
01:07:13.802 --> 01:07:16.541
Nilesh Gupta: so we want to get Del L.


470
01:07:18.362 --> 01:07:29.991
Nilesh Gupta: And by del like us, like w, 1 comma 2 is part of this matrix. So like, we want to like go backward in this chain.


471
01:07:30.532 --> 01:07:41.032
Nilesh Gupta: So so the does anyone help me get like or like, just


472
01:07:41.242 --> 01:07:45.292
Nilesh Gupta: want to say what what this chain rule is going to look like.


473
01:07:48.432 --> 01:07:53.532
Inderjit Dhillon: Nilesh. I think the slide is stuck of the optimization. Forward pass.


474
01:07:53.812 --> 01:07:54.552
Nilesh Gupta: Of.


475
01:07:56.802 --> 01:07:58.581
Inderjit Dhillon: Okay, so somehow, yeah.


476
01:07:59.352 --> 01:08:01.371
Nilesh Gupta: Oh, I didn't realize this. Sorry.


477
01:08:05.302 --> 01:08:05.922
Vishal Thyagarajan: Maybe.


478
01:08:05.922 --> 01:08:06.522
Nilesh Gupta: So


479
01:08:07.712 --> 01:08:14.242
Nilesh Gupta: okay, you can see the slide now, right like. And does anyone help me write the chain rule here.


480
01:08:16.422 --> 01:08:18.682
Vishal Thyagarajan: Maybe like a dl.


481
01:08:18.812 --> 01:08:20.751
Nilesh Gupta: Dy hat.


482
01:08:22.082 --> 01:08:24.812
Nilesh Gupta: What do I had? Okay.


483
01:08:25.912 --> 01:08:30.631
Vishal Thyagarajan: Multiplied by DY hat over dz 2.


484
01:08:32.622 --> 01:08:37.882
Vishal Thyagarajan: And basically, yeah, you just keep following like the like, the chain


485
01:08:38.483 --> 01:08:44.952
Vishal Thyagarajan: like the diagram that you drew out except backwards. So the next one would be like DZ 2 over da one.


486
01:08:46.062 --> 01:08:49.522
Nilesh Gupta: Right, we can keep going backwards.


487
01:08:50.562 --> 01:08:53.492
Nilesh Gupta: should have written it more neatly.


488
01:09:19.842 --> 01:09:26.685
Nilesh Gupta: Yeah. So that's what your chain rule is going to look like. And specifically because


489
01:09:29.192 --> 01:09:37.201
Nilesh Gupta: because we are kind of just like in this particular example, we are just looking at the w 1 comma 2. So we can


490
01:09:37.512 --> 01:09:38.392
Nilesh Gupta: the


491
01:09:38.822 --> 01:09:53.912
Nilesh Gupta: kind of further simplify this into like. Not all the parameters in our z. 1 is going to get affected, like as we can see, like there's like just a single chain, so like we can simplify this into Del L. By del


492
01:09:54.072 --> 01:09:55.552
Nilesh Gupta: my hat.


493
01:10:04.732 --> 01:10:10.642
Nilesh Gupta: Times tell me why I had my 10th c. 2.


494
01:10:11.092 --> 01:10:13.222
Inderjit Dhillon: Are you riding Nilesh, or.


495
01:10:13.222 --> 01:10:13.922
Nilesh Gupta: Yeah.


496
01:10:14.332 --> 01:10:15.592
Nilesh Gupta: Oh, you can't see.


497
01:10:15.592 --> 01:10:16.882
Inderjit Dhillon: Yeah, somehow. Okay.


498
01:10:16.882 --> 01:10:17.452
Nilesh Gupta: Okay?


499
01:10:17.792 --> 01:10:19.062
Nilesh Gupta: Why not?


500
01:10:19.502 --> 01:10:20.922
Inderjit Dhillon: I don't know why. There's a lag.


501
01:10:21.872 --> 01:10:22.492
Nilesh Gupta: I see.


502
01:10:23.822 --> 01:10:25.382
Nilesh Gupta: Sorry about that.


503
01:10:27.892 --> 01:10:34.151
Nilesh Gupta: Till C 2, you can see that only this the the second component of


504
01:10:34.292 --> 01:10:39.522
Nilesh Gupta: a, 2 is affecting it so like we can just write here like that.


505
01:10:40.772 --> 01:10:42.192
Nilesh Gupta: e. 1.


506
01:10:45.492 --> 01:10:50.202
Nilesh Gupta: Oh, 2


507
01:11:05.522 --> 01:11:08.971
Nilesh Gupta: little bit messy. So I'll I'll remove this part


508
01:11:25.442 --> 01:11:26.302
Nilesh Gupta: to.


509
01:11:34.982 --> 01:11:51.972
Nilesh Gupta: So I've just like simplified it for this specific weight value that we we are seeking. So like here, I just like written the specific coordinates which are going to affect this quantity, so


510
01:11:52.502 --> 01:12:12.341
Nilesh Gupta: I don't have too much time, so like I'll just like quickly go over it. But let me so this this quantity, if if you simplify for the binary Cross entropy loss, this will turn out to be a very simple quantity, which is going to be your Y hat minus y


511
01:12:14.072 --> 01:12:19.012
Nilesh Gupta: this this whole quantity, and then del z 2 by del a 12


512
01:12:19.702 --> 01:12:25.842
Nilesh Gupta: as we saw like, it's it's like how they are related like aid of


513
01:12:26.732 --> 01:12:29.526
Nilesh Gupta: a 1. 2 is kind of


514
01:12:31.612 --> 01:12:32.307
Nilesh Gupta: It's


515
01:12:33.402 --> 01:12:41.863
Nilesh Gupta: It's it's like we. We multiplied a 1 with W. 2 to get Z. 2. So that's why, like, it's, it's only going to be


516
01:12:43.055 --> 01:12:45.622
Nilesh Gupta: like, if Lippis.


517
01:12:50.562 --> 01:12:51.322
Nilesh Gupta: yeah.


518
01:12:52.912 --> 01:13:04.012
Nilesh Gupta: So my Z 2 was 22. Transpose he won.


519
01:13:04.662 --> 01:13:12.942
Nilesh Gupta: So that's the relation between z. 2 and a 1. So this quantity, if you compute this, will turn out to be


520
01:13:15.422 --> 01:13:17.635
Nilesh Gupta: This will turn out to be just


521
01:13:19.532 --> 01:13:25.712
Nilesh Gupta: W. 2, and the second coordinate of W. 2, 2.


522
01:13:26.792 --> 01:13:27.462
Nilesh Gupta: And


523
01:13:28.082 --> 01:13:34.661
Nilesh Gupta: this quantity like, because, like the relation between this is the sigmoid activation. So this will turn out to be


524
01:13:35.303 --> 01:13:39.311
Nilesh Gupta: the derivative of sigmoid sigmoid derivative is


525
01:13:39.752 --> 01:13:50.901
Nilesh Gupta: sigmoid sigmoid value times one minus sigmoid. So I'm just using this property to quickly write the equation here. It's going to be


526
01:13:52.652 --> 01:13:59.072
Nilesh Gupta: a 1 2 times 2.


527
01:13:59.372 --> 01:14:09.532
Nilesh Gupta: And this again is just this. c 1 was w. 1 transpose X,


528
01:14:09.752 --> 01:14:16.332
Nilesh Gupta: so this will turn out to be just the second coordinate of my X element. So dot x 2.


529
01:14:16.662 --> 01:14:20.751
Nilesh Gupta: So that's what your gradient will look like


530
01:14:35.432 --> 01:14:38.631
Nilesh Gupta: for this particular weight, wet weight value.


531
01:14:39.352 --> 01:15:07.661
Nilesh Gupta: And here, like I have shown in just just like for this particular W. But in practice, like as in forward pass, as we were dealing with matrices. And we had this compact representation. We also like, even in backward pass, we, we like compute the by the gradients like in in a matrix way so like for the whole weight vectors we compute simultaneously using matrix operations. So


532
01:15:08.312 --> 01:15:19.482
Nilesh Gupta: similar to the forward pass, like, it's going to be like it. Like all the scalar values will get replaced by your matrices or the intermediate vectors that you've got.


533
01:15:19.612 --> 01:15:33.992
Nilesh Gupta: but that you will have to do like in your homework so like you can get an idea how to do that. But for brevity like here, I have just like explored like how to compute gradient. For with respect to one weight value.


534
01:15:34.542 --> 01:15:50.951
Nilesh Gupta: So that kind of ends like our, the full algorithm, deep learning algorithm on a high level that that we set out to describe in this lecture, which is like the step one is your forward pass.


535
01:15:50.952 --> 01:16:09.112
Nilesh Gupta: You compute activations. Step 2 is your loss computation. You use the output of your neural network to compute the loss, then you do the backward pass, using back propagation, and then you do the parameter update according to the gradient, descend update rule, and you repeat this process for multiple steps until convergence.


536
01:16:09.412 --> 01:16:10.692
Nilesh Gupta: So


537
01:16:12.282 --> 01:16:30.221
Nilesh Gupta: in practice, I just want to like, take 1min to quickly. Just say that, like, there are other things that you want to carry like you need to incorporate into a successful deep learning algorithm. That is one thing is like efficiency, like right now, like gradient descent operates on the full data set.


538
01:16:30.272 --> 01:16:43.221
Nilesh Gupta: But that is going to be inefficient, and in last lecture as well like, we briefly touched upon it that, like, we want to do a Mini batch version of gradient descent to gain efficiency


539
01:16:43.262 --> 01:17:10.141
Nilesh Gupta: and better convergence. This will explore later in the class. And then also, like you, the learning rate. We didn't really talk about the learning rate and the gradient descent method. So and that is also something you don't want to keep constant. So like. There are like adaptive versions of your gradient descent, which, depending on the kind of loss surfaces that they are seeing. They adapt their learning rate or their scaling of your gradients.


540
01:17:10.222 --> 01:17:36.171
Nilesh Gupta: based on like, what kind of lost surface they are operating in. This also will explore more in the optimization lecture coming ahead. And then there is regularization that we saw in our previous classes. But here, in case of deep learning, regularization will look something different, and this also will explore later, and then rest of the homeworks.


541
01:17:36.542 --> 01:17:47.911
Nilesh Gupta: So sorry for that, that concludes the lecture, and I apologize for going a little bit over the time. But yeah, any questions.


542
01:17:58.582 --> 01:18:00.465
Inderjit Dhillon: Okay, well, thank you, Nilesh.


543
01:18:01.332 --> 01:18:05.501
Inderjit Dhillon: for giving this lecture. Thank you. And see you all on Wednesday.


544
01:18:08.432 --> 01:18:09.512
Nilesh Gupta: Thanks. Everyone.


545
01:18:10.232 --> 01:18:10.802
Hormoz Shahrzad: Bye.



